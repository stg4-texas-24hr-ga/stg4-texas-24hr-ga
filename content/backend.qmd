---
title: Backend
---

This section of the webpage is where I'm keeping notes on specific aspects of the project.  It's a little bit of a hodgepodge.

## The Repositories

The details on how the containers are constructed and run through GitHub Actions are contained in the repositories themselves.  I have a GitHub organization named 'stg4-texas-24hr-ga', this github org is owned by my main github 'cfurl'.  The benefit here is a) I can separate production code base from my messy repository and b) when an action run fails it hits my main email - which is shared with the cfurl repo.  Here is a description of what is in those repos:

1. 'stg4-texas-24hr-ga' [Public Github repo](https://github.com/stg4-texas-24hr-ga/stg4-texas-24hr-ga) for this website.  There are a bunch of good Quarto build examples [here](https://github.com/mcanouil/awesome-quarto?tab=readme-ov-file) along with instructions on how to turn your site into a CI/CD system that builds daily.  This website is a built off of a simple template provided by [NOAA](https://github.com/nmfs-opensci/NOAA-quarto-simple).  The website is static.

2. 'stg4-texas-24hr-backend-actions' [Private Github repo](https://github.com/stg4-texas-24hr-ga/stg4-texas-24hr-backend-actions) that contains the docker-compose.yml which controls container linking and orchestration, and the compose-workflow.yml which controls the triggering of the daily run.

3. 'stg4-texas-24hr-docker' [Private Github repo](https://github.com/stg4-texas-24hr-ga/stg4-texas-24hr-docker) that contains the codebase and the Dockerfile image build instructions to create each of the images.  My typical workflow is I'll develop in a separate sandbox repo.  When I have the code working on an .Rproj relative path basis, I'll amend the code to work with container paths, build the image, place on Docker Hub.  I take the container path code, Dockerfile, and docker_file_build_instructions.txt and place in this repo.  Everything you need to build the code from scratch (or ammend!) is in this repo. 

4. 'stg4-texas-24hr-frontend-actions' [Private Github repo](https://github.com/stg4-texas-24hr-ga/stg4-texas-24hr-frontend-actions) that contains a Github actions daily website build connected to Posit Cloud Connect.  The Github actions makes a daily commit, this triggers a PCC website build, and a new map is shown on the frontend.  I'm almost positive I can delete this, it's deprecated.

## Notes

### Delivery of Stage IV data

Stage IV data are packaged as GRIB2 files are available each in 1-hr, 6-hr, and 24-hr files.  The 6 and 24 hour files are summed from the one hour files.   Presently, I'm working with a 24-hr file and pulls daily.  I have 1-hr data processed for Texas for the entire period of record from 2002-2023 (or 2024), this is what I've been giving to Hatim.

Stage IV data are available in near real-time on NOMADS at :55 past the hour. For example, hourly rainfall ending 2:00 (representing 1:00-2:00) are available at 2:55. Twenty-four hour data are available from 12:00 - 12:00 UTC (available at 12:55 UTC).  Data for the last 14 days are kept on NOMADS after which they are cycled off. The real-time NOMADS site is available [here](https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/).  Note, there are .01h, .06hr, and .24hr files for CONUS, pr (Puerto Rico), and ak (Alaska).  

After the near real-time file drop, an additional rerun at 30h after valid time (18:55Z) is available to supplement 24-hr mosaics if RFCs need to update their QPEs or make changes.  Personal communication with the WGRFC indicates that this is done very infrequently.

Since at least 2012 when I started tinkering with the archived Stage IV data, the historical datasets were kept at [data.eol.ucar](https://data.eol.ucar.edu/cgi-bin/codiac/fgr_form/id=21.093).  As of summer 2025, the data have been moved to [rda.ucar.edu](https://rda.ucar.edu/datasets/d507005/dataaccess/).  At this new archive location they are tarring together Stage IV data on monthly increments.  The data appear to be coming available around the 20th of the next month (for example August data available ~ September 20th).

Routines for manual processing of the data are contained in this [Private repo](https://github.com/cfurl/manual_stg4_proces).  It's decently complete, but I really should go build it up since I do this work infrequently now.   

### GRIB2  data format

As previously described, Stage IV data are stored in GRIB2 binary format.  Stage IV precipitation has been packaged in both the GRIB1 format (until July 2020) and currently resides in the GRIB2 format.  The processing procedures and wgrib.exe utility are completely different between GRIB1 and GRIB2.  Again, could use some more documentation on how to build out historical records if I ever needed to do this outside of Texas. 

### Containers and Orchestration

Building an image that simply spawns a container to run a script with few libraries is very straightforward.  It begins to get complicated when your image and resulting containers begin to rely on varied technology streams.  Using the 'scraper' container as an example, pasted below is the 'Dockerfile' that you use to build the image.  Each image starts from another image, in this case the FROM command downloads a specific version of r from the 'rocker' repository.  This image or layer, contains all of the dependencies needed to run r 4.2.2, next I create a series of folders in my image, then I run an R command to install.packages, assign working directory, and lastly copy code from my local computer to the image.  

Now everytime I call on that image a container is spawned that runs 'stg4_24hr_scraper.R' in the proper R environment with the proper packages installed.  This script simply downloads the latest GRIB2 file and puts it in a shared volume where the next container can interact with the file.

Build that image in PowerShell: docker image build --tag stg4_24hr_scraper .
Run that image in PowerShell:  docker container run -v //c/stg4/stg4-texas-24hr-docker/scraper/data:/home/data stg4_24hr_scraper

You can then inspect you container through PowerShell to see if your file downloaded.

```{r}
#| eval: false
#| echo: true

FROM rocker/r-ver:4.2.2

RUN mkdir -p /home
RUN mkdir -p /home/code
RUN mkdir -p /home/data

RUN R -e "install.packages(c('dplyr','stringr','rvest'))"

WORKDIR /home

COPY /code/stg4_24hr_scraper.R /code/stg4_24hr_scraper.R
CMD Rscript /code/stg4_24hr_scraper.R

```



The order of the containers and their dependencies are managed through Docker Compose. I still build "docker image build --tag stg4_24hr_scraper .", test "docker container run -v //c/stg4/stg4-texas-24hr-docker/scraper/data:/home/data stg4_24hr_scraper", tag, push, pull containers manually in PowerShell, but to put it in production you'll want it in a docker-compose.yml.  These are simple files to write, in general, getting the guts of the image correct is much more difficult that putting containers together.  

Below is a piece of the docker-compose.yml used int the stg4 orchestration.  For the GitHub Actions CI/CD this must reside at the root folder of the GA repository. When you work in this .yml you have to mind your indentations, runs fail fast so it's nice, but very picky.  

The 'scraper' container is the simplest of the bunch. Docker-compose already knows 'cfurl/stg4_24hr_scraper:v1' is referring to Docker Hub repo.  The 'volumes' line indicates 'shared volume between containers':'local path in specific container'.  In the scraper example, data are downloaded from the web to /home/data, this path is automatically linked to the 'st4_data' volume so the rest of the individual containers can pick it up. 

The 'env_file' is how AWS credentials are injected at Runtime.  The GitHub Actions CI/CD builds the docker-compose environment daily and creates an AWS runtime each day.  AWS credentials are stored securely in the GitHub repo.  I learned the hard way it's a bad idea to layer credentials into you Docker Image that lives on a repository.

Another note, when you want to hardwire some assets into your image, for example shapefiles or a .csv, you can't place them in a folder named /data and then link your shared volume to that path: - st4_data:/home/data.  It will overwrite it.  When hardwiring assets in container keep them separate from shared volumes.

```{r}
#| eval: false
#| echo: true

services:
  
  scraper:
    image: cfurl/stg4_24hr_scraper:v1
    container_name: scrape24
    volumes:
      - st4_data:/home/data

  wgrib2:
    image: cfurl/sondngyn_wgrib2:v1
    container_name: wgrib2
    command: "wgrib2_commands.sh"
    depends_on:
      scraper:
        condition: service_completed_successfully
    volumes: 
      - st4_data:/srv/
      - st4_data:/opt/    
      
  parqs3:
    image: cfurl/stg4_24hr_parq_s3:v1
    container_name: parqs3
    depends_on:
      wgrib2:
        condition: service_completed_successfully
    volumes:
      - st4_data:/home/data
    env_file:
      - .env

volumes:
  st4_data:

```


Here is the list of containers chained together in the docker-compose.yml all live on Docker Hub repository:

1. cfurl/stg4_24hr_scraper:v1
2. cfurl/sondngyn_wgrib2:v1
3. cfurl/stg4_24hr_parq_s3:v2
4. cfurl/stg4_24hr_daily_stat_update:v1
5. cfurl/stg4_24hr_daily_make_map:v1
6. cfurl/stg4_24hr_daily_make_hyet:v1

The docker-compose.yml gets triggered each day via Github Actions with the standard 'docker-compose up' command. Github Actions is controlled by a yml file placed in .github/workflows (path has to be named this for Actions to work).  There I have my compose-workflow.yml.  This took a long time to set up prior to ChatGPT, but the basic steps are you tell it when to fire:

```{r}
#| eval: false
#| echo: true

on:
  schedule:
    - cron: '45 13 * * *'  # Runs daily at 13:45 UTC
  workflow_dispatch:       # Also allow manual triggering

```

then a series of commands that install Docker in the virtual environment, assign AWS credentials, and lastly run 'docker-compose up' which starts the docker-compose.yml in the root directory. Important that the AWS credentials are managed through Github via the repository, and only injected at runtime. I still to this day have never added a new imaage to the compose workflow and had Github Actions run it successfully. Lot's of debugging by examining the logs of your 'Actions' tab in the Github repository.  It's nice to drop some checkin points in your scripts in the containers to print 'sucessfully this far' to help know where you're crashing. 

### AWS S3 and Apache Arrow/.parq

The last 4 containers all interact with AWS S3 storage buckets.  I have 7 buckets right now on S3, here is what they contain, later with individual scripts I'll describe what writes to them:

1. stg4-edwards-24hr-historical: parquet files of stg4 data from 2002-present across EAR.
2. stg4-edwards-daily-maps: storage for .png maps generated daily (hyet, 24hr, ytd)
3. stg4-edwards-daily-stats-24hr: parquet of daily basin averaged precipitation for all 10 subbasins
4. stg4-edwards-latest: storage for most recent .png maps generated daily (hyet, 24hr, ytd). Only ever holds maps for most recent day.
5. stg4-texas-24hr: parquet files of stg4 data from 2002-present across Texas.
6. stg4-texas-24hr-backup: ChatGPT assisted 'snapshot' of "stg4-texas-24hr"
7. stg4-texas-24hr-historical: parquet files of stg4 data from 2002-present across Texas.

Arrow

### Images

#### Image 1 - scraper - cfurl/stg4_24hr_scraper:v1

This container takes the current system date and time, builds the appropriate NOMADS url based off of date and time, downloads the 24hr GRIB2 conus file at that url, and writes a shell script based off of url filename that is used in the next container to inject prompts to wgrib2.exe. Both the shell script and the GRIB2 file reside in a shared ephemeral storage volume that all containers are linked to.

External writes: none

Potential weaknesses: I wrote this script years ago.  I should standardize the way way in interact with system time between all images because i do it frequently.  Main problem though is if the script is run at night time  between (0 - 12:55 UTC) before the most recent stage4 drops the whole thing fails.  This hasn't been a problem becasue my chron job starts at 13:55, but it'w worth considering.

#### Image 2 - wgrib2 - cfurl/sondngyn_wgrib2:v1

I took this image from sondngyn and packaged it in my own repository so it won't go away.  This is just wgrib2.exe built into a container.  I execute my shell file written with the first container that supplies the commands to wgrib2.exe.  The output binary to .txt format for the radar data is captured in the shared volume.

External writes: none

Potential weaknesses:  I need to learn how to use Pygrib when i start working with other products.  The encapsulation of the stg4 product in the GRIB2 file format is dead simple.  I just export to text.  File sizes cooperate and there aren't a lot of complications of what's in the GRIB2 file.  If i start to work with other products like HRRR, GFS, etc I need to undertand details of how to chunk data, access specific layers etc.

#### Image 3 - parqs3 - cfurl/stg4_24hr_parq_s3:v2 

This docker container reads the .txt dump of radar data, tidy's the GRIB2 .txt output, clips to area of interest (Texas and EAR), and then writes .parquet files that are partitioned by (year, month, day) to three S3 buckets.  

A note, you would probably have to partition by hour if you do this hourly, because you can't append a .parquet file to existing data because of how it is written on the disk.  Your partitions have to be separate when you are doing this in an automated fashion.

External writes: "stg4-texas-24hr"; "stg4-texas-24hr-historical"; "stg4-edwards-24hr-historical"

Potential weaknesses:  You left_join your .csv of of radar bins that are in the Texas buffer with the individual radar bins for that day of data. You are joining by 'lat' and 'lon', so you have to make sure that those are unique to each other down to the decimal point so you don't throw and NA.  Seems to be working fine, but i recall some pain when working through the grid.  A note on parquet files, you would probably have to partition by hour if you do this hourly, because you can't append a .parquet file to existing data because of how it is written on the disk - it will overwrite.  Your partitions have to be separate when you are doing this in an automated fashion.


#### Image 4 - dailystat - cfurl/stg4_24hr_daily_stat_update:v1

This container pulls radar data from the most recent day (written just before in container 3) and calculates basin averaged precipitation for the 10 subbasins.  Within the image, "/home/gis" I have .csv files hardwired that describe area of each radar bin for every basin. Basin avg precip is calculated by solving for volume of water in each radar bin, summing, and then dividing by total area of basin.  

External writes: "stg4-edwards-daily-stats-24hr"

Potential weaknesses:  Is it time saving to read from "stg4-edwards-24hr-historical" instead of "stg4-texas-24hr"?  why do i convert from mm to in in this script? 

#### Image 5 - makemap - cfurl/stg4_24hr_daily_make_map:v1

This image accesses "stg4-texas-24hr" S3 bucket, and creates a tibble of the most recent daily rainfall across the EAA and the ytd across the EAA.  The most recent 24hr rainfall is shown on a map highlighting the stream network.  The ytd rainfall map shows the individual subbasin and minimizes  the stream network.  Both maps contain a semi-transparent table in the upper righthand corner of the figure.  The legend in the ytd map is dynamic and will scale according to rainfall amounts (ie time of year).  I'm curious to see how maps look in january with little rainfall.  The 24hr map legend is static.  Both maps have assigned/locked color bins.  The maps are written to a long-term s3 repository with the date captured in the name and a bucket that contains only the most recent map.  The "stg4-edwards-latest" bucket has pretty tight cache control so an individual viewing that link won't cache it and see a previous rainfall map.

External writes: "stg4-edwards-daily-maps"; "stg4-edwards-latest"

Potential weaknesses:  Is it time saving to read from "stg4-edwards-24hr-historical" instead of "stg4-texas-24hr" - probably so?  It would make your left_join(rain_24hr, by = "grib_id")|> go way faster.  

#### Image 6 - makehyet - cfurl/stg4_24hr_daily_make_hyet:v1

This one access the daily stats written in container 3 and creates a hyetograph for each of the 10 subbasins.  The ribbons are controlled by hard-wired statistics built from 2002-2024, these need to be updated manually at end of year.  .png files get placed in the same location as the map files. 

External writes: "stg4-edwards-daily-maps"; "stg4-edwards-latest"

