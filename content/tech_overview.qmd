---
title: Tech Overview
---
## High-level workflow 

The rainfall observations are brought to numerical and graphical format from the streaming repository to .html front end through a series of Docker Containers.  The containers live on [cfurl Docker Hub](https://hub.docker.com/u/cfurl) and orchestration is accomplished through Docker Compose.  Radar file output and graphs are stored on AWS S3.  The CI/CD pipeline runs daily using GitHub Actions.

Below is a brief description of the daily workflow and and some highlights of the technologies used.    

1. 24-hour GRIB2 radar file available on NOMADS at 12:55 UTC daily.  
2. GitHub Actions cron job is triggered at ~ 13:15 to setup Docker Compose env, AWS env, begin container series.
3. Container 1 - scrapes the GRIB2 file from NOMADS site and writes a shell script to execute container 2. 
4. Container 2 - runs wgrib2.exe taking radar data out of binary format to .txt.
5. Container 3 - tidy data, clip to AOI, and write .parquet files to AWS s3 bucket for Texas and EAA areas.
6. Container 4 - access S3 radar data and calculate basin avg precip on areas of interest.
7. Container 5 - Publish maps to long-term repository and 'latest' buckets
8. Container 6 - Publish hyetographs to long-term repository and 'latest' buckets
9. Posit Cloud Connect hosts simple website that displays .png files in the latest bucket.  These are rewritten daily.  


## Docker

Docker has been a game changer.  It takes reproducible, portable code to a new level. In short, you write code as usual, then package it into an image that includes the exact language runtime, system libraries, packages, and any required assets (e.g., shapefiles). That image is a portable, versioned artifact that runs the same everywhere. You push it to a registry (Docker Hub, AWS ECR, or a private repo), and whenever you need it, you pull the image and start a container from that image.  This results in a clean, repeatable run every single time. This kills environment drift and the “works on my machine” problem. You can make the containers are lightweight, multiple containers can be linked to form pipelines, and you can mount volumes for persistent data. My image repository is on Docker Hub here: https://hub.docker.com/repositories/cfurl. A benefit of the repository is you can pull other peoples containers.  There is a lot of stuff out there, it appears people have containerized MODFLOW, SWAT, HSPF.  There are containers for various AI frameworks, etc. I learned from 'Docker in a Month of Lunches' by Elton Stoneman. 

## wgrib2.exe

wgrib2.exe is a command-line utility for working with GRIB2 meteorological data. It reads GRIB2 files, inspects metadata, and produces concise inventories of the messages they contain. It is widely used to filter and extract variables (e.g., precipitation, temperature, wind), select time steps and vertical levels, and subset by geographic region. The tool can regrid or resample fields to different projections or resolutions, apply standard interpolations, and perform basic calculations. It also supports format conversion, writing outputs to human-readable text/CSV and, when built with the appropriate libraries, to NetCDF. Designed for speed and scripting, it scales well for batch processing and automated workflows on large archives. The “.exe” denotes the Windows build; equivalent functionality is available on Linux and macOS as wgrib2.  It is maintained on github by NOAA here: https://github.com/NOAA-EMC/wgrib2/releases

## Parquet and Apache Arrow

I learned about Apache Parquet and the Arrow R package at the Posit “Big Data with R” workshop, and they’ve been game-changers for radar data. The short of it is you get in-memory processing speeds for data sets that so large they are only held on disk. Parquet is a columnar, compressed, splittable file format built for analytics: it stores columns together, so reads are fast and selective (you only scan the columns and row groups you need), and files are much smaller than CSV. Apache Arrow provides a columnar in-memory format and tooling that lets R (and other languages) scan Parquet lazily, push filters down to disk, and stream data in chunks—so you can work with datasets far larger than RAM instead of “loading everything, then filtering.” In practice, that means querying and summarizing hundreds of gigabytes on a laptop like they were loaded in-memory, especially when data are partitioned (e.g., by year/month/day) and stored locally or on S3. For analytics at scale, Parquet + Arrow has effectively replaced CSV for me: smaller, faster, and designed for selective reads—exactly what large radar archives demand.

## S3

Amazon S3 is durable, scalable object storage used to hold files of any size with high availability and pay-as-you-go pricing. It’s a common backbone for “data lake” workflows: tools read/write directly via s3://… paths, and in R, aws.s3 and Arrow support streaming/lazy access without local copies. Access control is handled with IAM; data governance via versioning, lifecycle policies, and server-side encryption. It’s low-maintenance, widely supported, and integrates cleanly with modern analytics stacks. I've used this tutorial regularly to read/write my S3's: https://www.gormanalysis.com/blog/connecting-to-aws-s3-with-r/

## Github Actions

GitHub Actions is GitHub’s built-in CI/CD platform that runs workflows on events (push, PR), schedules (cron), or manual dispatch. Workflows are defined as YAML files stored with your code, so versioning and reviews are straightforward. Runners can execute shell steps, build and run Docker containers (including docker-compose), and orchestrate multi-step jobs across OS targets. Sensitive values are injected at runtime via Repository/Environment Secrets (e.g., AWS credentials), avoiding hard-coding in images or source.  This injection of secrets at runtime has saved me a lot of grief. Typical uses include scheduled data jobs, automated tests/linting, container builds and publishes, and static-site deploys (e.g., rebuilding documentation or a homepage on a timer). It’s a practical way to automate repeatable tasks directly from the repo without managing separate infrastructure—and can later hand off to a cloud-native backend if needed.

## posit.connect.cloud

Posit Connect (cloud-managed) is a production publishing platform for R/Python: it hosts Shiny apps, Quarto/R Markdown sites, and APIs (e.g., Plumber) with HTTPS, auth, and versioned deploys. It handles builds and dependencies, environment variables/secrets, scheduled jobs (e.g., daily refresh), and integrates cleanly with Git/GitHub Actions for push-to-deploy. You get logs, usage metrics, access controls/SSO, and simple scaling (multiple processes/workers) without running servers. It’s a fast way to deliver interactive analytics now, with a clear path to containerized hosting on AWS later (ECS/Fargate, EKS, or EC2) using the same CI/secrets patterns.



