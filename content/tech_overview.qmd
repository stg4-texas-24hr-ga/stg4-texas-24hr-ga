---
title: How it works
---
## Workflow overview 

Let's start with a very high level overview of the processes occurring, then we'll briefly chat about some of the main technologies used.  The details page has links to the github repos and further discussion of what is happening at each step.

1. 24-hour GRIB2 radar file available on NOMADS at 12:55 UTC.  
2. GitHub Actions cron job is triggered to start a series of docker containers.
3. Container 1 - scrapes the GRIB2 file from NOMADS site 
4. Container 2 - GRIB2 file taken out of binary and dumps txt
5. Container 5 - organize rain data, clip to AOI, and writes .parquet files to AWS S3
6. Container orchestration done through Docker compose - AWS secrets injected at runtime with Github Actions
7. Daily rainfall mapped with an R Shiny app and hosted through connect.posit.cloud
8. Map is updated daily through an empty github commit to connect orchestrated with Github Actions


## Docker

Docker has been the most influential piece of tech I've picked up since learning a scripting language.  The basics as I can describe it are you write a piece of code like you normally would and then are able to package that code with everything it needs to work - specific r build, packages to install, shapefiles or other data to inject, and then the entire functioning piece of your code lives in an image and works like it would on your normal local environment.  You can host this image on a repository (docker hub, AWS ECR, local repository) then everytime you want to run the code a container is spun up from the image.  This takes repeatability to a new level.  No more messing with environments when sharing code or hosting on different environments, the code and its processes are held in the container and are bullet proof.  Additionally, you can link containers together, attach data storage volumes to them all while being super lightweight.  My docker hub repository is https://hub.docker.com/repositories/cfurl.  I very much enjoyed learning from Docker in a month of lunches by Elton Stoneman.  

## wgrib2.exe


## Parquet and Apache Arrow

I picked up the .parquet file structure and Apache Arrow r package from a workshop I took at the POSIT conference in Seattle - Big data with R.  Working with radar files had always been a grind during the PhD years and after due to file sizes you run into.  Dragging some massive amount of data into your memory then manipulating it really limited you to maybe 10G worth of data which is not hard to get to with data coming off radar.  Apache Arrow + .parquet solves this issue by allowing you to query and script on data while it resides on disk and is held out of memory.  I'm the wrong guy to ask about the details, but you can query and manipulate hundreds of gigs of data at a time in a time efficient manner with a simple laptop.  Parquet is the only way I'll keep data files of any size now.  Csv is dead and big data problems are rapidly becoming dead.

## S3

There isn't really anything special here or groundbreaking, just had not used S3 storage before.  Since the data collected, manipulated, and displayed automatically I had to pick a cloud storage.  This is seems to be pretty pain free.  aws.s3 and apache arrow are pre-built to work with it, and I've had almost zero trouble spinning it up and maintaining it after some initial AWS woes.

## Github Actions

Github actions allows you to automatically run code on a variety of triggers including a cron trigger.  This has been a very nice 'training-wheels' tool for me to use to execute code while I sleep.  As I'll discuss later in the 'next steps' I want to move this to an all AWS backend like a real developer would (making excellent progress), but for the time being this works.  It's nice because you can manage the .yml's right there in github where you are maintaining your code, run my container orchestration with docker compose, and very importantly i can inject AWS credentials at run-time with repository secrets.  Went through some pain here trying to hide them in docker images and other tricks.  I also use it to republish this guide everyday after the new radar image drops that you see on the 'Home' page.

## posit.connect.cloud

I've spent less than a day building the rshiny app you see here.  Posit.cloud.connect was super easy to spin up, had a way to manage AWS credentials, and is able to auto-update daily with github actions.  In the future I think the the shiny app (or some other web application) should be held in a container and hosted on AWS. That's more stuff to learn and inthe meantime I can deliver rainfall maps and metrics quickly to end users with Connect.



