[
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "Pirate Weather learned an absolute ton about proper frameworks from this fella.\nKEWX Station\nr4ds\nDocker in a month of lunches\nHappy git with R\nArrow R Package\nNOAA Quarto website\nIowa State Mesonet"
  },
  {
    "objectID": "content/backend.html",
    "href": "content/backend.html",
    "title": "Images and Containers",
    "section": "",
    "text": "Stage IV data are packaged as GRIB2 files are available each in 1-hr, 6-hr, and 24-hr files. The 6 and 24 hour files are summed from the one hour files. Presently, this tutorial works with a 24-hr file and scrapes data once a day. It would not be difficult to ammend the code and start the workflow every hour.\nStage IV data are available in near real-time on NOMADS at :55 past the hour. For example, hourly rainfall ending 2:00 (representing 1:00-2:00) are available at 2:55. Twenty-four hour data are available from 12:00 - 12:00 UTC (available at 12:55 UTC). Data for the last 14 days are kept on NOMADS after which they are cycled off. The real-time NOMADS site is available here. Note, there are .01h, .06hr, and .24hr files for CONUS, pr (Puerto Rico), and ak (Alaska).\nAfter the near real-time file drop, an additional rerun at 30h after valid time (18:55Z) is available to supplement 24-hr mosaics if RFCs need to update their QPEs or make changes. Personal communication with the WGRFC indicates that this is done very infrequently.\nSince at least 2012 when I started tinkering with the archived Stage IV data, the historical datasets were kept at data.eol.ucar. As of summer 2025, the data have been moved to rda.ucar.edu. At this new archive location they are tarring together Stage IV data on monthly increments. The data appear to be coming available around the 20th of the next month (for example August data available ~ September 20th).\nI intend to add a page to this website describing my manual methods for processing Stage IV data. Currently, focused on real-time description.\n\n\n\nAs previously described, Stage IV data are stored in GRIB2 binary format. Stage IV precipitation has been packaged in both the GRIB1 format (until July 2020) and currently resides in the GRIB2 format. The processing procedures and wgrib.exe utility are completely different. When I get to the descritption of building a historical archive at a site I’ll have to cover GRIB1 processing.\n\n\n\nThe workflow is accomplished by linking containers together using a docker compose orchestration and tripping off the series of containers with a github actions cron job.\nContainer orchestration and Github Action workflows are located in the following repo: Container Orchestration\nThe containers are managed by a docker-compose.yml shown below which describes when to spin up each container, how to manage storage, environments etc. The docker-compose.yml is stored in the root folder of the github repo shown above.\n\n#docker-compose.yml - keep in root folder of your github actions repo\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \n  parqs3:\n    image: cfurl/stg4_24hr_parq_s3:v1\n    container_name: parqs3\n    depends_on:\n      wgrib2:\n        condition: service_completed_successfully\n    volumes:\n      - st4_data:/home/data\n    env_file:\n      - .env\n\nvolumes:\n  st4_data:\n\nThe docker-compose.yml gets triggered each day via Github Actions - described later. Each of the individual container images are stored on the docker hub repository at: hub.docker.cfurl. These should all be public. The workflow for processing the Stage IV data are contained in 3 individual containers described below.\n\n\n\nThe Dockerfile, code, and explicit Powershell prompts to create the Docker Image and load onto Docker Hub are contained in the following repo: Docker Images-Containers\nBefore I get to the scripts held in each individual container I want to take a quick look at how images are constructed. You simply give docker a set of instructions in your ‘Dockerfile’ and then build it through powershell commands. AFter it’s built, you can upload to your repository.\n\n# Dockerfile\n\nFROM rocker/r-ver:4.2.2\n\nRUN mkdir -p /home\nRUN mkdir -p /home/code\nRUN mkdir -p /home/data\n\nWORKDIR /home\n\nCOPY /code/write_parq_2_s3.R /home/code/write_parq_2_s3.R\nCOPY /code/install_packages.R /home/code/install_packages.R\nCOPY /data/texas_buffer_spatial_join.csv /home/data/texas_buffer_spatial_join.csv\n\nRUN Rscript /home/code/install_packages.R\n\nCMD Rscript /home/code/write_parq_2_s3.R\n\nBelow is are brief explanations of what each container accomplishes.\n\n\nThis code takes the current system date and time, visits the appropriate NOMADS url based off of date and time, downloads the 24hr GRIB2 conus file at that url, writes a shell script that is used in the next container to inject prompts to wgrib2.exe, and stores both the shell script and the actual GRIB2 file in a shared storage volume (controlled through docker-compose.yml) that all three containers are linked to.\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(rvest)\n\n# create function to print out UTC time with utc_time()\nnow_utc &lt;- function() {\n  now &lt;- Sys.time()\n  attr(now, \"tzone\") &lt;- \"UTC\"\n  now\n}\n\n# create character string of the hour\nhour_char&lt;-str_sub(as.character(now_utc()),start=12,end=13)\n# create numeric hour\nhour_num&lt;-as.numeric(hour_char)\n\n# create character string for date\ndate_char&lt;- str_sub(as.character(now_utc()),start=1,end=10) %&gt;% str_remove_all(\"-\")\n# create dateclass date with utc timezone\nnow_utc_date&lt;-as.Date(now_utc(),tz=\"UTC\")\n\n\n# read nomads stg4 html page using date from utc_time()\nstg4_http_page&lt;-read_html(paste0(\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/pcpanl.\",date_char,\"/\"))\n\n# find only files that end with .grb2 and have 'pcp' somewhere in the string\ngrib2_available &lt;- stg4_http_page %&gt;%\n  html_elements(\"a\") %&gt;%\n  html_text() %&gt;%\n  str_subset(\"conus\") %&gt;%\n  str_subset(\"24h.grb2$\") \n\n# create path to download\nsource_path&lt;-paste0(\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/pcpanl.\",date_char,\"//\",tail(grib2_available,n=1))\n\n# create download destination\ndestination_path&lt;-paste0(\"/home/data/\",tail(grib2_available,n=1))\n\n#download the file  \ndownload.file (source_path,destination_path,method = \"libcurl\")\n\n# Write your shell file to communicate with the wgrib2 container\ntxt&lt;- paste(\"wgrib2\", tail(grib2_available,n=1), \"-csv\",  str_replace(tail(grib2_available,n=1), \".grb2\", \".txt\"))\nwriteLines(txt,paste0(\"/home/data\",\"/wgrib2_commands.sh\"))\n\n\n\n\nThis code simply executes the wgrib2.exe application and takes the data from the binary format and dumps as text. This works in this case since the only thing wrapped in this GRIB2 file is stage4 rainfall. In many meteorological GRIBs there are many variables. The .sh shell file written in container one contain the instructions that are given to the wgrib2.exe applicatoin. This is controlled through docker-compose.yml, namely: “command:”wgrib2_commands.sh”“. The text output which is a bunch of lat/lons and rain values are stored in the same shared volume as the output files from Container 1.\nI didn’t place the wgrib.exe in an image. There are plenty of images available on docker hub where someone has already done this. I’ve been using the image located here: sondngyn/wgrib2:latest. Below I show the docker commands to copy that image into my repository so I’m not vulnerable to changes in the sondngyn hu repo.\n\nSimple pull, retag, push:\n\n# pull upstream\ndocker pull sondngyn/wgrib2:latest\n\n# retag to your namespace\ndocker tag sondngyn/wgrib2:latest cfurl/sondngyn_wgrib2:v1\n\n# login and push\ndocker login\ndocker push cfurl/sondngyn_wgrib2:v1\n\n# Test it with docker-compose.yml:\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    networks:\n      - some_name\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \nnetworks:\n  some_name:\n    external:\n        name: st4_net\n        \nvolumes:\n  st4_data:\n\n\n\n\nThis docker container connects to your “stg4-texas-24hr” AWS S3 bucket, tidy’s the GRIB2 csv output, clips to area of interest (Texas), and then writes .parquet files that are partitioned by (year, month, day). A note, you would probably have to partition by hour if you do this hourly, because you can’t append a .parquet file to existing data because of how it is written on the disk. Your partitions have to be separate when you are doing this in an automated fashion.\n\n\n\n\nGitHub Actions is a CI/CD platform integrated into GitHub. It runs workflows defined in YAML to build, test, and deploy code on events like pushes, pull requests, or schedules. Jobs run on hosted runners or self-hosted machines, support matrices, secrets, caching, and reusable actions, enabling reliable pipelines across repositories globally.\nGithub Actions\n\nlibrary(\"aws.s3\")\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyr\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\n\n# make sure you can connect to your bucket and open SubTreeFileSystem\nbucket &lt;- s3_bucket(\"stg4-texas-24hr\")\n\n# list everything in your bucket in a recursive manner\nbucket$ls(recursive = TRUE)\n\n# identify path where you will be writing the .parq files\ns3_path &lt;- bucket$path(\"\")\n\naoi_texas_buffer&lt;-read_csv(\"/home/data/texas_buffer_spatial_join.csv\")\n\n# list files that start with st4 and ends with .txt\nraw_grib2_text = list.files(\"/home/data\", pattern = \"^st4_conus.*.txt$\",full.names=FALSE)\n\nfor (h in raw_grib2_text) {\n  name &lt;- h |&gt;\n    str_replace(\"st4_conus.\", \"t\") |&gt;\n    str_replace(\".24h.txt\",\"\")\n  \n  aa&lt;-read_csv(paste0(\"/home/data/\",h), col_names=FALSE) %&gt;%\n  #aa&lt;-read_csv(h, col_names=FALSE) %&gt;%\n    setNames(c(\"x1\",\"x2\",\"x3\",\"x4\",\"center_lon\",\"center_lat\",name)) %&gt;%\n    select(-x1,-x2,-x3,-x4)   \n  \n  # joins by \"center_lon\", \"center_lat\"\n  bb&lt;- left_join(aoi_texas_buffer,aa,by=NULL)%&gt;%\n    pivot_longer(!1:5, names_to = \"time\", values_to = \"rain_mm\") %&gt;%\n    mutate(time = ymd_h(str_sub(time,2,11))) %&gt;%\n    mutate (year = year(time), month = month(time), day = day(time), hour = hour(time)) %&gt;%\n    relocate(rain_mm, .after = last_col()) \n}  \n\nbb|&gt;\n  group_by(year,month,day) |&gt;\n  write_dataset(path = s3_path,\n                format = \"parquet\")"
  },
  {
    "objectID": "content/backend.html#the-details",
    "href": "content/backend.html#the-details",
    "title": "Images and Containers",
    "section": "",
    "text": "Stage IV data are packaged as GRIB2 files are available each in 1-hr, 6-hr, and 24-hr files. The 6 and 24 hour files are summed from the one hour files. Presently, this tutorial works with a 24-hr file and scrapes data once a day. It would not be difficult to ammend the code and start the workflow every hour.\nStage IV data are available in near real-time on NOMADS at :55 past the hour. For example, hourly rainfall ending 2:00 (representing 1:00-2:00) are available at 2:55. Twenty-four hour data are available from 12:00 - 12:00 UTC (available at 12:55 UTC). Data for the last 14 days are kept on NOMADS after which they are cycled off. The real-time NOMADS site is available here. Note, there are .01h, .06hr, and .24hr files for CONUS, pr (Puerto Rico), and ak (Alaska).\nAfter the near real-time file drop, an additional rerun at 30h after valid time (18:55Z) is available to supplement 24-hr mosaics if RFCs need to update their QPEs or make changes. Personal communication with the WGRFC indicates that this is done very infrequently.\nSince at least 2012 when I started tinkering with the archived Stage IV data, the historical datasets were kept at data.eol.ucar. As of summer 2025, the data have been moved to rda.ucar.edu. At this new archive location they are tarring together Stage IV data on monthly increments. The data appear to be coming available around the 20th of the next month (for example August data available ~ September 20th).\nI intend to add a page to this website describing my manual methods for processing Stage IV data. Currently, focused on real-time description.\n\n\n\nAs previously described, Stage IV data are stored in GRIB2 binary format. Stage IV precipitation has been packaged in both the GRIB1 format (until July 2020) and currently resides in the GRIB2 format. The processing procedures and wgrib.exe utility are completely different. When I get to the descritption of building a historical archive at a site I’ll have to cover GRIB1 processing.\n\n\n\nThe workflow is accomplished by linking containers together using a docker compose orchestration and tripping off the series of containers with a github actions cron job.\nContainer orchestration and Github Action workflows are located in the following repo: Container Orchestration\nThe containers are managed by a docker-compose.yml shown below which describes when to spin up each container, how to manage storage, environments etc. The docker-compose.yml is stored in the root folder of the github repo shown above.\n\n#docker-compose.yml - keep in root folder of your github actions repo\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \n  parqs3:\n    image: cfurl/stg4_24hr_parq_s3:v1\n    container_name: parqs3\n    depends_on:\n      wgrib2:\n        condition: service_completed_successfully\n    volumes:\n      - st4_data:/home/data\n    env_file:\n      - .env\n\nvolumes:\n  st4_data:\n\nThe docker-compose.yml gets triggered each day via Github Actions - described later. Each of the individual container images are stored on the docker hub repository at: hub.docker.cfurl. These should all be public. The workflow for processing the Stage IV data are contained in 3 individual containers described below.\n\n\n\nThe Dockerfile, code, and explicit Powershell prompts to create the Docker Image and load onto Docker Hub are contained in the following repo: Docker Images-Containers\nBefore I get to the scripts held in each individual container I want to take a quick look at how images are constructed. You simply give docker a set of instructions in your ‘Dockerfile’ and then build it through powershell commands. AFter it’s built, you can upload to your repository.\n\n# Dockerfile\n\nFROM rocker/r-ver:4.2.2\n\nRUN mkdir -p /home\nRUN mkdir -p /home/code\nRUN mkdir -p /home/data\n\nWORKDIR /home\n\nCOPY /code/write_parq_2_s3.R /home/code/write_parq_2_s3.R\nCOPY /code/install_packages.R /home/code/install_packages.R\nCOPY /data/texas_buffer_spatial_join.csv /home/data/texas_buffer_spatial_join.csv\n\nRUN Rscript /home/code/install_packages.R\n\nCMD Rscript /home/code/write_parq_2_s3.R\n\nBelow is are brief explanations of what each container accomplishes.\n\n\nThis code takes the current system date and time, visits the appropriate NOMADS url based off of date and time, downloads the 24hr GRIB2 conus file at that url, writes a shell script that is used in the next container to inject prompts to wgrib2.exe, and stores both the shell script and the actual GRIB2 file in a shared storage volume (controlled through docker-compose.yml) that all three containers are linked to.\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(rvest)\n\n# create function to print out UTC time with utc_time()\nnow_utc &lt;- function() {\n  now &lt;- Sys.time()\n  attr(now, \"tzone\") &lt;- \"UTC\"\n  now\n}\n\n# create character string of the hour\nhour_char&lt;-str_sub(as.character(now_utc()),start=12,end=13)\n# create numeric hour\nhour_num&lt;-as.numeric(hour_char)\n\n# create character string for date\ndate_char&lt;- str_sub(as.character(now_utc()),start=1,end=10) %&gt;% str_remove_all(\"-\")\n# create dateclass date with utc timezone\nnow_utc_date&lt;-as.Date(now_utc(),tz=\"UTC\")\n\n\n# read nomads stg4 html page using date from utc_time()\nstg4_http_page&lt;-read_html(paste0(\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/pcpanl.\",date_char,\"/\"))\n\n# find only files that end with .grb2 and have 'pcp' somewhere in the string\ngrib2_available &lt;- stg4_http_page %&gt;%\n  html_elements(\"a\") %&gt;%\n  html_text() %&gt;%\n  str_subset(\"conus\") %&gt;%\n  str_subset(\"24h.grb2$\") \n\n# create path to download\nsource_path&lt;-paste0(\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/pcpanl.\",date_char,\"//\",tail(grib2_available,n=1))\n\n# create download destination\ndestination_path&lt;-paste0(\"/home/data/\",tail(grib2_available,n=1))\n\n#download the file  \ndownload.file (source_path,destination_path,method = \"libcurl\")\n\n# Write your shell file to communicate with the wgrib2 container\ntxt&lt;- paste(\"wgrib2\", tail(grib2_available,n=1), \"-csv\",  str_replace(tail(grib2_available,n=1), \".grb2\", \".txt\"))\nwriteLines(txt,paste0(\"/home/data\",\"/wgrib2_commands.sh\"))\n\n\n\n\nThis code simply executes the wgrib2.exe application and takes the data from the binary format and dumps as text. This works in this case since the only thing wrapped in this GRIB2 file is stage4 rainfall. In many meteorological GRIBs there are many variables. The .sh shell file written in container one contain the instructions that are given to the wgrib2.exe applicatoin. This is controlled through docker-compose.yml, namely: “command:”wgrib2_commands.sh”“. The text output which is a bunch of lat/lons and rain values are stored in the same shared volume as the output files from Container 1.\nI didn’t place the wgrib.exe in an image. There are plenty of images available on docker hub where someone has already done this. I’ve been using the image located here: sondngyn/wgrib2:latest. Below I show the docker commands to copy that image into my repository so I’m not vulnerable to changes in the sondngyn hu repo.\n\nSimple pull, retag, push:\n\n# pull upstream\ndocker pull sondngyn/wgrib2:latest\n\n# retag to your namespace\ndocker tag sondngyn/wgrib2:latest cfurl/sondngyn_wgrib2:v1\n\n# login and push\ndocker login\ndocker push cfurl/sondngyn_wgrib2:v1\n\n# Test it with docker-compose.yml:\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    networks:\n      - some_name\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \nnetworks:\n  some_name:\n    external:\n        name: st4_net\n        \nvolumes:\n  st4_data:\n\n\n\n\nThis docker container connects to your “stg4-texas-24hr” AWS S3 bucket, tidy’s the GRIB2 csv output, clips to area of interest (Texas), and then writes .parquet files that are partitioned by (year, month, day). A note, you would probably have to partition by hour if you do this hourly, because you can’t append a .parquet file to existing data because of how it is written on the disk. Your partitions have to be separate when you are doing this in an automated fashion.\n\n\n\n\nGitHub Actions is a CI/CD platform integrated into GitHub. It runs workflows defined in YAML to build, test, and deploy code on events like pushes, pull requests, or schedules. Jobs run on hosted runners or self-hosted machines, support matrices, secrets, caching, and reusable actions, enabling reliable pipelines across repositories globally.\nGithub Actions\n\nlibrary(\"aws.s3\")\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyr\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\n\n# make sure you can connect to your bucket and open SubTreeFileSystem\nbucket &lt;- s3_bucket(\"stg4-texas-24hr\")\n\n# list everything in your bucket in a recursive manner\nbucket$ls(recursive = TRUE)\n\n# identify path where you will be writing the .parq files\ns3_path &lt;- bucket$path(\"\")\n\naoi_texas_buffer&lt;-read_csv(\"/home/data/texas_buffer_spatial_join.csv\")\n\n# list files that start with st4 and ends with .txt\nraw_grib2_text = list.files(\"/home/data\", pattern = \"^st4_conus.*.txt$\",full.names=FALSE)\n\nfor (h in raw_grib2_text) {\n  name &lt;- h |&gt;\n    str_replace(\"st4_conus.\", \"t\") |&gt;\n    str_replace(\".24h.txt\",\"\")\n  \n  aa&lt;-read_csv(paste0(\"/home/data/\",h), col_names=FALSE) %&gt;%\n  #aa&lt;-read_csv(h, col_names=FALSE) %&gt;%\n    setNames(c(\"x1\",\"x2\",\"x3\",\"x4\",\"center_lon\",\"center_lat\",name)) %&gt;%\n    select(-x1,-x2,-x3,-x4)   \n  \n  # joins by \"center_lon\", \"center_lat\"\n  bb&lt;- left_join(aoi_texas_buffer,aa,by=NULL)%&gt;%\n    pivot_longer(!1:5, names_to = \"time\", values_to = \"rain_mm\") %&gt;%\n    mutate(time = ymd_h(str_sub(time,2,11))) %&gt;%\n    mutate (year = year(time), month = month(time), day = day(time), hour = hour(time)) %&gt;%\n    relocate(rain_mm, .after = last_col()) \n}  \n\nbb|&gt;\n  group_by(year,month,day) |&gt;\n  write_dataset(path = s3_path,\n                format = \"parquet\")"
  },
  {
    "objectID": "content/backend.html#github-actions-1",
    "href": "content/backend.html#github-actions-1",
    "title": "Images and Containers",
    "section": "Github Actions",
    "text": "Github Actions\nGithub actions is what makes the automation work. The docker-compose.yml is fired by the compose-workflow.yml. This compose-workflow.yml has to be helf in a folder called ‘.github/workflows’. AWS credentials are managed through Github via the repository. AWS credentials are injected at runtime through the actions .yml. The yml is shown below:\n\nname: stg4-texas-24hr-backend-actions\n\non:\n  schedule:\n    - cron: '45 13 * * *'  # Runs daily at 13:45 UTC\n  workflow_dispatch:       # Also allow manual triggering\n\njobs:\n  run-pipeline:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Install Docker using Docker's official script\n        run: |\n          curl -fsSL https://get.docker.com -o get-docker.sh\n          sudo sh get-docker.sh\n\n      - name: Install Docker Compose\n        run: |\n          sudo curl -L \"https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64\" -o /usr/local/bin/docker-compose\n          sudo chmod +x /usr/local/bin/docker-compose\n          docker-compose --version\n\n      - name: Create .env file with AWS credentials\n        run: |\n          echo \"AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}\" &gt;&gt; .env\n          echo \"AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}\" &gt;&gt; .env\n          echo \"AWS_REGION=${{ secrets.AWS_REGION }}\" &gt;&gt; .env\n\n      - name: Run Docker Compose (with AWS env)\n        run: docker-compose up\n        continue-on-error: false"
  },
  {
    "objectID": "content/backend.html#github-repositories-and-this-webpage",
    "href": "content/backend.html#github-repositories-and-this-webpage",
    "title": "Images and Containers",
    "section": "Github repositories and this webpage",
    "text": "Github repositories and this webpage\nI started a github organization called ‘stg4-texas-24hr-ga’ this allows me to segregate polished work from my messy personal github repository. Below are links to pertinent repositories.\n\nstg4-texas-24hr-ga organization\nGithub pages - this website\nDocker Image builds\nActions and Compose"
  },
  {
    "objectID": "content/frontend.html",
    "href": "content/frontend.html",
    "title": "Frontend",
    "section": "",
    "text": "Need to get a new repo going on my organizations page, get it off cfurl git repo, and provide a description of the shiny build and the automated publishing process."
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/roadmap.html",
    "href": "content/roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "To-do in the best order I can come up with:\nSmall things: 1. Build out the shiny app front end just a little more - add ytd precip maps, hyetograph, and table sub-basin rain calculations. 3. Add daily images of ytd rainfall and 24hour rainfall to your backend storage. 2. Add the historical data that you have on disk to your s3 bucket. Currently, you only hold data on AWS for Texas for 2025. 3. Figure out how to cache your map tiles to speed up shiny app 4. Document your manual processing flows when you load the 2002-2024 onto s3 bucket\nBig things: 1. Build out the backend on completely AWS framework. Cloudwatch -&gt; Fargate -&gt; S3 -&gt; EC2 hosted web app 2. Stand-up an hourly AWS real-time site 3. Get a well working surface water model, containerize it, and start forcing it hourly. 4. Add the RTMA product to your stack. 5. Start in on sub-hourly MRMS processing"
  },
  {
    "objectID": "content/stg4-product.html",
    "href": "content/stg4-product.html",
    "title": "Stage IV Precipitation",
    "section": "",
    "text": "The NCEP “Stage IV” precipitation product is a national, multi-sensor quantitative precipitation estimate (QPE) created by mosaicking the 12 NWS River Forecast Centers’ hourly/6-hourly Stage III analyses—each of which blends WSR-88D radar with rain-gauge observations and RFC forecaster quality control—onto a common ~4 km HRAP grid for CONUS (with extensions for Alaska and Puerto Rico). It is delivered as hourly, 6-hourly, and 24-hour accumulations, with the key distinction that Stage IV benefits from manual QC inherited from the RFC inputs, in contrast to automated-only Stage II.\nThe Stage IV data is a combination of observations and radar calculated reflectivity. Therefore the data combines the advantage of ground-truth provided by gauges with the spatially complete and high resolution radar data. Operationally, Stage IV is widely used for hydrologic verification, flood event forensics, and as precipitation forcing for models and water-resources studies. Data are distributed in GRIB (now GRIB2, after a 2020 format change) and the hourly product is available in near real-time via NOMADS.\n\n\n\nStg4 Conus output from NOMADS"
  },
  {
    "objectID": "content/stg4-product.html#overview",
    "href": "content/stg4-product.html#overview",
    "title": "Stage IV Precipitation",
    "section": "",
    "text": "The NCEP “Stage IV” precipitation product is a national, multi-sensor quantitative precipitation estimate (QPE) created by mosaicking the 12 NWS River Forecast Centers’ hourly/6-hourly Stage III analyses—each of which blends WSR-88D radar with rain-gauge observations and RFC forecaster quality control—onto a common ~4 km HRAP grid for CONUS (with extensions for Alaska and Puerto Rico). It is delivered as hourly, 6-hourly, and 24-hour accumulations, with the key distinction that Stage IV benefits from manual QC inherited from the RFC inputs, in contrast to automated-only Stage II.\nThe Stage IV data is a combination of observations and radar calculated reflectivity. Therefore the data combines the advantage of ground-truth provided by gauges with the spatially complete and high resolution radar data. Operationally, Stage IV is widely used for hydrologic verification, flood event forensics, and as precipitation forcing for models and water-resources studies. Data are distributed in GRIB (now GRIB2, after a 2020 format change) and the hourly product is available in near real-time via NOMADS.\n\n\n\nStg4 Conus output from NOMADS"
  },
  {
    "objectID": "content/stg4-product.html#additional-reading",
    "href": "content/stg4-product.html#additional-reading",
    "title": "Stage IV Precipitation",
    "section": "Additional reading",
    "text": "Additional reading\nPut your bibliography here"
  },
  {
    "objectID": "content/tech_overview.html",
    "href": "content/tech_overview.html",
    "title": "How it works",
    "section": "",
    "text": "24-hour GRIB2 radar file available on NOMADS at 12:55 UTC.\n\nGitHub Actions cron job is triggered at ~ 13:15 to start a series of docker containers.\nContainer 1 - scrapes the GRIB2 file from NOMADS site.\nContainer 2 - GRIB2 taken out of binary and dumps .txt using wgrib2.exe.\nContainer 3 - tidy data, clip to AOI, and write .parquet files to AWS s3 bucket.\nContainer orchestration done through Docker compose - AWS secrets injected at runtime with Github Actions.\nGitHub Actions cron job is triggered at ~ 13:30 prompting a republish of map through connect.posit.cloud.\nConnect.posit.cloud manages AWS credentials and publishes Rshiny app directly from Github frontend repo."
  },
  {
    "objectID": "content/tech_overview.html#high-level-workflow",
    "href": "content/tech_overview.html#high-level-workflow",
    "title": "How it works",
    "section": "",
    "text": "24-hour GRIB2 radar file available on NOMADS at 12:55 UTC.\n\nGitHub Actions cron job is triggered at ~ 13:15 to start a series of docker containers.\nContainer 1 - scrapes the GRIB2 file from NOMADS site.\nContainer 2 - GRIB2 taken out of binary and dumps .txt using wgrib2.exe.\nContainer 3 - tidy data, clip to AOI, and write .parquet files to AWS s3 bucket.\nContainer orchestration done through Docker compose - AWS secrets injected at runtime with Github Actions.\nGitHub Actions cron job is triggered at ~ 13:30 prompting a republish of map through connect.posit.cloud.\nConnect.posit.cloud manages AWS credentials and publishes Rshiny app directly from Github frontend repo."
  },
  {
    "objectID": "content/tech_overview.html#docker",
    "href": "content/tech_overview.html#docker",
    "title": "How it works",
    "section": "Docker",
    "text": "Docker\nDocker is the most impactful tool I’ve adopted since learning to script. In short: you write code as usual, then package it into an image that includes the exact R runtime, system libraries, packages, and any required assets (e.g., shapefiles). That image is a portable, versioned artifact that runs the same everywhere. You push it to a registry (Docker Hub, AWS ECR, or a private repo), and whenever you need it, you pull the image and start a container—a clean, repeatable run every time. This kills environment drift and the “works on my machine” problem. Containers are lightweight, can be linked to form pipelines, and can mount volumes for persistent data. My images live here: https://hub.docker.com/repositories/cfurl. I learned from ‘Docker in a Month of Lunches’ by Elton Stoneman."
  },
  {
    "objectID": "content/tech_overview.html#wgrib2.exe",
    "href": "content/tech_overview.html#wgrib2.exe",
    "title": "How it works",
    "section": "wgrib2.exe",
    "text": "wgrib2.exe\nwgrib2.exe is a command-line utility for working with GRIB2 meteorological data. It reads GRIB2 files, inspects metadata, and produces concise inventories of the messages they contain. It is widely used to filter and extract variables (e.g., precipitation, temperature, wind), select time steps and vertical levels, and subset by geographic region. The tool can regrid or resample fields to different projections or resolutions, apply standard interpolations, and perform basic calculations. It also supports format conversion, writing outputs to human-readable text/CSV and, when built with the appropriate libraries, to NetCDF. Designed for speed and scripting, it scales well for batch processing and automated workflows on large archives. The “.exe” denotes the Windows build; equivalent functionality is available on Linux and macOS as wgrib2. It is maintained on github by NOAA here: https://github.com/NOAA-EMC/wgrib2/releases"
  },
  {
    "objectID": "content/tech_overview.html#parquet-and-apache-arrow",
    "href": "content/tech_overview.html#parquet-and-apache-arrow",
    "title": "How it works",
    "section": "Parquet and Apache Arrow",
    "text": "Parquet and Apache Arrow\nI picked up Apache Parquet and the Arrow R package at the Posit “Big Data with R” workshop, and they’ve been game-changers for radar data. Parquet is a columnar, compressed, splittable file format built for analytics: it stores columns together, so reads are fast and selective (you only scan the columns and row groups you need), and files are much smaller than CSV. Apache Arrow provides a columnar in-memory format and tooling that lets R (and other languages) scan Parquet lazily, push filters down to disk, and stream data in chunks—so you can work with datasets far larger than RAM instead of “loading everything, then filtering.” In practice, that means querying and summarizing hundreds of gigabytes on a laptop, especially when data are partitioned (e.g., by year/month/day) and stored locally or on S3. For analytics at scale, Parquet + Arrow has effectively replaced CSV for me: smaller, faster, and designed for selective reads—exactly what large radar archives demand."
  },
  {
    "objectID": "content/tech_overview.html#s3",
    "href": "content/tech_overview.html#s3",
    "title": "How it works",
    "section": "S3",
    "text": "S3\nAmazon S3 is durable, scalable object storage used to hold files of any size (e.g., Parquet) with high availability and pay-as-you-go pricing. It’s a common backbone for “data lake” workflows: tools read/write directly via s3://… paths, and in R, aws.s3 and Arrow support streaming/lazy access without local copies. Access control is handled with IAM; data governance via versioning, lifecycle policies, and server-side encryption. It’s low-maintenance, widely supported, and integrates cleanly with modern analytics stacks. I’ve used this tutorial regularly: https://www.gormanalysis.com/blog/connecting-to-aws-s3-with-r/"
  },
  {
    "objectID": "content/tech_overview.html#github-actions",
    "href": "content/tech_overview.html#github-actions",
    "title": "How it works",
    "section": "Github Actions",
    "text": "Github Actions\nGitHub Actions is GitHub’s built-in CI/CD platform that runs workflows on events (push, PR), schedules (cron), or manual dispatch. Workflows are defined as YAML files stored with your code, so versioning and reviews are straightforward. Runners can execute shell steps, build and run Docker containers (including docker-compose), and orchestrate multi-step jobs across OS targets. Sensitive values are injected at runtime via Repository/Environment Secrets (e.g., AWS credentials), avoiding hard-coding in images or source. Typical uses include scheduled data jobs, automated tests/linting, container builds and publishes, and static-site deploys (e.g., rebuilding documentation or a homepage on a timer). It’s a practical way to automate repeatable tasks directly from the repo without managing separate infrastructure—and can later hand off to a cloud-native backend if needed."
  },
  {
    "objectID": "content/tech_overview.html#posit.connect.cloud",
    "href": "content/tech_overview.html#posit.connect.cloud",
    "title": "How it works",
    "section": "posit.connect.cloud",
    "text": "posit.connect.cloud\nI’ve spent less than a day building the rshiny app you see here. Posit Connect (cloud-managed) is a production publishing platform for R/Python: it hosts Shiny apps, Quarto/R Markdown sites, and APIs (e.g., Plumber) with HTTPS, auth, and versioned deploys. It handles builds and dependencies, environment variables/secrets, scheduled jobs (e.g., daily refresh), and integrates cleanly with Git/GitHub Actions for push-to-deploy. You get logs, usage metrics, access controls/SSO, and simple scaling (multiple processes/workers) without running servers. It’s a fast way to deliver interactive analytics now, with a clear path to containerized hosting on AWS later (ECS/Fargate, EKS, or EC2) using the same CI/secrets patterns."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automated Radar Rainfall Mapping",
    "section": "",
    "text": "Most weather-radar data come from government agencies, yet the outputs are difficult for water resource managers to process or tailor to local basins. This guide documents the development of an automated radar precipitation processing system across a watershed of interest using the Stage IV QPE product from NCEP. Following the framework described in this guide, users can automate collection of radar rainfall across a watershed that can be used to track droughts, forecast reservoir inflows, estimate soil moisture, force surface water models, etc.\nThe geographic region used in this guide is the Edwards Aquifer recharge zone located in south-central Texas. Rainfall maps and basin averaged accumulation values are published daily at approximately 9:15 AM Central here - Edwards Aquifer rainfall.\n\n\n\nPrecipitation from 7-13-2025 to 7-16-2025 across Edwards Aquifer Recharge Zone"
  }
]