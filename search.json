[
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "Pirate Weather\nKEWX Station\nr4ds\nDocker in a month of lunches\nHappy git with R\nArrow R Package\nNOAA Quarto website\nIowa State Mesonet\nRTMA-URMA\nNCEP Products inventory\nQuarto build i like\nstg4 data nomads"
  },
  {
    "objectID": "content/backend.html",
    "href": "content/backend.html",
    "title": "Backend",
    "section": "",
    "text": "Stage IV data are packaged as GRIB2 files are available each in 1-hr, 6-hr, and 24-hr files. The 6 and 24 hour files are summed from the one hour files. Presently, this tutorial works with a 24-hr file and scrapes data once a day. It would not be difficult to ammend the code and start the workflow every hour.\nStage IV data are available in near real-time on NOMADS at :55 past the hour. For example, hourly rainfall ending 2:00 (representing 1:00-2:00) are available at 2:55. Twenty-four hour data are available from 12:00 - 12:00 UTC (available at 12:55 UTC). Data for the last 14 days are kept on NOMADS after which they are cycled off. The real-time NOMADS site is available here. Note, there are .01h, .06hr, and .24hr files for CONUS, pr (Puerto Rico), and ak (Alaska).\nAfter the near real-time file drop, an additional rerun at 30h after valid time (18:55Z) is available to supplement 24-hr mosaics if RFCs need to update their QPEs or make changes. Personal communication with the WGRFC indicates that this is done very infrequently.\nSince at least 2012 when I started tinkering with the archived Stage IV data, the historical datasets were kept at data.eol.ucar. As of summer 2025, the data have been moved to rda.ucar.edu. At this new archive location they are tarring together Stage IV data on monthly increments. The data appear to be coming available around the 20th of the next month (for example August data available ~ September 20th).\nI intend to add a page to this website describing my manual methods for processing Stage IV data. Currently, focused on real-time description.\n\n\n\nAs previously described, Stage IV data are stored in GRIB2 binary format. Stage IV precipitation has been packaged in both the GRIB1 format (until July 2020) and currently resides in the GRIB2 format. The processing procedures and wgrib.exe utility are completely different. When I get to the descritption of building a historical archive at a site I’ll have to cover GRIB1 processing.\n\n\n\nThe workflow is accomplished by linking containers together using a docker compose orchestration and tripping off the series of containers with a github actions cron job.\nContainer orchestration and Github Action workflows are located in the following repo: Container Orchestration\nThe containers are managed by a docker-compose.yml shown below which describes when to spin up each container, how to manage storage, environments etc. The docker-compose.yml is stored in the root folder of the github repo shown above.\n\n#docker-compose.yml - keep in root folder of your github actions repo\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \n  parqs3:\n    image: cfurl/stg4_24hr_parq_s3:v1\n    container_name: parqs3\n    depends_on:\n      wgrib2:\n        condition: service_completed_successfully\n    volumes:\n      - st4_data:/home/data\n    env_file:\n      - .env\n\nvolumes:\n  st4_data:\n\nThe docker-compose.yml gets triggered each day via Github Actions - described later. Each of the individual container images are stored on the docker hub repository at: hub.docker.cfurl. These should all be public. The workflow for processing the Stage IV data are contained in 3 individual containers described below.\n\n\n\nThe Dockerfile, code, and explicit Powershell prompts to create the Docker Image and load onto Docker Hub are contained in the following repo: Docker Images-Containers\nBefore I get to the scripts held in each individual container I want to take a quick look at how images are constructed. You simply give docker a set of instructions in your ‘Dockerfile’ and then build it through powershell commands. AFter it’s built, you can upload to your repository.\n\n# Dockerfile\n\nFROM rocker/r-ver:4.2.2\n\nRUN mkdir -p /home\nRUN mkdir -p /home/code\nRUN mkdir -p /home/data\n\nWORKDIR /home\n\nCOPY /code/write_parq_2_s3.R /home/code/write_parq_2_s3.R\nCOPY /code/install_packages.R /home/code/install_packages.R\nCOPY /data/texas_buffer_spatial_join.csv /home/data/texas_buffer_spatial_join.csv\n\nRUN Rscript /home/code/install_packages.R\n\nCMD Rscript /home/code/write_parq_2_s3.R\n\nBelow is are brief explanations of what each container accomplishes.\n\n\nThis code takes the current system date and time, visits the appropriate NOMADS url based off of date and time, downloads the 24hr GRIB2 conus file at that url, writes a shell script that is used in the next container to inject prompts to wgrib2.exe, and stores both the shell script and the actual GRIB2 file in a shared storage volume (controlled through docker-compose.yml) that all three containers are linked to.\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(rvest)\n\n# create function to print out UTC time with utc_time()\nnow_utc &lt;- function() {\n  now &lt;- Sys.time()\n  attr(now, \"tzone\") &lt;- \"UTC\"\n  now\n}\n\n# create character string of the hour\nhour_char&lt;-str_sub(as.character(now_utc()),start=12,end=13)\n# create numeric hour\nhour_num&lt;-as.numeric(hour_char)\n\n# create character string for date\ndate_char&lt;- str_sub(as.character(now_utc()),start=1,end=10) %&gt;% str_remove_all(\"-\")\n# create dateclass date with utc timezone\nnow_utc_date&lt;-as.Date(now_utc(),tz=\"UTC\")\n\n\n# read nomads stg4 html page using date from utc_time()\nstg4_http_page&lt;-read_html(paste0(\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/pcpanl.\",date_char,\"/\"))\n\n# find only files that end with .grb2 and have 'pcp' somewhere in the string\ngrib2_available &lt;- stg4_http_page %&gt;%\n  html_elements(\"a\") %&gt;%\n  html_text() %&gt;%\n  str_subset(\"conus\") %&gt;%\n  str_subset(\"24h.grb2$\") \n\n# create path to download\nsource_path&lt;-paste0(\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/pcpanl.\",date_char,\"//\",tail(grib2_available,n=1))\n\n# create download destination\ndestination_path&lt;-paste0(\"/home/data/\",tail(grib2_available,n=1))\n\n#download the file  \ndownload.file (source_path,destination_path,method = \"libcurl\")\n\n# Write your shell file to communicate with the wgrib2 container\ntxt&lt;- paste(\"wgrib2\", tail(grib2_available,n=1), \"-csv\",  str_replace(tail(grib2_available,n=1), \".grb2\", \".txt\"))\nwriteLines(txt,paste0(\"/home/data\",\"/wgrib2_commands.sh\"))\n\n\n\n\nThis code simply executes the wgrib2.exe application and takes the data from the binary format and dumps as text. This works in this case since the only thing wrapped in this GRIB2 file is stage4 rainfall. In many meteorological GRIBs there are many variables. The .sh shell file written in container one contain the instructions that are given to the wgrib2.exe applicatoin. This is controlled through docker-compose.yml, namely: “command:”wgrib2_commands.sh”“. The text output which is a bunch of lat/lons and rain values are stored in the same shared volume as the output files from Container 1.\nI didn’t place the wgrib.exe in an image. There are plenty of images available on docker hub where someone has already done this. I’ve been using the image located here: sondngyn/wgrib2:latest. Below I show the docker commands to copy that image into my repository so I’m not vulnerable to changes in the sondngyn hu repo.\n\nSimple pull, retag, push:\n\n# pull upstream\ndocker pull sondngyn/wgrib2:latest\n\n# retag to your namespace\ndocker tag sondngyn/wgrib2:latest cfurl/sondngyn_wgrib2:v1\n\n# login and push\ndocker login\ndocker push cfurl/sondngyn_wgrib2:v1\n\n# Test it with docker-compose.yml:\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    networks:\n      - some_name\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \nnetworks:\n  some_name:\n    external:\n        name: st4_net\n        \nvolumes:\n  st4_data:\n\n\n\n\nThis docker container connects to your “stg4-texas-24hr” AWS S3 bucket, tidy’s the GRIB2 csv output, clips to area of interest (Texas), and then writes .parquet files that are partitioned by (year, month, day). A note, you would probably have to partition by hour if you do this hourly, because you can’t append a .parquet file to existing data because of how it is written on the disk. Your partitions have to be separate when you are doing this in an automated fashion.\n\n\n\n\nGitHub Actions is a CI/CD platform integrated into GitHub. It runs workflows defined in YAML to build, test, and deploy code on events like pushes, pull requests, or schedules. Jobs run on hosted runners or self-hosted machines, support matrices, secrets, caching, and reusable actions, enabling reliable pipelines across repositories globally.\nGithub Actions\n\nlibrary(\"aws.s3\")\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyr\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\n\n# make sure you can connect to your bucket and open SubTreeFileSystem\nbucket &lt;- s3_bucket(\"stg4-texas-24hr\")\n\n# list everything in your bucket in a recursive manner\nbucket$ls(recursive = TRUE)\n\n# identify path where you will be writing the .parq files\ns3_path &lt;- bucket$path(\"\")\n\naoi_texas_buffer&lt;-read_csv(\"/home/data/texas_buffer_spatial_join.csv\")\n\n# list files that start with st4 and ends with .txt\nraw_grib2_text = list.files(\"/home/data\", pattern = \"^st4_conus.*.txt$\",full.names=FALSE)\n\nfor (h in raw_grib2_text) {\n  name &lt;- h |&gt;\n    str_replace(\"st4_conus.\", \"t\") |&gt;\n    str_replace(\".24h.txt\",\"\")\n  \n  aa&lt;-read_csv(paste0(\"/home/data/\",h), col_names=FALSE) %&gt;%\n  #aa&lt;-read_csv(h, col_names=FALSE) %&gt;%\n    setNames(c(\"x1\",\"x2\",\"x3\",\"x4\",\"center_lon\",\"center_lat\",name)) %&gt;%\n    select(-x1,-x2,-x3,-x4)   \n  \n  # joins by \"center_lon\", \"center_lat\"\n  bb&lt;- left_join(aoi_texas_buffer,aa,by=NULL)%&gt;%\n    pivot_longer(!1:5, names_to = \"time\", values_to = \"rain_mm\") %&gt;%\n    mutate(time = ymd_h(str_sub(time,2,11))) %&gt;%\n    mutate (year = year(time), month = month(time), day = day(time), hour = hour(time)) %&gt;%\n    relocate(rain_mm, .after = last_col()) \n}  \n\nbb|&gt;\n  group_by(year,month,day) |&gt;\n  write_dataset(path = s3_path,\n                format = \"parquet\")"
  },
  {
    "objectID": "content/backend.html#the-details",
    "href": "content/backend.html#the-details",
    "title": "Backend",
    "section": "",
    "text": "Stage IV data are packaged as GRIB2 files are available each in 1-hr, 6-hr, and 24-hr files. The 6 and 24 hour files are summed from the one hour files. Presently, this tutorial works with a 24-hr file and scrapes data once a day. It would not be difficult to ammend the code and start the workflow every hour.\nStage IV data are available in near real-time on NOMADS at :55 past the hour. For example, hourly rainfall ending 2:00 (representing 1:00-2:00) are available at 2:55. Twenty-four hour data are available from 12:00 - 12:00 UTC (available at 12:55 UTC). Data for the last 14 days are kept on NOMADS after which they are cycled off. The real-time NOMADS site is available here. Note, there are .01h, .06hr, and .24hr files for CONUS, pr (Puerto Rico), and ak (Alaska).\nAfter the near real-time file drop, an additional rerun at 30h after valid time (18:55Z) is available to supplement 24-hr mosaics if RFCs need to update their QPEs or make changes. Personal communication with the WGRFC indicates that this is done very infrequently.\nSince at least 2012 when I started tinkering with the archived Stage IV data, the historical datasets were kept at data.eol.ucar. As of summer 2025, the data have been moved to rda.ucar.edu. At this new archive location they are tarring together Stage IV data on monthly increments. The data appear to be coming available around the 20th of the next month (for example August data available ~ September 20th).\nI intend to add a page to this website describing my manual methods for processing Stage IV data. Currently, focused on real-time description.\n\n\n\nAs previously described, Stage IV data are stored in GRIB2 binary format. Stage IV precipitation has been packaged in both the GRIB1 format (until July 2020) and currently resides in the GRIB2 format. The processing procedures and wgrib.exe utility are completely different. When I get to the descritption of building a historical archive at a site I’ll have to cover GRIB1 processing.\n\n\n\nThe workflow is accomplished by linking containers together using a docker compose orchestration and tripping off the series of containers with a github actions cron job.\nContainer orchestration and Github Action workflows are located in the following repo: Container Orchestration\nThe containers are managed by a docker-compose.yml shown below which describes when to spin up each container, how to manage storage, environments etc. The docker-compose.yml is stored in the root folder of the github repo shown above.\n\n#docker-compose.yml - keep in root folder of your github actions repo\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \n  parqs3:\n    image: cfurl/stg4_24hr_parq_s3:v1\n    container_name: parqs3\n    depends_on:\n      wgrib2:\n        condition: service_completed_successfully\n    volumes:\n      - st4_data:/home/data\n    env_file:\n      - .env\n\nvolumes:\n  st4_data:\n\nThe docker-compose.yml gets triggered each day via Github Actions - described later. Each of the individual container images are stored on the docker hub repository at: hub.docker.cfurl. These should all be public. The workflow for processing the Stage IV data are contained in 3 individual containers described below.\n\n\n\nThe Dockerfile, code, and explicit Powershell prompts to create the Docker Image and load onto Docker Hub are contained in the following repo: Docker Images-Containers\nBefore I get to the scripts held in each individual container I want to take a quick look at how images are constructed. You simply give docker a set of instructions in your ‘Dockerfile’ and then build it through powershell commands. AFter it’s built, you can upload to your repository.\n\n# Dockerfile\n\nFROM rocker/r-ver:4.2.2\n\nRUN mkdir -p /home\nRUN mkdir -p /home/code\nRUN mkdir -p /home/data\n\nWORKDIR /home\n\nCOPY /code/write_parq_2_s3.R /home/code/write_parq_2_s3.R\nCOPY /code/install_packages.R /home/code/install_packages.R\nCOPY /data/texas_buffer_spatial_join.csv /home/data/texas_buffer_spatial_join.csv\n\nRUN Rscript /home/code/install_packages.R\n\nCMD Rscript /home/code/write_parq_2_s3.R\n\nBelow is are brief explanations of what each container accomplishes.\n\n\nThis code takes the current system date and time, visits the appropriate NOMADS url based off of date and time, downloads the 24hr GRIB2 conus file at that url, writes a shell script that is used in the next container to inject prompts to wgrib2.exe, and stores both the shell script and the actual GRIB2 file in a shared storage volume (controlled through docker-compose.yml) that all three containers are linked to.\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(rvest)\n\n# create function to print out UTC time with utc_time()\nnow_utc &lt;- function() {\n  now &lt;- Sys.time()\n  attr(now, \"tzone\") &lt;- \"UTC\"\n  now\n}\n\n# create character string of the hour\nhour_char&lt;-str_sub(as.character(now_utc()),start=12,end=13)\n# create numeric hour\nhour_num&lt;-as.numeric(hour_char)\n\n# create character string for date\ndate_char&lt;- str_sub(as.character(now_utc()),start=1,end=10) %&gt;% str_remove_all(\"-\")\n# create dateclass date with utc timezone\nnow_utc_date&lt;-as.Date(now_utc(),tz=\"UTC\")\n\n\n# read nomads stg4 html page using date from utc_time()\nstg4_http_page&lt;-read_html(paste0(\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/pcpanl.\",date_char,\"/\"))\n\n# find only files that end with .grb2 and have 'pcp' somewhere in the string\ngrib2_available &lt;- stg4_http_page %&gt;%\n  html_elements(\"a\") %&gt;%\n  html_text() %&gt;%\n  str_subset(\"conus\") %&gt;%\n  str_subset(\"24h.grb2$\") \n\n# create path to download\nsource_path&lt;-paste0(\"https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/pcpanl.\",date_char,\"//\",tail(grib2_available,n=1))\n\n# create download destination\ndestination_path&lt;-paste0(\"/home/data/\",tail(grib2_available,n=1))\n\n#download the file  \ndownload.file (source_path,destination_path,method = \"libcurl\")\n\n# Write your shell file to communicate with the wgrib2 container\ntxt&lt;- paste(\"wgrib2\", tail(grib2_available,n=1), \"-csv\",  str_replace(tail(grib2_available,n=1), \".grb2\", \".txt\"))\nwriteLines(txt,paste0(\"/home/data\",\"/wgrib2_commands.sh\"))\n\n\n\n\nThis code simply executes the wgrib2.exe application and takes the data from the binary format and dumps as text. This works in this case since the only thing wrapped in this GRIB2 file is stage4 rainfall. In many meteorological GRIBs there are many variables. The .sh shell file written in container one contain the instructions that are given to the wgrib2.exe applicatoin. This is controlled through docker-compose.yml, namely: “command:”wgrib2_commands.sh”“. The text output which is a bunch of lat/lons and rain values are stored in the same shared volume as the output files from Container 1.\nI didn’t place the wgrib.exe in an image. There are plenty of images available on docker hub where someone has already done this. I’ve been using the image located here: sondngyn/wgrib2:latest. Below I show the docker commands to copy that image into my repository so I’m not vulnerable to changes in the sondngyn hu repo.\n\nSimple pull, retag, push:\n\n# pull upstream\ndocker pull sondngyn/wgrib2:latest\n\n# retag to your namespace\ndocker tag sondngyn/wgrib2:latest cfurl/sondngyn_wgrib2:v1\n\n# login and push\ndocker login\ndocker push cfurl/sondngyn_wgrib2:v1\n\n# Test it with docker-compose.yml:\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    networks:\n      - some_name\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \nnetworks:\n  some_name:\n    external:\n        name: st4_net\n        \nvolumes:\n  st4_data:\n\n\n\n\nThis docker container connects to your “stg4-texas-24hr” AWS S3 bucket, tidy’s the GRIB2 csv output, clips to area of interest (Texas), and then writes .parquet files that are partitioned by (year, month, day). A note, you would probably have to partition by hour if you do this hourly, because you can’t append a .parquet file to existing data because of how it is written on the disk. Your partitions have to be separate when you are doing this in an automated fashion.\n\n\n\n\nGitHub Actions is a CI/CD platform integrated into GitHub. It runs workflows defined in YAML to build, test, and deploy code on events like pushes, pull requests, or schedules. Jobs run on hosted runners or self-hosted machines, support matrices, secrets, caching, and reusable actions, enabling reliable pipelines across repositories globally.\nGithub Actions\n\nlibrary(\"aws.s3\")\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyr\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\n\n# make sure you can connect to your bucket and open SubTreeFileSystem\nbucket &lt;- s3_bucket(\"stg4-texas-24hr\")\n\n# list everything in your bucket in a recursive manner\nbucket$ls(recursive = TRUE)\n\n# identify path where you will be writing the .parq files\ns3_path &lt;- bucket$path(\"\")\n\naoi_texas_buffer&lt;-read_csv(\"/home/data/texas_buffer_spatial_join.csv\")\n\n# list files that start with st4 and ends with .txt\nraw_grib2_text = list.files(\"/home/data\", pattern = \"^st4_conus.*.txt$\",full.names=FALSE)\n\nfor (h in raw_grib2_text) {\n  name &lt;- h |&gt;\n    str_replace(\"st4_conus.\", \"t\") |&gt;\n    str_replace(\".24h.txt\",\"\")\n  \n  aa&lt;-read_csv(paste0(\"/home/data/\",h), col_names=FALSE) %&gt;%\n  #aa&lt;-read_csv(h, col_names=FALSE) %&gt;%\n    setNames(c(\"x1\",\"x2\",\"x3\",\"x4\",\"center_lon\",\"center_lat\",name)) %&gt;%\n    select(-x1,-x2,-x3,-x4)   \n  \n  # joins by \"center_lon\", \"center_lat\"\n  bb&lt;- left_join(aoi_texas_buffer,aa,by=NULL)%&gt;%\n    pivot_longer(!1:5, names_to = \"time\", values_to = \"rain_mm\") %&gt;%\n    mutate(time = ymd_h(str_sub(time,2,11))) %&gt;%\n    mutate (year = year(time), month = month(time), day = day(time), hour = hour(time)) %&gt;%\n    relocate(rain_mm, .after = last_col()) \n}  \n\nbb|&gt;\n  group_by(year,month,day) |&gt;\n  write_dataset(path = s3_path,\n                format = \"parquet\")"
  },
  {
    "objectID": "content/backend.html#github-actions-1",
    "href": "content/backend.html#github-actions-1",
    "title": "Backend",
    "section": "Github Actions",
    "text": "Github Actions\nGithub actions is what makes the automation work. The docker-compose.yml is fired by the compose-workflow.yml. This compose-workflow.yml has to be helf in a folder called ‘.github/workflows’. AWS credentials are managed through Github via the repository. AWS credentials are injected at runtime through the actions .yml. The yml is shown below:\n\nname: stg4-texas-24hr-backend-actions\n\non:\n  schedule:\n    - cron: '45 13 * * *'  # Runs daily at 13:45 UTC\n  workflow_dispatch:       # Also allow manual triggering\n\njobs:\n  run-pipeline:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Install Docker using Docker's official script\n        run: |\n          curl -fsSL https://get.docker.com -o get-docker.sh\n          sudo sh get-docker.sh\n\n      - name: Install Docker Compose\n        run: |\n          sudo curl -L \"https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64\" -o /usr/local/bin/docker-compose\n          sudo chmod +x /usr/local/bin/docker-compose\n          docker-compose --version\n\n      - name: Create .env file with AWS credentials\n        run: |\n          echo \"AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}\" &gt;&gt; .env\n          echo \"AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}\" &gt;&gt; .env\n          echo \"AWS_REGION=${{ secrets.AWS_REGION }}\" &gt;&gt; .env\n\n      - name: Run Docker Compose (with AWS env)\n        run: docker-compose up\n        continue-on-error: false"
  },
  {
    "objectID": "content/backend.html#github-repositories-and-this-webpage",
    "href": "content/backend.html#github-repositories-and-this-webpage",
    "title": "Backend",
    "section": "Github repositories and this webpage",
    "text": "Github repositories and this webpage\nI started a github organization called ‘stg4-texas-24hr-ga’ this allows me to segregate polished work from my messy personal github repository. Below are links to pertinent repositories.\n\nstg4-texas-24hr-ga organization\nThis website repo /stg4-texas-24hr-ga/stg4-texas-24hr-ga\nDocker Image builds /stg4-texas-24hr-ga/stg4-texas-24hr-docker\nActions and Compose /stg4-texas-24hr-backend-actions"
  },
  {
    "objectID": "content/frontend.html",
    "href": "content/frontend.html",
    "title": "Frontend",
    "section": "",
    "text": "I recall going through a lot of difficulty years ago trying to get the grid and bin mapping straightened away like I wanted it. One of the problems is that the HRAP cell size changes with Latitude. All of the answers lie in: “Coordinate Transformations for Using NEXRAD data in GIS-based Hydrologic Modeling’ by Reed and Maidment (1999). I may have read this journal article more than any other article. I was able to build a shapefile of the HRAP grid that I validated through published bin sizes (in Reed and Maidment) with this article and a pearl script from Stuart foote that wrote the corner intersecting vertices of the of the HRAP grid. Complicated stuff. I need to put the shapefile of the hrap grid on a github repo somewhere. I have .csv for state of Texas that associates lat-lon with HRAP Grid ID. My shapefile holds HRAP Grid ID and Stg4 dumps center point (lan-lon) of radar bin. So, i can quickly map rainfall amount to HRAP Grid and my shapefile holds the size of the radar bin.\nThe next time I work in a new geographic location I will document handling of the grid better.\n\n\n\nThe map displaying rainfall across the basin is a simple Rshiny app (app.R) that is hosted on posit.connect.cloud. This ggplot was written by Tanya (cfurl/stg4_edwards).\n\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyr\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"aws.s3\")\nlibrary(\"sf\")\nlibrary(\"ggspatial\")\nlibrary(\"ggplot2\")\nlibrary(\"prettymapr\")\nlibrary(\"shiny\")\n#library(\"ggiraph\")\n\n######################## Some S3 things #####################\n# remove this from container setup, this gives your local dev the AWS access\n#readRenviron(\"../.Renviron\") # this is for keys one level up from root directory\n#readRenviron(\".Renviron\") # when it's in gitignore\n\nrequired &lt;- c(\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_DEFAULT_REGION\")\nmissing  &lt;- required[Sys.getenv(required) == \"\"]\nif (length(missing)) {\n  stop(\"Missing env vars on Connect: \", paste(missing, collapse = \", \"))\n}\n\n# make sure you can connect to your bucket and open SubTreeFileSystem and identify path\n# then connect to the .parq files on the s3 storage\nbucket &lt;- s3_bucket(\"stg4-texas-24hr\")\ns3_path &lt;- bucket$path(\"\")\nstg4_24hr_texas_parq &lt;- open_dataset(s3_path)\n\n############################ time stamps #############\n\ncurrent_utc_date_time &lt;- with_tz(Sys.time(), \"UTC\")\ncurrent_central_date_time &lt;- with_tz(Sys.time(), \"America/Chicago\")\ncurrent_utc_time &lt;- format(with_tz(Sys.time(), \"UTC\"), \"%H:%M:%S\")\ncurrent_utc_date &lt;- as_date(with_tz(Sys.time(), \"UTC\"))\n\n\n# parquet gets populated with most recent 24hr file at 13:45 UTC\n# new shiny page pushed to posit.cloud.connect at 13:57 UTC\n\n# This solves the problem of when the UTC time is in the current day, but the STG4 hasn't dopped yet, so the script is looking\n# for parquet files that haven't been populated yet. Until you reach 13:52 (when parquet is safely populated), it kicks you\n# back to yesterday\n#t1_offset &lt;- case_when (current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -0, TRUE ~ 0)\n#t1_offset &lt;- case_when (current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -1, TRUE ~ 0)\n#t2_offset &lt;- case_when( current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -2, TRUE ~ -1)\n\n# Create exact timestamps (UTC) for noon on yesterday and today\nt1 &lt;- as.POSIXct(paste(Sys.Date() - 0, \"12:00:00\"), tz = \"UTC\")  # today 0\nt2 &lt;- as.POSIXct(paste(Sys.Date() - 1, \"12:00:00\"), tz = \"UTC\") # yesterday 1\n\n#create some timestamps for labels\n\n\n\n\n# This is where you query the parq files by time (not location yet)\n# carrying these commands around for whole state, could clip first\n\ntime_check &lt;- stg4_24hr_texas_parq |&gt;\n  select(time)|&gt;\n  filter (time %in% c(t1)) |&gt;\n  collect()\n\nif (nrow(time_check) == 0) {\n  time_filter&lt;-t2\n} else {\n  time_filter&lt;-t1\n}\n\nd &lt;- stg4_24hr_texas_parq |&gt;\n    filter (time %in% c(time_filter)) |&gt;\n    group_by (grib_id) %&gt;%\n    summarize(\n      sum_rain = sum(rain_mm, na.rm=TRUE)) %&gt;%\n    arrange(desc(sum_rain)) |&gt;\n    collect()\n\n# Make local time labels for main title. Precipitation from xxxx - xxxx\nend_time_local &lt;- with_tz(time_filter, \"America/Chicago\")\nbegin_time_local &lt;- end_time_local - days(1)\n\n\n# call the gis layers you want mapped\nmap &lt;- sf::read_sf(\"./gis/usgs_dissolved.shp\")\nstreams &lt;- read_sf(\"./gis/streams_recharge.shp\")\nlakes &lt;- read_sf(\"./gis/reservoirs.shp\")\n\n# this is where you subset the statewide set of bins by your shapefile area of interest\nmap_rain &lt;- map|&gt;\n  left_join(d, by = \"grib_id\")|&gt;\n  mutate(cubic_m_precip = bin_area * sum_rain * 0.001)|&gt;\n  mutate(sum_rain_in = sum_rain/25.4)\n\n# Mapping function edited from Tanya's work\nplot_bin_map&lt;-function(\n    title = 'Edwards Aquifer Recharge Zone',\n    subtitle= NA,\n    note_title = NA,\n    font = \"Open Sans\",\n    map_rain = NA,\n    map_streams = NA, \n    map_lakes = NA,\n    pal_water='black',\n    pal_title='white',\n    pal_subtitle='white',\n    pal_outline='black',\n    pal_bin_outline='black',\n    pal_legend_text='white',\n    bin_alpha = 0.7,\n    map_type='cartodark'\n){\n  \n  bbox &lt;- st_bbox(c(\n    xmin = -100.85,\n    ymin = 29.0, \n    xmax = -97.75, \n    ymax = 30.47\n  ), crs = 4326)\n  \n  coord_sys&lt;-3857\n  \n  # Convert bbox to an sf object for ggplot compatibility\n  bbox_sf &lt;- st_as_sfc(bbox)\n  bbox_transformed &lt;- st_transform(bbox_sf, crs = coord_sys)\n  \n  outline &lt;- map |&gt; summarise(geometry = st_union(geometry)) |&gt; st_cast(\"MULTILINESTRING\")  \n  \n  title_pos &lt;- st_sfc(st_point(c(-100.88, 30.43)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  subtitle_pos &lt;- st_sfc(st_point(c(-100.88, 30.43 - 0.085)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  note_title_pos &lt;- st_sfc(st_point(c(-100.88, 30.43 - 1.41)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  # --- Static legend settings (always show full range) ---\n  rain_breaks  &lt;- c(0, 0.1, 0.25, 0.5, 1, 2, 3, 4, 6, 8, 10, 12)\n  rain_labels  &lt;- c(\"0\",\"0.1\",\"0.25\",\"0.5\",\"1\",\"2\",\"3\",\"4\",\"6\",\"8\",\"10\",\"12+\")\n  rain_limits  &lt;- c(0, 12)\n  \n  # --- Set 0 rainfall to NA for transparency ---\n  map_rain &lt;- map_rain |&gt;\n    mutate(fill_val = ifelse(sum_rain_in == 0, NA_real_, sum_rain_in))\n  \n  plot&lt;-ggplot()+\n    annotation_map_tile(\n      type = map_type,  # Use the \"Carto Light\" basemap\n      zoom = 9  # Adjust zoom level as needed\n    )+\n    annotate(geom=\"text\",x= title_pos$X,y=title_pos$Y,label=title,size=8,hjust=0, color = pal_title, family=font, fontface='bold')+\n    annotate(geom=\"text\",x= subtitle_pos$X,y=subtitle_pos$Y,label=subtitle,size=5,hjust=0, color = pal_subtitle, family=font)+\n    annotate(geom=\"text\",x=  note_title_pos$X,y= note_title_pos$Y,label=note_title,size=2,hjust=0, color = pal_subtitle, family=font)+\n    geom_sf(data = map_rain, mapping = aes(fill = fill_val), color = pal_bin_outline, alpha = bin_alpha, na.rm = FALSE) +\n    geom_sf(data = outline|&gt;st_transform(crs = coord_sys), color = pal_outline, linewidth = 0.4) +  \n    geom_sf(data=map_lakes|&gt;st_transform(crs = coord_sys), fill= pal_water, color= pal_water, linewidth = 0.2)+\n    geom_sf(data=map_streams|&gt;st_transform(crs = coord_sys), color= pal_water)+\n    \n    scale_fill_stepsn(\n      colours = c(\"#82D3F0\",\"#0826A2\",\"#22FE05\",\"#248418\",\n                  \"#F6FB07\",\"#FFC348\",\"#E01E17\",\"#8C302C\",\n                  \"#CC17DA\",\"#AE60B3\",\"#FDF5FF\"),\n      breaks    = rain_breaks,\n      limits    = rain_limits,\n      labels    = rain_labels,\n      oob       = scales::squish,\n      name      = \"Rainfall (in)\",\n      na.value  = NA  # keep transparency for NA (zero rainfall)\n    ) +\n    guides(\n      fill = guide_colorsteps(\n        title.position = \"top\",\n        title.vjust = 0.1,\n        show.limits = TRUE\n      )\n    )+\n    coord_sf(\n      xlim = c(st_bbox(bbox_transformed)[\"xmin\"], st_bbox(bbox_transformed)[\"xmax\"]),\n      ylim = c(st_bbox(bbox_transformed)[\"ymin\"], st_bbox(bbox_transformed)[\"ymax\"])\n    ) +\n    theme_void()+\n    theme(\n      text = element_text(family=font),\n      legend.position = \"inside\",\n      legend.position.inside = c(0.70,0.1),  \n      legend.direction = \"horizontal\", \n      legend.margin = margin(t = 0, r = 10, b = 0, l = 10),\n      legend.title = element_text(size = 10, face='bold', color=pal_legend_text), \n      legend.text = element_text(size = 9, color=pal_legend_text),  \n      legend.key.width = unit(2.5, \"cm\"), \n      legend.key.height = unit(0.5, \"cm\")  \n    )\n  \n  return(plot)\n}\n\n\n\n#ui &lt;- fluidPage(\n#  tags$head(tags$title(\"Rainfall Map\")),\n#  fluidRow(\n#    column(\n#      width = 12,\n#      plotOutput(\"rain_map\", height = \"800px\")\n#    )\n#  )\n#)\n\n\nui &lt;- fluidPage(\n  style = \"padding:0; margin:0;\",\n  tags$head(tags$title(\"Rainfall Map\")),\n  plotOutput(\"rain_map\", width = \"100%\", height = \"100vh\")\n)\n\n\n\n\n\nserver &lt;- function(input, output, session) {\n  output$rain_map &lt;- renderPlot({\n                            plot_bin_map(title = 'Edwards Aquifer Recharge Zone',\n                            subtitle = paste(\"Precipitation from\", format(begin_time_local, \"%Y-%m-%d %H:%M %Z\"), \"to\",format(end_time_local, \"%Y-%m-%d %H:%M %Z\")),\n                            note_title = paste(\"This map queried .parq at\", format(current_utc_date_time, \"%Y-%m-%d %H:%M %Z\"), \"and\", format(current_central_date_time, \"%Y-%m-%d %H:%M %Z\")) ,\n                            font = \"\",\n                            map_rain = map_rain,\n                            map_streams = streams,\n                            map_lakes = lakes,\n                            #pal_water='#697984',\n                            pal_water = '#2C6690',\n                            pal_title='black',\n                            # pal_legend = 'YlOrRd',\n                            bin_alpha = 0.9,\n                            pal_subtitle='black',\n                            pal_outline=\"#697984\",\n                            pal_bin_outline='white',\n                            pal_legend_text='black',\n                            map_type='cartolight')}, res = 144)  # crisp output\n}\n\nshinyApp(ui, server)\n\n\n\n\nPosit.Connect.Cloud is nice for a few reasons 1). you can stand up a website, quarto build, or shiny app in a few minutes 2). It publishes directly from your github repository so you can develop in the same place you publish, and 3). You can add secrets to inject passwords at runtime - so you can interact with you AWS environment with very little pain.\nIn this case, I have a front end repository up at /stg4-texas-24hr-frontend-actions however my website is not publishing from there. I am publishing from cfurl/st4_front_query_map. My posit.connect.cloud is associated with ‘cfurl’ repository and the third party app that links github-PCC can only be associated with one repo owner (in this case cfurl).\nThe PCC website republishes everytime there is a commit to the github repo. This is fine when your developing, but causes issue if you want regular republishing. To work around this, my PCC website publishes daily by an ‘empty’ commit on a cron job via Github Actions. PCC doesn’t have the ability to schedule cron jobs so my workaround was to do a dummy commit with GA so that it publishes daily.\n\nname: Daily Connect redeploy nudge\n\non:\n  schedule:\n    # 8:45 AM America/Chicago (DST Mar–Oct)vv\n    - cron: \"57 13 * 3-10 *\"\n    # 8:45 AM America/Chicago (Standard Nov–Feb)\n    - cron: \"57 14 * 11,12,1,2 *\"\n  workflow_dispatch:\n\njobs:\n  nudge:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          persist-credentials: true\n\n      - name: Configure git identity\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n\n      - name: Create empty commit\n        run: |\n          git commit --allow-empty -m 'chore: daily redeploy nudge [skip ci]'\n\n      - name: Push to default branch\n        run: |\n          git push origin \"HEAD:${{ github.event.repository.default_branch }}\""
  },
  {
    "objectID": "content/frontend.html#the-details",
    "href": "content/frontend.html#the-details",
    "title": "Frontend",
    "section": "",
    "text": "I recall going through a lot of difficulty years ago trying to get the grid and bin mapping straightened away like I wanted it. One of the problems is that the HRAP cell size changes with Latitude. All of the answers lie in: “Coordinate Transformations for Using NEXRAD data in GIS-based Hydrologic Modeling’ by Reed and Maidment (1999). I may have read this journal article more than any other article. I was able to build a shapefile of the HRAP grid that I validated through published bin sizes (in Reed and Maidment) with this article and a pearl script from Stuart foote that wrote the corner intersecting vertices of the of the HRAP grid. Complicated stuff. I need to put the shapefile of the hrap grid on a github repo somewhere. I have .csv for state of Texas that associates lat-lon with HRAP Grid ID. My shapefile holds HRAP Grid ID and Stg4 dumps center point (lan-lon) of radar bin. So, i can quickly map rainfall amount to HRAP Grid and my shapefile holds the size of the radar bin.\nThe next time I work in a new geographic location I will document handling of the grid better.\n\n\n\nThe map displaying rainfall across the basin is a simple Rshiny app (app.R) that is hosted on posit.connect.cloud. This ggplot was written by Tanya (cfurl/stg4_edwards).\n\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyr\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"aws.s3\")\nlibrary(\"sf\")\nlibrary(\"ggspatial\")\nlibrary(\"ggplot2\")\nlibrary(\"prettymapr\")\nlibrary(\"shiny\")\n#library(\"ggiraph\")\n\n######################## Some S3 things #####################\n# remove this from container setup, this gives your local dev the AWS access\n#readRenviron(\"../.Renviron\") # this is for keys one level up from root directory\n#readRenviron(\".Renviron\") # when it's in gitignore\n\nrequired &lt;- c(\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_DEFAULT_REGION\")\nmissing  &lt;- required[Sys.getenv(required) == \"\"]\nif (length(missing)) {\n  stop(\"Missing env vars on Connect: \", paste(missing, collapse = \", \"))\n}\n\n# make sure you can connect to your bucket and open SubTreeFileSystem and identify path\n# then connect to the .parq files on the s3 storage\nbucket &lt;- s3_bucket(\"stg4-texas-24hr\")\ns3_path &lt;- bucket$path(\"\")\nstg4_24hr_texas_parq &lt;- open_dataset(s3_path)\n\n############################ time stamps #############\n\ncurrent_utc_date_time &lt;- with_tz(Sys.time(), \"UTC\")\ncurrent_central_date_time &lt;- with_tz(Sys.time(), \"America/Chicago\")\ncurrent_utc_time &lt;- format(with_tz(Sys.time(), \"UTC\"), \"%H:%M:%S\")\ncurrent_utc_date &lt;- as_date(with_tz(Sys.time(), \"UTC\"))\n\n\n# parquet gets populated with most recent 24hr file at 13:45 UTC\n# new shiny page pushed to posit.cloud.connect at 13:57 UTC\n\n# This solves the problem of when the UTC time is in the current day, but the STG4 hasn't dopped yet, so the script is looking\n# for parquet files that haven't been populated yet. Until you reach 13:52 (when parquet is safely populated), it kicks you\n# back to yesterday\n#t1_offset &lt;- case_when (current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -0, TRUE ~ 0)\n#t1_offset &lt;- case_when (current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -1, TRUE ~ 0)\n#t2_offset &lt;- case_when( current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -2, TRUE ~ -1)\n\n# Create exact timestamps (UTC) for noon on yesterday and today\nt1 &lt;- as.POSIXct(paste(Sys.Date() - 0, \"12:00:00\"), tz = \"UTC\")  # today 0\nt2 &lt;- as.POSIXct(paste(Sys.Date() - 1, \"12:00:00\"), tz = \"UTC\") # yesterday 1\n\n#create some timestamps for labels\n\n\n\n\n# This is where you query the parq files by time (not location yet)\n# carrying these commands around for whole state, could clip first\n\ntime_check &lt;- stg4_24hr_texas_parq |&gt;\n  select(time)|&gt;\n  filter (time %in% c(t1)) |&gt;\n  collect()\n\nif (nrow(time_check) == 0) {\n  time_filter&lt;-t2\n} else {\n  time_filter&lt;-t1\n}\n\nd &lt;- stg4_24hr_texas_parq |&gt;\n    filter (time %in% c(time_filter)) |&gt;\n    group_by (grib_id) %&gt;%\n    summarize(\n      sum_rain = sum(rain_mm, na.rm=TRUE)) %&gt;%\n    arrange(desc(sum_rain)) |&gt;\n    collect()\n\n# Make local time labels for main title. Precipitation from xxxx - xxxx\nend_time_local &lt;- with_tz(time_filter, \"America/Chicago\")\nbegin_time_local &lt;- end_time_local - days(1)\n\n\n# call the gis layers you want mapped\nmap &lt;- sf::read_sf(\"./gis/usgs_dissolved.shp\")\nstreams &lt;- read_sf(\"./gis/streams_recharge.shp\")\nlakes &lt;- read_sf(\"./gis/reservoirs.shp\")\n\n# this is where you subset the statewide set of bins by your shapefile area of interest\nmap_rain &lt;- map|&gt;\n  left_join(d, by = \"grib_id\")|&gt;\n  mutate(cubic_m_precip = bin_area * sum_rain * 0.001)|&gt;\n  mutate(sum_rain_in = sum_rain/25.4)\n\n# Mapping function edited from Tanya's work\nplot_bin_map&lt;-function(\n    title = 'Edwards Aquifer Recharge Zone',\n    subtitle= NA,\n    note_title = NA,\n    font = \"Open Sans\",\n    map_rain = NA,\n    map_streams = NA, \n    map_lakes = NA,\n    pal_water='black',\n    pal_title='white',\n    pal_subtitle='white',\n    pal_outline='black',\n    pal_bin_outline='black',\n    pal_legend_text='white',\n    bin_alpha = 0.7,\n    map_type='cartodark'\n){\n  \n  bbox &lt;- st_bbox(c(\n    xmin = -100.85,\n    ymin = 29.0, \n    xmax = -97.75, \n    ymax = 30.47\n  ), crs = 4326)\n  \n  coord_sys&lt;-3857\n  \n  # Convert bbox to an sf object for ggplot compatibility\n  bbox_sf &lt;- st_as_sfc(bbox)\n  bbox_transformed &lt;- st_transform(bbox_sf, crs = coord_sys)\n  \n  outline &lt;- map |&gt; summarise(geometry = st_union(geometry)) |&gt; st_cast(\"MULTILINESTRING\")  \n  \n  title_pos &lt;- st_sfc(st_point(c(-100.88, 30.43)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  subtitle_pos &lt;- st_sfc(st_point(c(-100.88, 30.43 - 0.085)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  note_title_pos &lt;- st_sfc(st_point(c(-100.88, 30.43 - 1.41)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  # --- Static legend settings (always show full range) ---\n  rain_breaks  &lt;- c(0, 0.1, 0.25, 0.5, 1, 2, 3, 4, 6, 8, 10, 12)\n  rain_labels  &lt;- c(\"0\",\"0.1\",\"0.25\",\"0.5\",\"1\",\"2\",\"3\",\"4\",\"6\",\"8\",\"10\",\"12+\")\n  rain_limits  &lt;- c(0, 12)\n  \n  # --- Set 0 rainfall to NA for transparency ---\n  map_rain &lt;- map_rain |&gt;\n    mutate(fill_val = ifelse(sum_rain_in == 0, NA_real_, sum_rain_in))\n  \n  plot&lt;-ggplot()+\n    annotation_map_tile(\n      type = map_type,  # Use the \"Carto Light\" basemap\n      zoom = 9  # Adjust zoom level as needed\n    )+\n    annotate(geom=\"text\",x= title_pos$X,y=title_pos$Y,label=title,size=8,hjust=0, color = pal_title, family=font, fontface='bold')+\n    annotate(geom=\"text\",x= subtitle_pos$X,y=subtitle_pos$Y,label=subtitle,size=5,hjust=0, color = pal_subtitle, family=font)+\n    annotate(geom=\"text\",x=  note_title_pos$X,y= note_title_pos$Y,label=note_title,size=2,hjust=0, color = pal_subtitle, family=font)+\n    geom_sf(data = map_rain, mapping = aes(fill = fill_val), color = pal_bin_outline, alpha = bin_alpha, na.rm = FALSE) +\n    geom_sf(data = outline|&gt;st_transform(crs = coord_sys), color = pal_outline, linewidth = 0.4) +  \n    geom_sf(data=map_lakes|&gt;st_transform(crs = coord_sys), fill= pal_water, color= pal_water, linewidth = 0.2)+\n    geom_sf(data=map_streams|&gt;st_transform(crs = coord_sys), color= pal_water)+\n    \n    scale_fill_stepsn(\n      colours = c(\"#82D3F0\",\"#0826A2\",\"#22FE05\",\"#248418\",\n                  \"#F6FB07\",\"#FFC348\",\"#E01E17\",\"#8C302C\",\n                  \"#CC17DA\",\"#AE60B3\",\"#FDF5FF\"),\n      breaks    = rain_breaks,\n      limits    = rain_limits,\n      labels    = rain_labels,\n      oob       = scales::squish,\n      name      = \"Rainfall (in)\",\n      na.value  = NA  # keep transparency for NA (zero rainfall)\n    ) +\n    guides(\n      fill = guide_colorsteps(\n        title.position = \"top\",\n        title.vjust = 0.1,\n        show.limits = TRUE\n      )\n    )+\n    coord_sf(\n      xlim = c(st_bbox(bbox_transformed)[\"xmin\"], st_bbox(bbox_transformed)[\"xmax\"]),\n      ylim = c(st_bbox(bbox_transformed)[\"ymin\"], st_bbox(bbox_transformed)[\"ymax\"])\n    ) +\n    theme_void()+\n    theme(\n      text = element_text(family=font),\n      legend.position = \"inside\",\n      legend.position.inside = c(0.70,0.1),  \n      legend.direction = \"horizontal\", \n      legend.margin = margin(t = 0, r = 10, b = 0, l = 10),\n      legend.title = element_text(size = 10, face='bold', color=pal_legend_text), \n      legend.text = element_text(size = 9, color=pal_legend_text),  \n      legend.key.width = unit(2.5, \"cm\"), \n      legend.key.height = unit(0.5, \"cm\")  \n    )\n  \n  return(plot)\n}\n\n\n\n#ui &lt;- fluidPage(\n#  tags$head(tags$title(\"Rainfall Map\")),\n#  fluidRow(\n#    column(\n#      width = 12,\n#      plotOutput(\"rain_map\", height = \"800px\")\n#    )\n#  )\n#)\n\n\nui &lt;- fluidPage(\n  style = \"padding:0; margin:0;\",\n  tags$head(tags$title(\"Rainfall Map\")),\n  plotOutput(\"rain_map\", width = \"100%\", height = \"100vh\")\n)\n\n\n\n\n\nserver &lt;- function(input, output, session) {\n  output$rain_map &lt;- renderPlot({\n                            plot_bin_map(title = 'Edwards Aquifer Recharge Zone',\n                            subtitle = paste(\"Precipitation from\", format(begin_time_local, \"%Y-%m-%d %H:%M %Z\"), \"to\",format(end_time_local, \"%Y-%m-%d %H:%M %Z\")),\n                            note_title = paste(\"This map queried .parq at\", format(current_utc_date_time, \"%Y-%m-%d %H:%M %Z\"), \"and\", format(current_central_date_time, \"%Y-%m-%d %H:%M %Z\")) ,\n                            font = \"\",\n                            map_rain = map_rain,\n                            map_streams = streams,\n                            map_lakes = lakes,\n                            #pal_water='#697984',\n                            pal_water = '#2C6690',\n                            pal_title='black',\n                            # pal_legend = 'YlOrRd',\n                            bin_alpha = 0.9,\n                            pal_subtitle='black',\n                            pal_outline=\"#697984\",\n                            pal_bin_outline='white',\n                            pal_legend_text='black',\n                            map_type='cartolight')}, res = 144)  # crisp output\n}\n\nshinyApp(ui, server)\n\n\n\n\nPosit.Connect.Cloud is nice for a few reasons 1). you can stand up a website, quarto build, or shiny app in a few minutes 2). It publishes directly from your github repository so you can develop in the same place you publish, and 3). You can add secrets to inject passwords at runtime - so you can interact with you AWS environment with very little pain.\nIn this case, I have a front end repository up at /stg4-texas-24hr-frontend-actions however my website is not publishing from there. I am publishing from cfurl/st4_front_query_map. My posit.connect.cloud is associated with ‘cfurl’ repository and the third party app that links github-PCC can only be associated with one repo owner (in this case cfurl).\nThe PCC website republishes everytime there is a commit to the github repo. This is fine when your developing, but causes issue if you want regular republishing. To work around this, my PCC website publishes daily by an ‘empty’ commit on a cron job via Github Actions. PCC doesn’t have the ability to schedule cron jobs so my workaround was to do a dummy commit with GA so that it publishes daily.\n\nname: Daily Connect redeploy nudge\n\non:\n  schedule:\n    # 8:45 AM America/Chicago (DST Mar–Oct)vv\n    - cron: \"57 13 * 3-10 *\"\n    # 8:45 AM America/Chicago (Standard Nov–Feb)\n    - cron: \"57 14 * 11,12,1,2 *\"\n  workflow_dispatch:\n\njobs:\n  nudge:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          persist-credentials: true\n\n      - name: Configure git identity\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n\n      - name: Create empty commit\n        run: |\n          git commit --allow-empty -m 'chore: daily redeploy nudge [skip ci]'\n\n      - name: Push to default branch\n        run: |\n          git push origin \"HEAD:${{ github.event.repository.default_branch }}\""
  },
  {
    "objectID": "content/historical-builds.html",
    "href": "content/historical-builds.html",
    "title": "Historical Build",
    "section": "",
    "text": "You have to check out https://github.com/cfurl/manual_stg4_proces/ to get the deets. It’s pretty well documented in there. There is a pre-compiled version of wgrib2 in that repo. I had wgrib.exe for grib1 files on my desktop. Still had to do a little bulk rename outside of R. Also, the S3 bucket got hairy so i made the archive its own bucket stg4-texas-24hr-historical. I need to build out this section of the webpage because if would be nice to build this outside of Texas if need be one day."
  },
  {
    "objectID": "content/historical-builds.html#adding-2002-2024-records",
    "href": "content/historical-builds.html#adding-2002-2024-records",
    "title": "Historical Build",
    "section": "",
    "text": "You have to check out https://github.com/cfurl/manual_stg4_proces/ to get the deets. It’s pretty well documented in there. There is a pre-compiled version of wgrib2 in that repo. I had wgrib.exe for grib1 files on my desktop. Still had to do a little bulk rename outside of R. Also, the S3 bucket got hairy so i made the archive its own bucket stg4-texas-24hr-historical. I need to build out this section of the webpage because if would be nice to build this outside of Texas if need be one day."
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/roadmap.html",
    "href": "content/roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "To-do in the best order I can come up with:\nSmall things:\n\nDocument your manual processing flows when you load the 2002-2024 onto s3 bucket - add document on this site\nadd 2025 to stg4-texas-24hr-historical and start a daily parquet write to this bucket\nremove bucket$ls call from container\ncreate new current year and historical buckets with just Edwards radar bins (include recharge and artesian), stop carrying state\npoint your makemap container to the newly created EAA S3.\ncreate hyetograph figure (png), containerize, and to docker-compose workflow for daily publish and daily PCC site\nadd your dailystat, makemap, hyetograph container to the stg4-texas-docker repo after you monitor for a few days\nlink this page with github actions to publish daily\nkeep working on this website, a number of things have changed in your workflows 10.build out the 2002-2025 mapper for internal use with EAA S3\n\nBig things:\n\nBuild out the backend on completely AWS framework. Cloudwatch -&gt; Fargate -&gt; S3 -&gt; EC2 hosted web app\nStand-up an hourly AWS real-time site\nGet a well working surface water model, containerize it, and start forcing it hourly.\nAdd the RTMA product to your stack.\nStart in on sub-hourly MRMS processing\n\nDONE - Add daily images of ytd rainfall and 24hour rainfall to your S3 storage. (Found in stg4-edwards-daily-maps Bucket) - Add the historical data that you have on disk to your S3 bucket. (2002-2024 added; in stg4-texas-24hr-historical Bucket) - Figure out how to cache your map tiles to speed up shiny app (Done, hardwired them in container)"
  },
  {
    "objectID": "content/stg4-product.html",
    "href": "content/stg4-product.html",
    "title": "Stage IV Precipitation",
    "section": "",
    "text": "The NCEP “Stage IV” precipitation product is a national, multi-sensor quantitative precipitation estimate (QPE) created by mosaicking the 12 NWS River Forecast Centers’ hourly/6-hourly Stage III analyses. Each of the individual RFCs blends WSR-88D radar with rain-gauge observations and are forecaster quality controlled—onto a common ~4 km HRAP grid for CONUS (with extensions for Alaska and Puerto Rico). It is delivered as hourly, 6-hourly, and 24-hour accumulations. The key distinction with Stage IV from precursor products is that it benefits from manual QC inherited from the RFC inputs, in contrast to automated-only Stage II streams.\nThe Stage IV data is a combination of observations and radar calculated reflectivity. Therefore the data combines the advantage of ground-truth provided by gauges with the spatially complete and high resolution radar data. Operationally, Stage IV is widely used for hydrologic verification, flood event forensics, and as precipitation forcing for models and water-resources studies. Data are distributed in GRIB (now GRIB2, after a 2020 format change) and the hourly product is available for the previous fourteen days in near real-time via NOMADS. A long-term repository is kept with UCAR and updated monthly.\nNear real-time repository\nUCAR long-term repository\n\n\n\nStg4 Conus output from NOMADS"
  },
  {
    "objectID": "content/stg4-product.html#overview",
    "href": "content/stg4-product.html#overview",
    "title": "Stage IV Precipitation",
    "section": "",
    "text": "The NCEP “Stage IV” precipitation product is a national, multi-sensor quantitative precipitation estimate (QPE) created by mosaicking the 12 NWS River Forecast Centers’ hourly/6-hourly Stage III analyses. Each of the individual RFCs blends WSR-88D radar with rain-gauge observations and are forecaster quality controlled—onto a common ~4 km HRAP grid for CONUS (with extensions for Alaska and Puerto Rico). It is delivered as hourly, 6-hourly, and 24-hour accumulations. The key distinction with Stage IV from precursor products is that it benefits from manual QC inherited from the RFC inputs, in contrast to automated-only Stage II streams.\nThe Stage IV data is a combination of observations and radar calculated reflectivity. Therefore the data combines the advantage of ground-truth provided by gauges with the spatially complete and high resolution radar data. Operationally, Stage IV is widely used for hydrologic verification, flood event forensics, and as precipitation forcing for models and water-resources studies. Data are distributed in GRIB (now GRIB2, after a 2020 format change) and the hourly product is available for the previous fourteen days in near real-time via NOMADS. A long-term repository is kept with UCAR and updated monthly.\nNear real-time repository\nUCAR long-term repository\n\n\n\nStg4 Conus output from NOMADS"
  },
  {
    "objectID": "content/tech_overview.html",
    "href": "content/tech_overview.html",
    "title": "Tech Overview",
    "section": "",
    "text": "The rainfall observations are brought to numerical and graphical format from the streaming repository to .html front end through a series of Docker Containers. The containers live on cfurl Docker Hub and orchestration is accomplished through Docker Compose. Radar file output and graphs are stored on AWS S3. The CI/CD pipeline runs daily using GitHub Actions.\nBelow is a brief description of the daily workflow and and some highlights of the technologies used.\n\n24-hour GRIB2 radar file available on NOMADS at 12:55 UTC.\n\nGitHub Actions cron job is triggered at ~ 13:15 to start a series of docker containers.\nContainer 1 - scrapes the GRIB2 file from NOMADS site.\nContainer 2 - GRIB2 taken out of binary and dumps .txt using wgrib2.exe.\nContainer 3 - tidy data, clip to AOI, and write .parquet files to AWS s3 bucket.\nContainer orchestration done through Docker compose - AWS secrets injected at runtime with Github Actions.\nGitHub Actions cron job is triggered at ~ 13:30 prompting a republish of map through connect.posit.cloud.\nConnect.posit.cloud manages AWS credentials and publishes Rshiny app directly from Github frontend repo."
  },
  {
    "objectID": "content/tech_overview.html#high-level-workflow",
    "href": "content/tech_overview.html#high-level-workflow",
    "title": "Tech Overview",
    "section": "",
    "text": "The rainfall observations are brought to numerical and graphical format from the streaming repository to .html front end through a series of Docker Containers. The containers live on cfurl Docker Hub and orchestration is accomplished through Docker Compose. Radar file output and graphs are stored on AWS S3. The CI/CD pipeline runs daily using GitHub Actions.\nBelow is a brief description of the daily workflow and and some highlights of the technologies used.\n\n24-hour GRIB2 radar file available on NOMADS at 12:55 UTC.\n\nGitHub Actions cron job is triggered at ~ 13:15 to start a series of docker containers.\nContainer 1 - scrapes the GRIB2 file from NOMADS site.\nContainer 2 - GRIB2 taken out of binary and dumps .txt using wgrib2.exe.\nContainer 3 - tidy data, clip to AOI, and write .parquet files to AWS s3 bucket.\nContainer orchestration done through Docker compose - AWS secrets injected at runtime with Github Actions.\nGitHub Actions cron job is triggered at ~ 13:30 prompting a republish of map through connect.posit.cloud.\nConnect.posit.cloud manages AWS credentials and publishes Rshiny app directly from Github frontend repo."
  },
  {
    "objectID": "content/tech_overview.html#docker",
    "href": "content/tech_overview.html#docker",
    "title": "Tech Overview",
    "section": "Docker",
    "text": "Docker\nDocker is the most impactful tool I’ve adopted since learning to script. In short: you write code as usual, then package it into an image that includes the exact R runtime, system libraries, packages, and any required assets (e.g., shapefiles). That image is a portable, versioned artifact that runs the same everywhere. You push it to a registry (Docker Hub, AWS ECR, or a private repo), and whenever you need it, you pull the image and start a container—a clean, repeatable run every time. This kills environment drift and the “works on my machine” problem. Containers are lightweight, can be linked to form pipelines, and can mount volumes for persistent data. My images live here: https://hub.docker.com/repositories/cfurl. I learned from ‘Docker in a Month of Lunches’ by Elton Stoneman."
  },
  {
    "objectID": "content/tech_overview.html#wgrib2.exe",
    "href": "content/tech_overview.html#wgrib2.exe",
    "title": "Tech Overview",
    "section": "wgrib2.exe",
    "text": "wgrib2.exe\nwgrib2.exe is a command-line utility for working with GRIB2 meteorological data. It reads GRIB2 files, inspects metadata, and produces concise inventories of the messages they contain. It is widely used to filter and extract variables (e.g., precipitation, temperature, wind), select time steps and vertical levels, and subset by geographic region. The tool can regrid or resample fields to different projections or resolutions, apply standard interpolations, and perform basic calculations. It also supports format conversion, writing outputs to human-readable text/CSV and, when built with the appropriate libraries, to NetCDF. Designed for speed and scripting, it scales well for batch processing and automated workflows on large archives. The “.exe” denotes the Windows build; equivalent functionality is available on Linux and macOS as wgrib2. It is maintained on github by NOAA here: https://github.com/NOAA-EMC/wgrib2/releases"
  },
  {
    "objectID": "content/tech_overview.html#parquet-and-apache-arrow",
    "href": "content/tech_overview.html#parquet-and-apache-arrow",
    "title": "Tech Overview",
    "section": "Parquet and Apache Arrow",
    "text": "Parquet and Apache Arrow\nI picked up Apache Parquet and the Arrow R package at the Posit “Big Data with R” workshop, and they’ve been game-changers for radar data. Parquet is a columnar, compressed, splittable file format built for analytics: it stores columns together, so reads are fast and selective (you only scan the columns and row groups you need), and files are much smaller than CSV. Apache Arrow provides a columnar in-memory format and tooling that lets R (and other languages) scan Parquet lazily, push filters down to disk, and stream data in chunks—so you can work with datasets far larger than RAM instead of “loading everything, then filtering.” In practice, that means querying and summarizing hundreds of gigabytes on a laptop, especially when data are partitioned (e.g., by year/month/day) and stored locally or on S3. For analytics at scale, Parquet + Arrow has effectively replaced CSV for me: smaller, faster, and designed for selective reads—exactly what large radar archives demand."
  },
  {
    "objectID": "content/tech_overview.html#s3",
    "href": "content/tech_overview.html#s3",
    "title": "Tech Overview",
    "section": "S3",
    "text": "S3\nAmazon S3 is durable, scalable object storage used to hold files of any size (e.g., Parquet) with high availability and pay-as-you-go pricing. It’s a common backbone for “data lake” workflows: tools read/write directly via s3://… paths, and in R, aws.s3 and Arrow support streaming/lazy access without local copies. Access control is handled with IAM; data governance via versioning, lifecycle policies, and server-side encryption. It’s low-maintenance, widely supported, and integrates cleanly with modern analytics stacks. I’ve used this tutorial regularly: https://www.gormanalysis.com/blog/connecting-to-aws-s3-with-r/"
  },
  {
    "objectID": "content/tech_overview.html#github-actions",
    "href": "content/tech_overview.html#github-actions",
    "title": "Tech Overview",
    "section": "Github Actions",
    "text": "Github Actions\nGitHub Actions is GitHub’s built-in CI/CD platform that runs workflows on events (push, PR), schedules (cron), or manual dispatch. Workflows are defined as YAML files stored with your code, so versioning and reviews are straightforward. Runners can execute shell steps, build and run Docker containers (including docker-compose), and orchestrate multi-step jobs across OS targets. Sensitive values are injected at runtime via Repository/Environment Secrets (e.g., AWS credentials), avoiding hard-coding in images or source. Typical uses include scheduled data jobs, automated tests/linting, container builds and publishes, and static-site deploys (e.g., rebuilding documentation or a homepage on a timer). It’s a practical way to automate repeatable tasks directly from the repo without managing separate infrastructure—and can later hand off to a cloud-native backend if needed."
  },
  {
    "objectID": "content/tech_overview.html#posit.connect.cloud",
    "href": "content/tech_overview.html#posit.connect.cloud",
    "title": "Tech Overview",
    "section": "posit.connect.cloud",
    "text": "posit.connect.cloud\nI’ve spent less than a day building the rshiny app you see here. Posit Connect (cloud-managed) is a production publishing platform for R/Python: it hosts Shiny apps, Quarto/R Markdown sites, and APIs (e.g., Plumber) with HTTPS, auth, and versioned deploys. It handles builds and dependencies, environment variables/secrets, scheduled jobs (e.g., daily refresh), and integrates cleanly with Git/GitHub Actions for push-to-deploy. You get logs, usage metrics, access controls/SSO, and simple scaling (multiple processes/workers) without running servers. It’s a fast way to deliver interactive analytics now, with a clear path to containerized hosting on AWS later (ECS/Fargate, EKS, or EC2) using the same CI/secrets patterns."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automated Radar Rainfall Mapping",
    "section": "",
    "text": "This guide tracks the development of an automated radar precipitation processing system across the Edwards Aquifer Recharge Zone using the Stage IV QPE product from NCEP. Following the framework described in this guide, users can automate collection of radar rainfall across a watershed that can be used to track droughts, forecast reservoir inflows, estimate soil moisture, force surface water models, etc.\nRainfall maps and basin averaged accumulation values are published daily at approximately 8:00 AM CST (9:00 CDT) here - Edwards Aquifer rainfall.\n\n\n\nYear-to-Date Precipitation Totals across Edwards Aquifer Recharge Zone"
  }
]