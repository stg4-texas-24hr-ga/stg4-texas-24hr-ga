[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automated Radar Rainfall Mapping",
    "section": "",
    "text": "This guide tracks the development of an automated radar precipitation processing system across the Edwards Aquifer Recharge Zone using the Stage IV QPE product from NCEP. Following the framework described in this guide, users can automate collection of radar rainfall across a watershed that can be used to track droughts, forecast reservoir inflows, estimate soil moisture, force surface water models, etc.\nRainfall maps and basin averaged accumulation values are published daily at approximately 8:00 AM CST (9:00 CDT) here - Edwards Aquifer rainfall.\n\n\n\nYear-to-Date Precipitation Totals across Edwards Aquifer Recharge Zone",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/stg4-product.html",
    "href": "content/stg4-product.html",
    "title": "Stage IV Precipitation",
    "section": "",
    "text": "The NCEP “Stage IV” precipitation product is a national, multi-sensor quantitative precipitation estimate (QPE) created by mosaicking the 12 NWS River Forecast Centers’ hourly/6-hourly Stage III analyses. Each of the individual RFCs blends WSR-88D radar with rain-gauge observations and are forecaster quality controlled—onto a common ~4 km HRAP grid for CONUS (with extensions for Alaska and Puerto Rico). It is delivered as hourly, 6-hourly, and 24-hour accumulations. The key distinction with Stage IV from precursor products is that it benefits from manual QC inherited from the RFC inputs, in contrast to automated-only Stage II streams.\nThe Stage IV data is a combination of observations and radar calculated reflectivity. Therefore the data combines the advantage of ground-truth provided by gauges with the spatially complete and high resolution radar data. Operationally, Stage IV is widely used for hydrologic verification, flood event forensics, and as precipitation forcing for models and water-resources studies. Data are distributed in GRIB (now GRIB2, after a 2020 format change) and the hourly product is available for the previous fourteen days in near real-time via NOMADS. A long-term repository is kept with UCAR and updated monthly.\nNear real-time repository\nUCAR long-term repository\n ## Stage4 v PRISM v DAYMET4\nPRISM - Since January 2002, both the D1 and D2 daily/monthly precipitation versions use a combination of CAI (climatologically-aided interpolation of gauges) and Stage 4 RADAR interpolation in the central and eastern U.S. For earlier years, PRISM relies solely on station observations plud DEM and knowledge-based weighting, plus CAI tricks for daily/monthly. PRISM NCAR Climate data guide\nBelow are two storms from North Texas. The first is from the pre-radar era (1998) and the second is with Stage4 integration (2012). Note the difference in spatial complexity.\nknitr::include_graphics(c(\n  \"content_img/prism_crop_19981203.png\",\n  \"content_img/prism_crop_20120404.png\"\n))\n\n\n\n\n\n\nPRISM precipitation in North Texas 12-3-1998 (left) and 04-04-2012 (right).\n\n\n\n\n\n\n\nPRISM precipitation in North Texas 12-3-1998 (left) and 04-04-2012 (right).\n\n\n\n\n\n\n\n\nPRISM precipitation North Texas 12-3-1998\n\n\n \nDAYMET4 - The core inputs for Daymet are daily precipitation and temperature from ground-based weather stations, primarily GHCN-Daily. The V4 Scientific Data paper notes that radar-based precipitation is used as an independent validation dataset, not as an input to the analysis. NCAR Climate data guide cites daily precipitation product not making use of potentially valuable information from radar networks as a key product limitation. DAYMET NCAR Climate data guide\n\n\n\nDAYMET gage locations",
    "crumbs": [
      "Stage IV precipitation"
    ]
  },
  {
    "objectID": "content/stg4-product.html#stage4-overview",
    "href": "content/stg4-product.html#stage4-overview",
    "title": "Stage IV Precipitation",
    "section": "",
    "text": "The NCEP “Stage IV” precipitation product is a national, multi-sensor quantitative precipitation estimate (QPE) created by mosaicking the 12 NWS River Forecast Centers’ hourly/6-hourly Stage III analyses. Each of the individual RFCs blends WSR-88D radar with rain-gauge observations and are forecaster quality controlled—onto a common ~4 km HRAP grid for CONUS (with extensions for Alaska and Puerto Rico). It is delivered as hourly, 6-hourly, and 24-hour accumulations. The key distinction with Stage IV from precursor products is that it benefits from manual QC inherited from the RFC inputs, in contrast to automated-only Stage II streams.\nThe Stage IV data is a combination of observations and radar calculated reflectivity. Therefore the data combines the advantage of ground-truth provided by gauges with the spatially complete and high resolution radar data. Operationally, Stage IV is widely used for hydrologic verification, flood event forensics, and as precipitation forcing for models and water-resources studies. Data are distributed in GRIB (now GRIB2, after a 2020 format change) and the hourly product is available for the previous fourteen days in near real-time via NOMADS. A long-term repository is kept with UCAR and updated monthly.\nNear real-time repository\nUCAR long-term repository\n ## Stage4 v PRISM v DAYMET4\nPRISM - Since January 2002, both the D1 and D2 daily/monthly precipitation versions use a combination of CAI (climatologically-aided interpolation of gauges) and Stage 4 RADAR interpolation in the central and eastern U.S. For earlier years, PRISM relies solely on station observations plud DEM and knowledge-based weighting, plus CAI tricks for daily/monthly. PRISM NCAR Climate data guide\nBelow are two storms from North Texas. The first is from the pre-radar era (1998) and the second is with Stage4 integration (2012). Note the difference in spatial complexity.\nknitr::include_graphics(c(\n  \"content_img/prism_crop_19981203.png\",\n  \"content_img/prism_crop_20120404.png\"\n))\n\n\n\n\n\n\nPRISM precipitation in North Texas 12-3-1998 (left) and 04-04-2012 (right).\n\n\n\n\n\n\n\nPRISM precipitation in North Texas 12-3-1998 (left) and 04-04-2012 (right).\n\n\n\n\n\n\n\n\nPRISM precipitation North Texas 12-3-1998\n\n\n \nDAYMET4 - The core inputs for Daymet are daily precipitation and temperature from ground-based weather stations, primarily GHCN-Daily. The V4 Scientific Data paper notes that radar-based precipitation is used as an independent validation dataset, not as an input to the analysis. NCAR Climate data guide cites daily precipitation product not making use of potentially valuable information from radar networks as a key product limitation. DAYMET NCAR Climate data guide\n\n\n\nDAYMET gage locations",
    "crumbs": [
      "Stage IV precipitation"
    ]
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/frontend.html",
    "href": "content/frontend.html",
    "title": "Frontend",
    "section": "",
    "text": "I recall going through a lot of difficulty years ago trying to get the grid and bin mapping straightened away like I wanted it. One of the problems is that the HRAP cell size changes with Latitude. All of the answers lie in: “Coordinate Transformations for Using NEXRAD data in GIS-based Hydrologic Modeling’ by Reed and Maidment (1999). I may have read this journal article more than any other article. I was able to build a shapefile of the HRAP grid that I validated through published bin sizes (in Reed and Maidment) with this article and a pearl script from Stuart foote that wrote the corner intersecting vertices of the of the HRAP grid. Complicated stuff. I need to put the shapefile of the hrap grid on a github repo somewhere. I have .csv for state of Texas that associates lat-lon with HRAP Grid ID. My shapefile holds HRAP Grid ID and Stg4 dumps center point (lan-lon) of radar bin. So, i can quickly map rainfall amount to HRAP Grid and my shapefile holds the size of the radar bin.\nThe next time I work in a new geographic location I will document handling of the grid better.\n\n\n\nThe map displaying rainfall across the basin is a simple Rshiny app (app.R) that is hosted on posit.connect.cloud. This ggplot was written by Tanya (cfurl/stg4_edwards).\n\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyr\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"aws.s3\")\nlibrary(\"sf\")\nlibrary(\"ggspatial\")\nlibrary(\"ggplot2\")\nlibrary(\"prettymapr\")\nlibrary(\"shiny\")\n#library(\"ggiraph\")\n\n######################## Some S3 things #####################\n# remove this from container setup, this gives your local dev the AWS access\n#readRenviron(\"../.Renviron\") # this is for keys one level up from root directory\n#readRenviron(\".Renviron\") # when it's in gitignore\n\nrequired &lt;- c(\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_DEFAULT_REGION\")\nmissing  &lt;- required[Sys.getenv(required) == \"\"]\nif (length(missing)) {\n  stop(\"Missing env vars on Connect: \", paste(missing, collapse = \", \"))\n}\n\n# make sure you can connect to your bucket and open SubTreeFileSystem and identify path\n# then connect to the .parq files on the s3 storage\nbucket &lt;- s3_bucket(\"stg4-texas-24hr\")\ns3_path &lt;- bucket$path(\"\")\nstg4_24hr_texas_parq &lt;- open_dataset(s3_path)\n\n############################ time stamps #############\n\ncurrent_utc_date_time &lt;- with_tz(Sys.time(), \"UTC\")\ncurrent_central_date_time &lt;- with_tz(Sys.time(), \"America/Chicago\")\ncurrent_utc_time &lt;- format(with_tz(Sys.time(), \"UTC\"), \"%H:%M:%S\")\ncurrent_utc_date &lt;- as_date(with_tz(Sys.time(), \"UTC\"))\n\n\n# parquet gets populated with most recent 24hr file at 13:45 UTC\n# new shiny page pushed to posit.cloud.connect at 13:57 UTC\n\n# This solves the problem of when the UTC time is in the current day, but the STG4 hasn't dopped yet, so the script is looking\n# for parquet files that haven't been populated yet. Until you reach 13:52 (when parquet is safely populated), it kicks you\n# back to yesterday\n#t1_offset &lt;- case_when (current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -0, TRUE ~ 0)\n#t1_offset &lt;- case_when (current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -1, TRUE ~ 0)\n#t2_offset &lt;- case_when( current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -2, TRUE ~ -1)\n\n# Create exact timestamps (UTC) for noon on yesterday and today\nt1 &lt;- as.POSIXct(paste(Sys.Date() - 0, \"12:00:00\"), tz = \"UTC\")  # today 0\nt2 &lt;- as.POSIXct(paste(Sys.Date() - 1, \"12:00:00\"), tz = \"UTC\") # yesterday 1\n\n#create some timestamps for labels\n\n\n\n\n# This is where you query the parq files by time (not location yet)\n# carrying these commands around for whole state, could clip first\n\ntime_check &lt;- stg4_24hr_texas_parq |&gt;\n  select(time)|&gt;\n  filter (time %in% c(t1)) |&gt;\n  collect()\n\nif (nrow(time_check) == 0) {\n  time_filter&lt;-t2\n} else {\n  time_filter&lt;-t1\n}\n\nd &lt;- stg4_24hr_texas_parq |&gt;\n    filter (time %in% c(time_filter)) |&gt;\n    group_by (grib_id) %&gt;%\n    summarize(\n      sum_rain = sum(rain_mm, na.rm=TRUE)) %&gt;%\n    arrange(desc(sum_rain)) |&gt;\n    collect()\n\n# Make local time labels for main title. Precipitation from xxxx - xxxx\nend_time_local &lt;- with_tz(time_filter, \"America/Chicago\")\nbegin_time_local &lt;- end_time_local - days(1)\n\n\n# call the gis layers you want mapped\nmap &lt;- sf::read_sf(\"./gis/usgs_dissolved.shp\")\nstreams &lt;- read_sf(\"./gis/streams_recharge.shp\")\nlakes &lt;- read_sf(\"./gis/reservoirs.shp\")\n\n# this is where you subset the statewide set of bins by your shapefile area of interest\nmap_rain &lt;- map|&gt;\n  left_join(d, by = \"grib_id\")|&gt;\n  mutate(cubic_m_precip = bin_area * sum_rain * 0.001)|&gt;\n  mutate(sum_rain_in = sum_rain/25.4)\n\n# Mapping function edited from Tanya's work\nplot_bin_map&lt;-function(\n    title = 'Edwards Aquifer Recharge Zone',\n    subtitle= NA,\n    note_title = NA,\n    font = \"Open Sans\",\n    map_rain = NA,\n    map_streams = NA, \n    map_lakes = NA,\n    pal_water='black',\n    pal_title='white',\n    pal_subtitle='white',\n    pal_outline='black',\n    pal_bin_outline='black',\n    pal_legend_text='white',\n    bin_alpha = 0.7,\n    map_type='cartodark'\n){\n  \n  bbox &lt;- st_bbox(c(\n    xmin = -100.85,\n    ymin = 29.0, \n    xmax = -97.75, \n    ymax = 30.47\n  ), crs = 4326)\n  \n  coord_sys&lt;-3857\n  \n  # Convert bbox to an sf object for ggplot compatibility\n  bbox_sf &lt;- st_as_sfc(bbox)\n  bbox_transformed &lt;- st_transform(bbox_sf, crs = coord_sys)\n  \n  outline &lt;- map |&gt; summarise(geometry = st_union(geometry)) |&gt; st_cast(\"MULTILINESTRING\")  \n  \n  title_pos &lt;- st_sfc(st_point(c(-100.88, 30.43)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  subtitle_pos &lt;- st_sfc(st_point(c(-100.88, 30.43 - 0.085)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  note_title_pos &lt;- st_sfc(st_point(c(-100.88, 30.43 - 1.41)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  # --- Static legend settings (always show full range) ---\n  rain_breaks  &lt;- c(0, 0.1, 0.25, 0.5, 1, 2, 3, 4, 6, 8, 10, 12)\n  rain_labels  &lt;- c(\"0\",\"0.1\",\"0.25\",\"0.5\",\"1\",\"2\",\"3\",\"4\",\"6\",\"8\",\"10\",\"12+\")\n  rain_limits  &lt;- c(0, 12)\n  \n  # --- Set 0 rainfall to NA for transparency ---\n  map_rain &lt;- map_rain |&gt;\n    mutate(fill_val = ifelse(sum_rain_in == 0, NA_real_, sum_rain_in))\n  \n  plot&lt;-ggplot()+\n    annotation_map_tile(\n      type = map_type,  # Use the \"Carto Light\" basemap\n      zoom = 9  # Adjust zoom level as needed\n    )+\n    annotate(geom=\"text\",x= title_pos$X,y=title_pos$Y,label=title,size=8,hjust=0, color = pal_title, family=font, fontface='bold')+\n    annotate(geom=\"text\",x= subtitle_pos$X,y=subtitle_pos$Y,label=subtitle,size=5,hjust=0, color = pal_subtitle, family=font)+\n    annotate(geom=\"text\",x=  note_title_pos$X,y= note_title_pos$Y,label=note_title,size=2,hjust=0, color = pal_subtitle, family=font)+\n    geom_sf(data = map_rain, mapping = aes(fill = fill_val), color = pal_bin_outline, alpha = bin_alpha, na.rm = FALSE) +\n    geom_sf(data = outline|&gt;st_transform(crs = coord_sys), color = pal_outline, linewidth = 0.4) +  \n    geom_sf(data=map_lakes|&gt;st_transform(crs = coord_sys), fill= pal_water, color= pal_water, linewidth = 0.2)+\n    geom_sf(data=map_streams|&gt;st_transform(crs = coord_sys), color= pal_water)+\n    \n    scale_fill_stepsn(\n      colours = c(\"#82D3F0\",\"#0826A2\",\"#22FE05\",\"#248418\",\n                  \"#F6FB07\",\"#FFC348\",\"#E01E17\",\"#8C302C\",\n                  \"#CC17DA\",\"#AE60B3\",\"#FDF5FF\"),\n      breaks    = rain_breaks,\n      limits    = rain_limits,\n      labels    = rain_labels,\n      oob       = scales::squish,\n      name      = \"Rainfall (in)\",\n      na.value  = NA  # keep transparency for NA (zero rainfall)\n    ) +\n    guides(\n      fill = guide_colorsteps(\n        title.position = \"top\",\n        title.vjust = 0.1,\n        show.limits = TRUE\n      )\n    )+\n    coord_sf(\n      xlim = c(st_bbox(bbox_transformed)[\"xmin\"], st_bbox(bbox_transformed)[\"xmax\"]),\n      ylim = c(st_bbox(bbox_transformed)[\"ymin\"], st_bbox(bbox_transformed)[\"ymax\"])\n    ) +\n    theme_void()+\n    theme(\n      text = element_text(family=font),\n      legend.position = \"inside\",\n      legend.position.inside = c(0.70,0.1),  \n      legend.direction = \"horizontal\", \n      legend.margin = margin(t = 0, r = 10, b = 0, l = 10),\n      legend.title = element_text(size = 10, face='bold', color=pal_legend_text), \n      legend.text = element_text(size = 9, color=pal_legend_text),  \n      legend.key.width = unit(2.5, \"cm\"), \n      legend.key.height = unit(0.5, \"cm\")  \n    )\n  \n  return(plot)\n}\n\n\n\n#ui &lt;- fluidPage(\n#  tags$head(tags$title(\"Rainfall Map\")),\n#  fluidRow(\n#    column(\n#      width = 12,\n#      plotOutput(\"rain_map\", height = \"800px\")\n#    )\n#  )\n#)\n\n\nui &lt;- fluidPage(\n  style = \"padding:0; margin:0;\",\n  tags$head(tags$title(\"Rainfall Map\")),\n  plotOutput(\"rain_map\", width = \"100%\", height = \"100vh\")\n)\n\n\n\n\n\nserver &lt;- function(input, output, session) {\n  output$rain_map &lt;- renderPlot({\n                            plot_bin_map(title = 'Edwards Aquifer Recharge Zone',\n                            subtitle = paste(\"Precipitation from\", format(begin_time_local, \"%Y-%m-%d %H:%M %Z\"), \"to\",format(end_time_local, \"%Y-%m-%d %H:%M %Z\")),\n                            note_title = paste(\"This map queried .parq at\", format(current_utc_date_time, \"%Y-%m-%d %H:%M %Z\"), \"and\", format(current_central_date_time, \"%Y-%m-%d %H:%M %Z\")) ,\n                            font = \"\",\n                            map_rain = map_rain,\n                            map_streams = streams,\n                            map_lakes = lakes,\n                            #pal_water='#697984',\n                            pal_water = '#2C6690',\n                            pal_title='black',\n                            # pal_legend = 'YlOrRd',\n                            bin_alpha = 0.9,\n                            pal_subtitle='black',\n                            pal_outline=\"#697984\",\n                            pal_bin_outline='white',\n                            pal_legend_text='black',\n                            map_type='cartolight')}, res = 144)  # crisp output\n}\n\nshinyApp(ui, server)\n\n\n\n\nPosit.Connect.Cloud is nice for a few reasons 1). you can stand up a website, quarto build, or shiny app in a few minutes 2). It publishes directly from your github repository so you can develop in the same place you publish, and 3). You can add secrets to inject passwords at runtime - so you can interact with you AWS environment with very little pain.\nIn this case, I have a front end repository up at /stg4-texas-24hr-frontend-actions however my website is not publishing from there. I am publishing from cfurl/st4_front_query_map. My posit.connect.cloud is associated with ‘cfurl’ repository and the third party app that links github-PCC can only be associated with one repo owner (in this case cfurl).\nThe PCC website republishes everytime there is a commit to the github repo. This is fine when your developing, but causes issue if you want regular republishing. To work around this, my PCC website publishes daily by an ‘empty’ commit on a cron job via Github Actions. PCC doesn’t have the ability to schedule cron jobs so my workaround was to do a dummy commit with GA so that it publishes daily.\n\nname: Daily Connect redeploy nudge\n\non:\n  schedule:\n    # 8:45 AM America/Chicago (DST Mar–Oct)vv\n    - cron: \"57 13 * 3-10 *\"\n    # 8:45 AM America/Chicago (Standard Nov–Feb)\n    - cron: \"57 14 * 11,12,1,2 *\"\n  workflow_dispatch:\n\njobs:\n  nudge:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          persist-credentials: true\n\n      - name: Configure git identity\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n\n      - name: Create empty commit\n        run: |\n          git commit --allow-empty -m 'chore: daily redeploy nudge [skip ci]'\n\n      - name: Push to default branch\n        run: |\n          git push origin \"HEAD:${{ github.event.repository.default_branch }}\"",
    "crumbs": [
      "Frontend"
    ]
  },
  {
    "objectID": "content/frontend.html#the-details",
    "href": "content/frontend.html#the-details",
    "title": "Frontend",
    "section": "",
    "text": "I recall going through a lot of difficulty years ago trying to get the grid and bin mapping straightened away like I wanted it. One of the problems is that the HRAP cell size changes with Latitude. All of the answers lie in: “Coordinate Transformations for Using NEXRAD data in GIS-based Hydrologic Modeling’ by Reed and Maidment (1999). I may have read this journal article more than any other article. I was able to build a shapefile of the HRAP grid that I validated through published bin sizes (in Reed and Maidment) with this article and a pearl script from Stuart foote that wrote the corner intersecting vertices of the of the HRAP grid. Complicated stuff. I need to put the shapefile of the hrap grid on a github repo somewhere. I have .csv for state of Texas that associates lat-lon with HRAP Grid ID. My shapefile holds HRAP Grid ID and Stg4 dumps center point (lan-lon) of radar bin. So, i can quickly map rainfall amount to HRAP Grid and my shapefile holds the size of the radar bin.\nThe next time I work in a new geographic location I will document handling of the grid better.\n\n\n\nThe map displaying rainfall across the basin is a simple Rshiny app (app.R) that is hosted on posit.connect.cloud. This ggplot was written by Tanya (cfurl/stg4_edwards).\n\nlibrary(\"arrow\")\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"tidyr\")\nlibrary(\"readr\")\nlibrary(\"stringr\")\nlibrary(\"aws.s3\")\nlibrary(\"sf\")\nlibrary(\"ggspatial\")\nlibrary(\"ggplot2\")\nlibrary(\"prettymapr\")\nlibrary(\"shiny\")\n#library(\"ggiraph\")\n\n######################## Some S3 things #####################\n# remove this from container setup, this gives your local dev the AWS access\n#readRenviron(\"../.Renviron\") # this is for keys one level up from root directory\n#readRenviron(\".Renviron\") # when it's in gitignore\n\nrequired &lt;- c(\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_DEFAULT_REGION\")\nmissing  &lt;- required[Sys.getenv(required) == \"\"]\nif (length(missing)) {\n  stop(\"Missing env vars on Connect: \", paste(missing, collapse = \", \"))\n}\n\n# make sure you can connect to your bucket and open SubTreeFileSystem and identify path\n# then connect to the .parq files on the s3 storage\nbucket &lt;- s3_bucket(\"stg4-texas-24hr\")\ns3_path &lt;- bucket$path(\"\")\nstg4_24hr_texas_parq &lt;- open_dataset(s3_path)\n\n############################ time stamps #############\n\ncurrent_utc_date_time &lt;- with_tz(Sys.time(), \"UTC\")\ncurrent_central_date_time &lt;- with_tz(Sys.time(), \"America/Chicago\")\ncurrent_utc_time &lt;- format(with_tz(Sys.time(), \"UTC\"), \"%H:%M:%S\")\ncurrent_utc_date &lt;- as_date(with_tz(Sys.time(), \"UTC\"))\n\n\n# parquet gets populated with most recent 24hr file at 13:45 UTC\n# new shiny page pushed to posit.cloud.connect at 13:57 UTC\n\n# This solves the problem of when the UTC time is in the current day, but the STG4 hasn't dopped yet, so the script is looking\n# for parquet files that haven't been populated yet. Until you reach 13:52 (when parquet is safely populated), it kicks you\n# back to yesterday\n#t1_offset &lt;- case_when (current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -0, TRUE ~ 0)\n#t1_offset &lt;- case_when (current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -1, TRUE ~ 0)\n#t2_offset &lt;- case_when( current_utc_time &gt;= \"00:00\" & current_utc_time &lt;= \"13:52\" ~ -2, TRUE ~ -1)\n\n# Create exact timestamps (UTC) for noon on yesterday and today\nt1 &lt;- as.POSIXct(paste(Sys.Date() - 0, \"12:00:00\"), tz = \"UTC\")  # today 0\nt2 &lt;- as.POSIXct(paste(Sys.Date() - 1, \"12:00:00\"), tz = \"UTC\") # yesterday 1\n\n#create some timestamps for labels\n\n\n\n\n# This is where you query the parq files by time (not location yet)\n# carrying these commands around for whole state, could clip first\n\ntime_check &lt;- stg4_24hr_texas_parq |&gt;\n  select(time)|&gt;\n  filter (time %in% c(t1)) |&gt;\n  collect()\n\nif (nrow(time_check) == 0) {\n  time_filter&lt;-t2\n} else {\n  time_filter&lt;-t1\n}\n\nd &lt;- stg4_24hr_texas_parq |&gt;\n    filter (time %in% c(time_filter)) |&gt;\n    group_by (grib_id) %&gt;%\n    summarize(\n      sum_rain = sum(rain_mm, na.rm=TRUE)) %&gt;%\n    arrange(desc(sum_rain)) |&gt;\n    collect()\n\n# Make local time labels for main title. Precipitation from xxxx - xxxx\nend_time_local &lt;- with_tz(time_filter, \"America/Chicago\")\nbegin_time_local &lt;- end_time_local - days(1)\n\n\n# call the gis layers you want mapped\nmap &lt;- sf::read_sf(\"./gis/usgs_dissolved.shp\")\nstreams &lt;- read_sf(\"./gis/streams_recharge.shp\")\nlakes &lt;- read_sf(\"./gis/reservoirs.shp\")\n\n# this is where you subset the statewide set of bins by your shapefile area of interest\nmap_rain &lt;- map|&gt;\n  left_join(d, by = \"grib_id\")|&gt;\n  mutate(cubic_m_precip = bin_area * sum_rain * 0.001)|&gt;\n  mutate(sum_rain_in = sum_rain/25.4)\n\n# Mapping function edited from Tanya's work\nplot_bin_map&lt;-function(\n    title = 'Edwards Aquifer Recharge Zone',\n    subtitle= NA,\n    note_title = NA,\n    font = \"Open Sans\",\n    map_rain = NA,\n    map_streams = NA, \n    map_lakes = NA,\n    pal_water='black',\n    pal_title='white',\n    pal_subtitle='white',\n    pal_outline='black',\n    pal_bin_outline='black',\n    pal_legend_text='white',\n    bin_alpha = 0.7,\n    map_type='cartodark'\n){\n  \n  bbox &lt;- st_bbox(c(\n    xmin = -100.85,\n    ymin = 29.0, \n    xmax = -97.75, \n    ymax = 30.47\n  ), crs = 4326)\n  \n  coord_sys&lt;-3857\n  \n  # Convert bbox to an sf object for ggplot compatibility\n  bbox_sf &lt;- st_as_sfc(bbox)\n  bbox_transformed &lt;- st_transform(bbox_sf, crs = coord_sys)\n  \n  outline &lt;- map |&gt; summarise(geometry = st_union(geometry)) |&gt; st_cast(\"MULTILINESTRING\")  \n  \n  title_pos &lt;- st_sfc(st_point(c(-100.88, 30.43)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  subtitle_pos &lt;- st_sfc(st_point(c(-100.88, 30.43 - 0.085)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  note_title_pos &lt;- st_sfc(st_point(c(-100.88, 30.43 - 1.41)), crs = 4326) |&gt; \n    st_transform(crs = 3857) |&gt; \n    st_coordinates() |&gt; as.data.frame()\n  \n  # --- Static legend settings (always show full range) ---\n  rain_breaks  &lt;- c(0, 0.1, 0.25, 0.5, 1, 2, 3, 4, 6, 8, 10, 12)\n  rain_labels  &lt;- c(\"0\",\"0.1\",\"0.25\",\"0.5\",\"1\",\"2\",\"3\",\"4\",\"6\",\"8\",\"10\",\"12+\")\n  rain_limits  &lt;- c(0, 12)\n  \n  # --- Set 0 rainfall to NA for transparency ---\n  map_rain &lt;- map_rain |&gt;\n    mutate(fill_val = ifelse(sum_rain_in == 0, NA_real_, sum_rain_in))\n  \n  plot&lt;-ggplot()+\n    annotation_map_tile(\n      type = map_type,  # Use the \"Carto Light\" basemap\n      zoom = 9  # Adjust zoom level as needed\n    )+\n    annotate(geom=\"text\",x= title_pos$X,y=title_pos$Y,label=title,size=8,hjust=0, color = pal_title, family=font, fontface='bold')+\n    annotate(geom=\"text\",x= subtitle_pos$X,y=subtitle_pos$Y,label=subtitle,size=5,hjust=0, color = pal_subtitle, family=font)+\n    annotate(geom=\"text\",x=  note_title_pos$X,y= note_title_pos$Y,label=note_title,size=2,hjust=0, color = pal_subtitle, family=font)+\n    geom_sf(data = map_rain, mapping = aes(fill = fill_val), color = pal_bin_outline, alpha = bin_alpha, na.rm = FALSE) +\n    geom_sf(data = outline|&gt;st_transform(crs = coord_sys), color = pal_outline, linewidth = 0.4) +  \n    geom_sf(data=map_lakes|&gt;st_transform(crs = coord_sys), fill= pal_water, color= pal_water, linewidth = 0.2)+\n    geom_sf(data=map_streams|&gt;st_transform(crs = coord_sys), color= pal_water)+\n    \n    scale_fill_stepsn(\n      colours = c(\"#82D3F0\",\"#0826A2\",\"#22FE05\",\"#248418\",\n                  \"#F6FB07\",\"#FFC348\",\"#E01E17\",\"#8C302C\",\n                  \"#CC17DA\",\"#AE60B3\",\"#FDF5FF\"),\n      breaks    = rain_breaks,\n      limits    = rain_limits,\n      labels    = rain_labels,\n      oob       = scales::squish,\n      name      = \"Rainfall (in)\",\n      na.value  = NA  # keep transparency for NA (zero rainfall)\n    ) +\n    guides(\n      fill = guide_colorsteps(\n        title.position = \"top\",\n        title.vjust = 0.1,\n        show.limits = TRUE\n      )\n    )+\n    coord_sf(\n      xlim = c(st_bbox(bbox_transformed)[\"xmin\"], st_bbox(bbox_transformed)[\"xmax\"]),\n      ylim = c(st_bbox(bbox_transformed)[\"ymin\"], st_bbox(bbox_transformed)[\"ymax\"])\n    ) +\n    theme_void()+\n    theme(\n      text = element_text(family=font),\n      legend.position = \"inside\",\n      legend.position.inside = c(0.70,0.1),  \n      legend.direction = \"horizontal\", \n      legend.margin = margin(t = 0, r = 10, b = 0, l = 10),\n      legend.title = element_text(size = 10, face='bold', color=pal_legend_text), \n      legend.text = element_text(size = 9, color=pal_legend_text),  \n      legend.key.width = unit(2.5, \"cm\"), \n      legend.key.height = unit(0.5, \"cm\")  \n    )\n  \n  return(plot)\n}\n\n\n\n#ui &lt;- fluidPage(\n#  tags$head(tags$title(\"Rainfall Map\")),\n#  fluidRow(\n#    column(\n#      width = 12,\n#      plotOutput(\"rain_map\", height = \"800px\")\n#    )\n#  )\n#)\n\n\nui &lt;- fluidPage(\n  style = \"padding:0; margin:0;\",\n  tags$head(tags$title(\"Rainfall Map\")),\n  plotOutput(\"rain_map\", width = \"100%\", height = \"100vh\")\n)\n\n\n\n\n\nserver &lt;- function(input, output, session) {\n  output$rain_map &lt;- renderPlot({\n                            plot_bin_map(title = 'Edwards Aquifer Recharge Zone',\n                            subtitle = paste(\"Precipitation from\", format(begin_time_local, \"%Y-%m-%d %H:%M %Z\"), \"to\",format(end_time_local, \"%Y-%m-%d %H:%M %Z\")),\n                            note_title = paste(\"This map queried .parq at\", format(current_utc_date_time, \"%Y-%m-%d %H:%M %Z\"), \"and\", format(current_central_date_time, \"%Y-%m-%d %H:%M %Z\")) ,\n                            font = \"\",\n                            map_rain = map_rain,\n                            map_streams = streams,\n                            map_lakes = lakes,\n                            #pal_water='#697984',\n                            pal_water = '#2C6690',\n                            pal_title='black',\n                            # pal_legend = 'YlOrRd',\n                            bin_alpha = 0.9,\n                            pal_subtitle='black',\n                            pal_outline=\"#697984\",\n                            pal_bin_outline='white',\n                            pal_legend_text='black',\n                            map_type='cartolight')}, res = 144)  # crisp output\n}\n\nshinyApp(ui, server)\n\n\n\n\nPosit.Connect.Cloud is nice for a few reasons 1). you can stand up a website, quarto build, or shiny app in a few minutes 2). It publishes directly from your github repository so you can develop in the same place you publish, and 3). You can add secrets to inject passwords at runtime - so you can interact with you AWS environment with very little pain.\nIn this case, I have a front end repository up at /stg4-texas-24hr-frontend-actions however my website is not publishing from there. I am publishing from cfurl/st4_front_query_map. My posit.connect.cloud is associated with ‘cfurl’ repository and the third party app that links github-PCC can only be associated with one repo owner (in this case cfurl).\nThe PCC website republishes everytime there is a commit to the github repo. This is fine when your developing, but causes issue if you want regular republishing. To work around this, my PCC website publishes daily by an ‘empty’ commit on a cron job via Github Actions. PCC doesn’t have the ability to schedule cron jobs so my workaround was to do a dummy commit with GA so that it publishes daily.\n\nname: Daily Connect redeploy nudge\n\non:\n  schedule:\n    # 8:45 AM America/Chicago (DST Mar–Oct)vv\n    - cron: \"57 13 * 3-10 *\"\n    # 8:45 AM America/Chicago (Standard Nov–Feb)\n    - cron: \"57 14 * 11,12,1,2 *\"\n  workflow_dispatch:\n\njobs:\n  nudge:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          persist-credentials: true\n\n      - name: Configure git identity\n        run: |\n          git config user.name \"github-actions[bot]\"\n          git config user.email \"41898282+github-actions[bot]@users.noreply.github.com\"\n\n      - name: Create empty commit\n        run: |\n          git commit --allow-empty -m 'chore: daily redeploy nudge [skip ci]'\n\n      - name: Push to default branch\n        run: |\n          git push origin \"HEAD:${{ github.event.repository.default_branch }}\"",
    "crumbs": [
      "Frontend"
    ]
  },
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "About",
    "section": "",
    "text": "Pirate Weather\nKEWX Station\nr4ds\nDocker in a month of lunches\nHappy git with R\nArrow R Package\nNOAA Quarto website\nIowa State Mesonet\nRTMA-URMA\nNCEP Products inventory\nQuarto build i like\nstg4 data nomads",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "content/backend.html",
    "href": "content/backend.html",
    "title": "Backend",
    "section": "",
    "text": "This section of the webpage is where I’m keeping notes on specific aspects of the project. It’s a little bit of a hodgepodge.",
    "crumbs": [
      "Backend"
    ]
  },
  {
    "objectID": "content/backend.html#the-repositories",
    "href": "content/backend.html#the-repositories",
    "title": "Backend",
    "section": "The Repositories",
    "text": "The Repositories\nThe details on how the containers are constructed and run through GitHub Actions are contained in the repositories themselves. I have a GitHub organization named ‘stg4-texas-24hr-ga’, this github org is owned by my main github ‘cfurl’. The benefit here is a) I can separate production code base from my messy repository and b) when an action run fails it hits my main email - which is shared with the cfurl repo. Here is a description of what is in those repos:\n\n‘stg4-texas-24hr-ga’ Public Github repo for this website. There are a bunch of good Quarto build examples here along with instructions on how to turn your site into a CI/CD system that builds daily. This website is a built off of a simple template provided by NOAA. The website is static.\n‘stg4-texas-24hr-backend-actions’ Private Github repo that contains the docker-compose.yml which controls container linking and orchestration, and the compose-workflow.yml which controls the triggering of the daily run.\n‘stg4-texas-24hr-docker’ Private Github repo that contains the codebase and the Dockerfile image build instructions to create each of the images. My typical workflow is I’ll develop in a separate sandbox repo. When I have the code working on an .Rproj relative path basis, I’ll amend the code to work with container paths, build the image, place on Docker Hub. I take the container path code, Dockerfile, and docker_file_build_instructions.txt and place in this repo. Everything you need to build the code from scratch (or ammend!) is in this repo.\n‘stg4-texas-24hr-frontend-actions’ Private Github repo that contains a Github actions daily website build connected to Posit Cloud Connect. The Github actions makes a daily commit, this triggers a PCC website build, and a new map is shown on the frontend. I’m almost positive I can delete this, it’s deprecated.",
    "crumbs": [
      "Backend"
    ]
  },
  {
    "objectID": "content/backend.html#notes",
    "href": "content/backend.html#notes",
    "title": "Backend",
    "section": "Notes",
    "text": "Notes\n\nDelivery of Stage IV data\nStage IV data are packaged as GRIB2 files are available each in 1-hr, 6-hr, and 24-hr files. The 6 and 24 hour files are summed from the one hour files. Presently, I’m working with a 24-hr file and pulls daily. I have 1-hr data processed for Texas for the entire period of record from 2002-2023 (or 2024), this is what I’ve been giving to Hatim.\nStage IV data are available in near real-time on NOMADS at :55 past the hour. For example, hourly rainfall ending 2:00 (representing 1:00-2:00) are available at 2:55. Twenty-four hour data are available from 12:00 - 12:00 UTC (available at 12:55 UTC). Data for the last 14 days are kept on NOMADS after which they are cycled off. The real-time NOMADS site is available here. Note, there are .01h, .06hr, and .24hr files for CONUS, pr (Puerto Rico), and ak (Alaska).\nAfter the near real-time file drop, an additional rerun at 30h after valid time (18:55Z) is available to supplement 24-hr mosaics if RFCs need to update their QPEs or make changes. Personal communication with the WGRFC indicates that this is done very infrequently.\nSince at least 2012 when I started tinkering with the archived Stage IV data, the historical datasets were kept at data.eol.ucar. As of summer 2025, the data have been moved to rda.ucar.edu. At this new archive location they are tarring together Stage IV data on monthly increments. The data appear to be coming available around the 20th of the next month (for example August data available ~ September 20th).\nRoutines for manual processing of the data are contained in this Private repo. It’s decently complete, but I really should go build it up since I do this work infrequently now.\n\n\nGRIB2 data format\nAs previously described, Stage IV data are stored in GRIB2 binary format. Stage IV precipitation has been packaged in both the GRIB1 format (until July 2020) and currently resides in the GRIB2 format. The processing procedures and wgrib.exe utility are completely different between GRIB1 and GRIB2. Again, could use some more documentation on how to build out historical records if I ever needed to do this outside of Texas.\n\n\nContainers and Orchestration\nBuilding an image that simply spawns a container to run a script with few libraries is very straightforward. It begins to get complicated when your image and resulting containers begin to rely on varied technology streams. Using the ‘scraper’ container as an example, pasted below is the ‘Dockerfile’ that you use to build the image. Each image starts from another image, in this case the FROM command downloads a specific version of r from the ‘rocker’ repository. This image or layer, contains all of the dependencies needed to run r 4.2.2, next I create a series of folders in my image, then I run an R command to install.packages, assign working directory, and lastly copy code from my local computer to the image.\nNow everytime I call on that image a container is spawned that runs ‘stg4_24hr_scraper.R’ in the proper R environment with the proper packages installed. This script simply downloads the latest GRIB2 file and puts it in a shared volume where the next container can interact with the file.\nBuild that image in PowerShell: docker image build –tag stg4_24hr_scraper . Run that image in PowerShell: docker container run -v //c/stg4/stg4-texas-24hr-docker/scraper/data:/home/data stg4_24hr_scraper\nYou can then inspect you container through PowerShell to see if your file downloaded.\n\nFROM rocker/r-ver:4.2.2\n\nRUN mkdir -p /home\nRUN mkdir -p /home/code\nRUN mkdir -p /home/data\n\nRUN R -e \"install.packages(c('dplyr','stringr','rvest'))\"\n\nWORKDIR /home\n\nCOPY /code/stg4_24hr_scraper.R /code/stg4_24hr_scraper.R\nCMD Rscript /code/stg4_24hr_scraper.R\n\nThe order of the containers and their dependencies are managed through Docker Compose. I still build “docker image build –tag stg4_24hr_scraper .”, test “docker container run -v //c/stg4/stg4-texas-24hr-docker/scraper/data:/home/data stg4_24hr_scraper”, tag, push, pull containers manually in PowerShell, but to put it in production you’ll want it in a docker-compose.yml. These are simple files to write, in general, getting the guts of the image correct is much more difficult that putting containers together.\nBelow is a piece of the docker-compose.yml used int the stg4 orchestration. For the GitHub Actions CI/CD this must reside at the root folder of the GA repository. When you work in this .yml you have to mind your indentations, runs fail fast so it’s nice, but very picky.\nThe ‘scraper’ container is the simplest of the bunch. Docker-compose already knows ‘cfurl/stg4_24hr_scraper:v1’ is referring to Docker Hub repo. The ‘volumes’ line indicates ‘shared volume between containers’:‘local path in specific container’. In the scraper example, data are downloaded from the web to /home/data, this path is automatically linked to the ‘st4_data’ volume so the rest of the individual containers can pick it up.\nThe ‘env_file’ is how AWS credentials are injected at Runtime. The GitHub Actions CI/CD builds the docker-compose environment daily and creates an AWS runtime each day. AWS credentials are stored securely in the GitHub repo. I learned the hard way it’s a bad idea to layer credentials into you Docker Image that lives on a repository.\nAnother note, when you want to hardwire some assets into your image, for example shapefiles or a .csv, you can’t place them in a folder named /data and then link your shared volume to that path: - st4_data:/home/data. It will overwrite it. When hardwiring assets in container keep them separate from shared volumes.\n\nservices:\n  \n  scraper:\n    image: cfurl/stg4_24hr_scraper:v1\n    container_name: scrape24\n    volumes:\n      - st4_data:/home/data\n\n  wgrib2:\n    image: cfurl/sondngyn_wgrib2:v1\n    container_name: wgrib2\n    command: \"wgrib2_commands.sh\"\n    depends_on:\n      scraper:\n        condition: service_completed_successfully\n    volumes: \n      - st4_data:/srv/\n      - st4_data:/opt/    \n      \n  parqs3:\n    image: cfurl/stg4_24hr_parq_s3:v1\n    container_name: parqs3\n    depends_on:\n      wgrib2:\n        condition: service_completed_successfully\n    volumes:\n      - st4_data:/home/data\n    env_file:\n      - .env\n\nvolumes:\n  st4_data:\n\nHere is the list of containers chained together in the docker-compose.yml all live on Docker Hub repository:\n\ncfurl/stg4_24hr_scraper:v1\ncfurl/sondngyn_wgrib2:v1\ncfurl/stg4_24hr_parq_s3:v2\ncfurl/stg4_24hr_daily_stat_update:v1\ncfurl/stg4_24hr_daily_make_map:v1\ncfurl/stg4_24hr_daily_make_hyet:v1\n\nThe docker-compose.yml gets triggered each day via Github Actions with the standard ‘docker-compose up’ command. Github Actions is controlled by a yml file placed in .github/workflows (path has to be named this for Actions to work). There I have my compose-workflow.yml. This took a long time to set up prior to ChatGPT, but the basic steps are you tell it when to fire:\n\non:\n  schedule:\n    - cron: '45 13 * * *'  # Runs daily at 13:45 UTC\n  workflow_dispatch:       # Also allow manual triggering\n\nthen a series of commands that install Docker in the virtual environment, assign AWS credentials, and lastly run ‘docker-compose up’ which starts the docker-compose.yml in the root directory. Important that the AWS credentials are managed through Github via the repository, and only injected at runtime. I still to this day have never added a new imaage to the compose workflow and had Github Actions run it successfully. Lot’s of debugging by examining the logs of your ‘Actions’ tab in the Github repository. It’s nice to drop some checkin points in your scripts in the containers to print ‘sucessfully this far’ to help know where you’re crashing.\n\n\nAWS S3 and Apache Arrow/.parq\nThe last 4 containers all interact with AWS S3 storage buckets. I have 7 buckets right now on S3, here is what they contain, later with individual scripts I’ll describe what writes to them:\n\nstg4-edwards-24hr-historical: parquet files of stg4 data from 2002-present across EAR.\nstg4-edwards-daily-maps: storage for .png maps generated daily (hyet, 24hr, ytd)\nstg4-edwards-daily-stats-24hr: parquet of daily basin averaged precipitation for all 10 subbasins\nstg4-edwards-latest: storage for most recent .png maps generated daily (hyet, 24hr, ytd). Only ever holds maps for most recent day.\nstg4-texas-24hr: parquet files of stg4 data from 2002-present across Texas.\nstg4-texas-24hr-backup: ChatGPT assisted ‘snapshot’ of “stg4-texas-24hr”\nstg4-texas-24hr-historical: parquet files of stg4 data from 2002-present across Texas.\n\nArrow\n\n\nImages\n\nImage 1 - scraper - cfurl/stg4_24hr_scraper:v1\nThis container takes the current system date and time, builds the appropriate NOMADS url based off of date and time, downloads the 24hr GRIB2 conus file at that url, and writes a shell script based off of url filename that is used in the next container to inject prompts to wgrib2.exe. Both the shell script and the GRIB2 file reside in a shared ephemeral storage volume that all containers are linked to.\nExternal writes: none\nPotential weaknesses: I wrote this script years ago. I should standardize the way way in interact with system time between all images because i do it frequently. Main problem though is if the script is run at night time between (0 - 12:55 UTC) before the most recent stage4 drops the whole thing fails. This hasn’t been a problem becasue my chron job starts at 13:55, but it’w worth considering.\n\n\nImage 2 - wgrib2 - cfurl/sondngyn_wgrib2:v1\nI took this image from sondngyn and packaged it in my own repository so it won’t go away. This is just wgrib2.exe built into a container. I execute my shell file written with the first container that supplies the commands to wgrib2.exe. The output binary to .txt format for the radar data is captured in the shared volume.\nExternal writes: none\nPotential weaknesses: I need to learn how to use Pygrib when i start working with other products. The encapsulation of the stg4 product in the GRIB2 file format is dead simple. I just export to text. File sizes cooperate and there aren’t a lot of complications of what’s in the GRIB2 file. If i start to work with other products like HRRR, GFS, etc I need to undertand details of how to chunk data, access specific layers etc.\n\n\nImage 3 - parqs3 - cfurl/stg4_24hr_parq_s3:v2\nThis docker container reads the .txt dump of radar data, tidy’s the GRIB2 .txt output, clips to area of interest (Texas and EAR), and then writes .parquet files that are partitioned by (year, month, day) to three S3 buckets.\nA note, you would probably have to partition by hour if you do this hourly, because you can’t append a .parquet file to existing data because of how it is written on the disk. Your partitions have to be separate when you are doing this in an automated fashion.\nExternal writes: “stg4-texas-24hr”; “stg4-texas-24hr-historical”; “stg4-edwards-24hr-historical”\nPotential weaknesses: You left_join your .csv of of radar bins that are in the Texas buffer with the individual radar bins for that day of data. You are joining by ‘lat’ and ‘lon’, so you have to make sure that those are unique to each other down to the decimal point so you don’t throw and NA. Seems to be working fine, but i recall some pain when working through the grid. A note on parquet files, you would probably have to partition by hour if you do this hourly, because you can’t append a .parquet file to existing data because of how it is written on the disk - it will overwrite. Your partitions have to be separate when you are doing this in an automated fashion.\n\n\nImage 4 - dailystat - cfurl/stg4_24hr_daily_stat_update:v1\nThis container pulls radar data from the most recent day (written just before in container 3) and calculates basin averaged precipitation for the 10 subbasins. Within the image, “/home/gis” I have .csv files hardwired that describe area of each radar bin for every basin. Basin avg precip is calculated by solving for volume of water in each radar bin, summing, and then dividing by total area of basin.\nExternal writes: “stg4-edwards-daily-stats-24hr”\nPotential weaknesses: Is it time saving to read from “stg4-edwards-24hr-historical” instead of “stg4-texas-24hr”? why do i convert from mm to in in this script?\n\n\nImage 5 - makemap - cfurl/stg4_24hr_daily_make_map:v1\nThis image accesses “stg4-texas-24hr” S3 bucket, and creates a tibble of the most recent daily rainfall across the EAA and the ytd across the EAA. The most recent 24hr rainfall is shown on a map highlighting the stream network. The ytd rainfall map shows the individual subbasin and minimizes the stream network. Both maps contain a semi-transparent table in the upper righthand corner of the figure. The legend in the ytd map is dynamic and will scale according to rainfall amounts (ie time of year). I’m curious to see how maps look in january with little rainfall. The 24hr map legend is static. Both maps have assigned/locked color bins. The maps are written to a long-term s3 repository with the date captured in the name and a bucket that contains only the most recent map. The “stg4-edwards-latest” bucket has pretty tight cache control so an individual viewing that link won’t cache it and see a previous rainfall map.\nExternal writes: “stg4-edwards-daily-maps”; “stg4-edwards-latest”\nPotential weaknesses: Is it time saving to read from “stg4-edwards-24hr-historical” instead of “stg4-texas-24hr” - probably so? It would make your left_join(rain_24hr, by = “grib_id”)|&gt; go way faster.\n\n\nImage 6 - makehyet - cfurl/stg4_24hr_daily_make_hyet:v1\nThis one access the daily stats written in container 3 and creates a hyetograph for each of the 10 subbasins. The ribbons are controlled by hard-wired statistics built from 2002-2024, these need to be updated manually at end of year. .png files get placed in the same location as the map files.\nExternal writes: “stg4-edwards-daily-maps”; “stg4-edwards-latest”",
    "crumbs": [
      "Backend"
    ]
  },
  {
    "objectID": "content/historical-builds.html",
    "href": "content/historical-builds.html",
    "title": "Historical Build",
    "section": "",
    "text": "You have to check out https://github.com/cfurl/manual_stg4_proces/ to get the deets. It’s pretty well documented in there. There is a pre-compiled version of wgrib2 in that repo. I had wgrib.exe for grib1 files on my desktop. Still had to do a little bulk rename outside of R. Also, the S3 bucket got hairy so i made the archive its own bucket stg4-texas-24hr-historical. I need to build out this section of the webpage because if would be nice to build this outside of Texas if need be one day.",
    "crumbs": [
      "Historical Build"
    ]
  },
  {
    "objectID": "content/historical-builds.html#adding-2002-2024-records",
    "href": "content/historical-builds.html#adding-2002-2024-records",
    "title": "Historical Build",
    "section": "",
    "text": "You have to check out https://github.com/cfurl/manual_stg4_proces/ to get the deets. It’s pretty well documented in there. There is a pre-compiled version of wgrib2 in that repo. I had wgrib.exe for grib1 files on my desktop. Still had to do a little bulk rename outside of R. Also, the S3 bucket got hairy so i made the archive its own bucket stg4-texas-24hr-historical. I need to build out this section of the webpage because if would be nice to build this outside of Texas if need be one day.",
    "crumbs": [
      "Historical Build"
    ]
  },
  {
    "objectID": "content/roadmap.html",
    "href": "content/roadmap.html",
    "title": "Roadmap",
    "section": "",
    "text": "To-do in the best order I can come up with:\nSmall things:\n\nDocument your manual processing flows when you load the 2002-2024 onto s3 bucket - add document on this site\nadd 2025 to stg4-texas-24hr-historical and start a daily parquet write to this bucket\nremove bucket$ls call from container\ncreate new current year and historical buckets with just Edwards radar bins (include recharge and artesian), stop carrying state\npoint your makemap container to the newly created EAA S3.\ncreate hyetograph figure (png), containerize, and to docker-compose workflow for daily publish and daily PCC site\nadd your dailystat, makemap, hyetograph container to the stg4-texas-docker repo after you monitor for a few days\nlink this page with github actions to publish daily\nkeep working on this website, a number of things have changed in your workflows 10.build out the 2002-2025 mapper for internal use with EAA S3\n\nBig things:\n\nBuild out the backend on completely AWS framework. Cloudwatch -&gt; Fargate -&gt; S3 -&gt; EC2 hosted web app\nStand-up an hourly AWS real-time site\nGet a well working surface water model, containerize it, and start forcing it hourly.\nAdd the RTMA product to your stack.\nStart in on sub-hourly MRMS processing\n\nDONE - Add daily images of ytd rainfall and 24hour rainfall to your S3 storage. (Found in stg4-edwards-daily-maps Bucket) - Add the historical data that you have on disk to your S3 bucket. (2002-2024 added; in stg4-texas-24hr-historical Bucket) - Figure out how to cache your map tiles to speed up shiny app (Done, hardwired them in container)",
    "crumbs": [
      "Roadmap"
    ]
  },
  {
    "objectID": "content/tech_overview.html",
    "href": "content/tech_overview.html",
    "title": "Tech Overview",
    "section": "",
    "text": "The rainfall observations are brought to numerical and graphical format from the streaming repository to .html front end through a series of Docker Containers. The containers live on cfurl Docker Hub and orchestration is accomplished through Docker Compose. Radar file output and graphs are stored on AWS S3. The CI/CD pipeline runs daily using GitHub Actions.\nBelow is a brief description of the daily workflow and and some highlights of the technologies used.\n\n24-hour GRIB2 radar file available on NOMADS at 12:55 UTC daily.\n\nGitHub Actions cron job is triggered at ~ 13:15 to setup Docker Compose env, AWS env, begin container series.\nContainer 1 - scrapes the GRIB2 file from NOMADS site and writes a shell script to execute container 2.\nContainer 2 - runs wgrib2.exe taking radar data out of binary format to .txt.\nContainer 3 - tidy data, clip to AOI, and write .parquet files to AWS s3 bucket for Texas and EAA areas.\nContainer 4 - access S3 radar data and calculate basin avg precip on areas of interest.\nContainer 5 - Publish maps to long-term repository and ‘latest’ buckets\nContainer 6 - Publish hyetographs to long-term repository and ‘latest’ buckets\nPosit Cloud Connect hosts simple website that displays .png files in the latest bucket. These are rewritten daily.",
    "crumbs": [
      "Tech overview"
    ]
  },
  {
    "objectID": "content/tech_overview.html#high-level-workflow",
    "href": "content/tech_overview.html#high-level-workflow",
    "title": "Tech Overview",
    "section": "",
    "text": "The rainfall observations are brought to numerical and graphical format from the streaming repository to .html front end through a series of Docker Containers. The containers live on cfurl Docker Hub and orchestration is accomplished through Docker Compose. Radar file output and graphs are stored on AWS S3. The CI/CD pipeline runs daily using GitHub Actions.\nBelow is a brief description of the daily workflow and and some highlights of the technologies used.\n\n24-hour GRIB2 radar file available on NOMADS at 12:55 UTC daily.\n\nGitHub Actions cron job is triggered at ~ 13:15 to setup Docker Compose env, AWS env, begin container series.\nContainer 1 - scrapes the GRIB2 file from NOMADS site and writes a shell script to execute container 2.\nContainer 2 - runs wgrib2.exe taking radar data out of binary format to .txt.\nContainer 3 - tidy data, clip to AOI, and write .parquet files to AWS s3 bucket for Texas and EAA areas.\nContainer 4 - access S3 radar data and calculate basin avg precip on areas of interest.\nContainer 5 - Publish maps to long-term repository and ‘latest’ buckets\nContainer 6 - Publish hyetographs to long-term repository and ‘latest’ buckets\nPosit Cloud Connect hosts simple website that displays .png files in the latest bucket. These are rewritten daily.",
    "crumbs": [
      "Tech overview"
    ]
  },
  {
    "objectID": "content/tech_overview.html#docker",
    "href": "content/tech_overview.html#docker",
    "title": "Tech Overview",
    "section": "Docker",
    "text": "Docker\nDocker has been incredibly useful. It makes code reproducible and portable more than any other tool I’ve used. You are able to package code (or data, databases, executables, applications, GIS) into an image that includes everything it needs to run the exact same way everytime. The same language runtime, system libraries, packages, and any required assets. That image can be pretty small (1-2 G) lives on a registry so it’s easy to call. I’m using Docker Hub ( https://hub.docker.com/repositories/cfurl), but AWS ECR, or a private repo is just as good. When you call the image it spawns a container executing what you put in the image. This results in a clean, repeatable run every single time - kills environment drift and the “works on my machine” problem. Multiple containers can be linked together so it’s very development friendly. You can mount volumes like drives for persistent data. Images are shared so you can pull other peoples images and start containers. There is a lot of stuff out there opensource. Looks like people have images of MODFLOW, SWAT, HSPF. There are containers for various AI frameworks, etc. My learning resource was ‘Docker in a Month of Lunches’ by Elton Stoneman.",
    "crumbs": [
      "Tech overview"
    ]
  },
  {
    "objectID": "content/tech_overview.html#wgrib2.exe",
    "href": "content/tech_overview.html#wgrib2.exe",
    "title": "Tech Overview",
    "section": "wgrib2.exe",
    "text": "wgrib2.exe\nwgrib2.exe is a command-line utility for working with GRIB2 meteorological data. It reads GRIB2 files, inspects metadata, and produces concise inventories of the messages they contain. It is widely used to filter and extract variables (e.g., precipitation, temperature, wind), select time steps and vertical levels, and subset by geographic region. The tool can regrid or resample fields to different projections or resolutions, apply standard interpolations, and perform basic calculations. It also supports format conversion, writing outputs to human-readable text/CSV and, when built with the appropriate libraries, to NetCDF. Designed for speed and scripting, it scales well for batch processing and automated workflows on large archives. The “.exe” denotes the Windows build; equivalent functionality is available on Linux and macOS as wgrib2. It is maintained on github by NOAA here: https://github.com/NOAA-EMC/wgrib2/releases",
    "crumbs": [
      "Tech overview"
    ]
  },
  {
    "objectID": "content/tech_overview.html#parquet-and-apache-arrow",
    "href": "content/tech_overview.html#parquet-and-apache-arrow",
    "title": "Tech Overview",
    "section": "Parquet and Apache Arrow",
    "text": "Parquet and Apache Arrow\nI learned about Apache Parquet and the Arrow R package at the Posit “Big Data with R” workshop, and they’ve changed my workflows for radar data. The short of it is you get in-memory processing speeds for data sets that so large they are only held on disk. Parquet is a columnar, compressed, splittable file format built for analytics: it stores columns together, so reads are fast and selective (you only scan the columns and row groups you need), and files are much smaller than CSV. Apache Arrow provides a columnar in-memory format and tooling that lets R (and other languages) scan Parquet lazily, push filters down to disk, and stream data in chunks—so you can work with datasets far larger than RAM instead of “loading everything, then filtering.” In practice, that means querying and summarizing hundreds of gigabytes on a laptop like they were loaded in-memory, especially when data are partitioned (e.g., by year/month/day) and stored locally or on S3. For analytics at scale, Parquet + Arrow has effectively replaced CSV for me: smaller, faster, and designed for selective reads—exactly what large radar archives demand.",
    "crumbs": [
      "Tech overview"
    ]
  },
  {
    "objectID": "content/tech_overview.html#s3",
    "href": "content/tech_overview.html#s3",
    "title": "Tech Overview",
    "section": "S3",
    "text": "S3\nAmazon S3 is durable, scalable object storage used to hold files of any size with high availability and pay-as-you-go pricing. It’s a common backbone for “data lake” workflows: tools read/write directly via s3://… paths, and in R, aws.s3 and Arrow support streaming/lazy access without local copies. Access control is handled with IAM; data governance via versioning, lifecycle policies, and server-side encryption. It’s low-maintenance, widely supported, and integrates cleanly with modern analytics stacks. I’ve used this tutorial regularly to read/write my S3’s: https://www.gormanalysis.com/blog/connecting-to-aws-s3-with-r/",
    "crumbs": [
      "Tech overview"
    ]
  },
  {
    "objectID": "content/tech_overview.html#github-actions",
    "href": "content/tech_overview.html#github-actions",
    "title": "Tech Overview",
    "section": "Github Actions",
    "text": "Github Actions\nGitHub Actions is GitHub’s built-in CI/CD platform that runs workflows on events (push, PR), schedules (cron), or manual dispatch. Workflows are defined as YAML files stored with your code, so versioning and reviews are straightforward. Runners can execute shell steps, build and run Docker containers (including docker-compose), and orchestrate multi-step jobs across OS targets. Sensitive values are injected at runtime via Repository/Environment Secrets (e.g., AWS credentials), avoiding hard-coding in images or source. This injection of secrets at runtime has saved me a lot of grief. Typical uses include scheduled data jobs, automated tests/linting, container builds and publishes, and static-site deploys (e.g., rebuilding documentation or a homepage on a timer). It’s a practical way to automate repeatable tasks directly from the repo without managing separate infrastructure—and can later hand off to a cloud-native backend if needed.",
    "crumbs": [
      "Tech overview"
    ]
  },
  {
    "objectID": "content/tech_overview.html#posit.connect.cloud",
    "href": "content/tech_overview.html#posit.connect.cloud",
    "title": "Tech Overview",
    "section": "posit.connect.cloud",
    "text": "posit.connect.cloud\nPosit Connect (cloud-managed) is a production publishing platform for R/Python: it hosts Shiny apps, Quarto/R Markdown sites, and APIs (e.g., Plumber) with HTTPS, auth, and versioned deploys. It handles builds and dependencies, environment variables/secrets, scheduled jobs (e.g., daily refresh), and integrates cleanly with Git/GitHub Actions for push-to-deploy. You get logs, usage metrics, access controls/SSO, and simple scaling (multiple processes/workers) without running servers. It’s a fast way to deliver interactive analytics now, with a clear path to containerized hosting on AWS later (ECS/Fargate, EKS, or EC2) using the same CI/secrets patterns.",
    "crumbs": [
      "Tech overview"
    ]
  }
]