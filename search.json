[
  {
    "objectID": "content/about.html",
    "href": "content/about.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files.\nx &lt;- c(5, 15, 25, 35, 45, 55)\ny &lt;- c(5, 20, 14, 32, 22, 38)\nlm(x ~ y)\n\n\nCall:\nlm(formula = x ~ y)\n\nCoefficients:\n(Intercept)            y  \n      1.056        1.326"
  },
  {
    "objectID": "content/about.html#modify-the-github-action",
    "href": "content/about.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/about.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/about.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/automation.html",
    "href": "content/automation.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files.\nx &lt;- c(5, 15, 25, 35, 45, 55)\ny &lt;- c(5, 20, 14, 32, 22, 38)\nlm(x ~ y)\n\n\nCall:\nlm(formula = x ~ y)\n\nCoefficients:\n(Intercept)            y  \n      1.056        1.326"
  },
  {
    "objectID": "content/automation.html#modify-the-github-action",
    "href": "content/automation.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/automation.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/automation.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/containers.html",
    "href": "content/containers.html",
    "title": "Images and Containers",
    "section": "",
    "text": "Stage IV data are available in near real-time on NOMADS at :55 past the hour. StageIV data are available hourly, 6-hr, and 24-hr. The 6 and 24 hour files are simply summed one hour files. For example, hourly rainfall ending 2:00 (representing 1:00-2:00) are available at 2:55. Twenty-four hour data are available from 12:00 - 12:00 UTC (available at 12:55 UTC). Data for the last 14 days are kept on NOMADS after which they are cycled off. The real-time NOMADS site is available at: https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/. Note there are CONUS, pr (Puerto Rico), and ak (Alaska) files.\nAn additional rerun at 30h after valid time (18:55Z) is available to supplement 24-hr mosaics if RFCs need to update their QPEs or make changes. Personal communication with the WGRFC indicates that this is done very infrequently.\nThe archived data were kept for years (decade?) at https://data.eol.ucar.edu/cgi-bin/codiac/fgr_form/id=21.093. As of summer 2025, the data were moved to https://rda.ucar.edu/datasets/d507005/dataaccess/.\nIn this manual, we are concerned with automating the 24-hr Stage IV rainfall product across Texas. However, these methods can be altered to work with hourly rainfall anywhere in the lower 48, Alaska or Puerto Rico.\n\n\n\nStage IV data are stored in GRIB2 binary format. GRIB (GRIdded Binary) is a WMO standard file format for gridded meteorological data. It stores fields (e.g., temperature, precipitation) as self-contained messages with metadata (grid, units, time, level) and compressed binary arrays for efficiency. Two versions exist: GRIB1 and GRIB2 (more flexible templates, better compression). Widely used for NWP model outputs and analyses; supports multiple projections, time steps, and packing methods. Tools like wgrib2 read/convert it. NOAA maintains a github page with the lastest grib conversion tool wgrib2.exe https://github.com/NOAA-EMC/wgrib2/releases.\nStage IV data were previously contained in GRIB1 format until July 2020. There are significant differences in the processing workflows between GRIB 1 and 2. I don’t intend to write about those in this manual.\n\n\n\nContainer orchestration and Github Action workflows are located in the following repo:\nContainer Orchestration\nThe workflow for processing the StageIV data are contained in 3 individual containers described below. The containers are managed by the docker-compose.yml which describes when to spin up each container, how to manage storage, environments etc. The docker-compose.yml is stored in the root folder of the github repo shown above. The docker-compose.yml gets triggered each day via Github Actions - described later. Each of the individual container images are stored on the docker hub repository at: https://hub.docker.com/repositories/cfurl. These should all be public.\n\n\n\nThe Dockerfile, code, and explicit Powershell prompts to create the Docker Image and load onto Docker Hub are contained in the following repo:\nDocker Images-Containers\nBelow is a brief explanation of what each container does.\n\n\nThis code takes the current system date and time, visits the appropriate NOMADS url based off of date and time, downloads the 24hr GRIB2 conus file at that url, writes a shell script that is used in the next container to inject prompts to wgrib2.exe, and stores both the shell script and the actual GRIB2 file in a shared storage volume (controlled through docker-compose.yml) that all three containers are linked to.\n\n\n\nThis code simply executes the wgrib2.exe application and takes the data from the binary format and dumps as text. This works in this case since the only thing wrapped in this GRIB2 file is stage4 rainfall. In many meteorological GRIBs there are many variables. The .sh shell file written in container one contain the instructions that are given to the wgrib2.exe applicatoin. This is controlled through docker-compose.yml, namely: “command:”wgrib2_commands.sh”“. The text output which is a bunch of lat/lons and rain values are stored in the same shared volume as the output files from Container 1.\n\n\n\nThis docker container connects to your “stg4-texas-24hr” AWS S3 bucket, tidy’s the GRIB2 csv output, clips to area of interest (Texas), and then writes .parquet files that are partitioned by (year, month, day). A note, you would probably have to partition by hour if you do this hourly, because you can’t append a .parquet file to existing data because of how it is written on the disk. Your partitions have to be separate when you are doing this in an automated fashion.\n\n\n\n\nGitHub Actions is a CI/CD platform integrated into GitHub. It runs workflows defined in YAML to build, test, and deploy code on events like pushes, pull requests, or schedules. Jobs run on hosted runners or self-hosted machines, support matrices, secrets, caching, and reusable actions, enabling reliable pipelines across repositories globally.\nGithub Actions\nI keep everything in a single GitHub repo—Dockerfiles, R scripts, and the workflow YAML. GitHub Actions is the scheduler and glue: a nightly cron (plus a manual trigger) launches a three-step, containerized pipeline. Step 1 is scrape: a lightweight Docker image pulls the latest NCEP Stage IV GRIB2 files from NOMADS, drops them and a ready-to-run shell script into a shared volume/workspace, and records the timestamp/digest so runs are idempotent. Step 2 is degrib: I spin up a container that contains wgrib2 and simply execute the script produced in step 1. That dumps the GRIB2 into tidy, delimited text (or binary) with fields like time, grib_id (HRAP bin), and precip. Step 3 is parq-s3: an R-based Docker image (arrow + dplyr + aws.s3) ingests the dump, standardizes column types, and writes partitioned Parquet to AWS S3 with keys like s3://bucket/stg4/ymd=YYYY-MM-DD/…. Partitions (year/month/day) make append-only updates safe and keep queries fast. AWS credentials are injected into the job via GitHub Secrets as environment variables; nothing sensitive is baked into images.\nFor mining the parquet, I use Arrow’s S3FileSystem to read datasets lazily straight from S3: open_dataset(“s3://bucket/stg4”) |&gt; filter(time &gt;= …) |&gt; group_by(hrap_id) |&gt; summarize(…). Most analysis is in R with dplyr, which Arrow translates into efficient scans and column pruning. I keep small, pre-aggregated artifacts (e.g., basin/day totals) in S3 as Parquet to accelerate the UI.\nThe front end is a Shiny app deployed to Posit Connect Cloud. It reads directly from S3 at runtime (no local copies), applies filters (date range, basin, thresholds), and renders maps/plots (ggplot2 + sf). Because the backend is columnar/parquet and partitioned, responses stay snappy even on large time windows. Deploys are automated: a Connect API token lets Actions redeploy the app on successful nightly runs, so the site reflects fresh data each morning. Net result: GitHub manages source and orchestration, Docker guarantees reproducibility, wgrib2 handles the meteorology-specific conversion, S3 provides durable and cheap storage, Arrow/dplyr power analysis, and Posit Connect publishes a clean, always-current front end."
  },
  {
    "objectID": "content/containers.html#the-details",
    "href": "content/containers.html#the-details",
    "title": "Images and Containers",
    "section": "",
    "text": "Stage IV data are available in near real-time on NOMADS at :55 past the hour. StageIV data are available hourly, 6-hr, and 24-hr. The 6 and 24 hour files are simply summed one hour files. For example, hourly rainfall ending 2:00 (representing 1:00-2:00) are available at 2:55. Twenty-four hour data are available from 12:00 - 12:00 UTC (available at 12:55 UTC). Data for the last 14 days are kept on NOMADS after which they are cycled off. The real-time NOMADS site is available at: https://nomads.ncep.noaa.gov/pub/data/nccf/com/pcpanl/prod/. Note there are CONUS, pr (Puerto Rico), and ak (Alaska) files.\nAn additional rerun at 30h after valid time (18:55Z) is available to supplement 24-hr mosaics if RFCs need to update their QPEs or make changes. Personal communication with the WGRFC indicates that this is done very infrequently.\nThe archived data were kept for years (decade?) at https://data.eol.ucar.edu/cgi-bin/codiac/fgr_form/id=21.093. As of summer 2025, the data were moved to https://rda.ucar.edu/datasets/d507005/dataaccess/.\nIn this manual, we are concerned with automating the 24-hr Stage IV rainfall product across Texas. However, these methods can be altered to work with hourly rainfall anywhere in the lower 48, Alaska or Puerto Rico.\n\n\n\nStage IV data are stored in GRIB2 binary format. GRIB (GRIdded Binary) is a WMO standard file format for gridded meteorological data. It stores fields (e.g., temperature, precipitation) as self-contained messages with metadata (grid, units, time, level) and compressed binary arrays for efficiency. Two versions exist: GRIB1 and GRIB2 (more flexible templates, better compression). Widely used for NWP model outputs and analyses; supports multiple projections, time steps, and packing methods. Tools like wgrib2 read/convert it. NOAA maintains a github page with the lastest grib conversion tool wgrib2.exe https://github.com/NOAA-EMC/wgrib2/releases.\nStage IV data were previously contained in GRIB1 format until July 2020. There are significant differences in the processing workflows between GRIB 1 and 2. I don’t intend to write about those in this manual.\n\n\n\nContainer orchestration and Github Action workflows are located in the following repo:\nContainer Orchestration\nThe workflow for processing the StageIV data are contained in 3 individual containers described below. The containers are managed by the docker-compose.yml which describes when to spin up each container, how to manage storage, environments etc. The docker-compose.yml is stored in the root folder of the github repo shown above. The docker-compose.yml gets triggered each day via Github Actions - described later. Each of the individual container images are stored on the docker hub repository at: https://hub.docker.com/repositories/cfurl. These should all be public.\n\n\n\nThe Dockerfile, code, and explicit Powershell prompts to create the Docker Image and load onto Docker Hub are contained in the following repo:\nDocker Images-Containers\nBelow is a brief explanation of what each container does.\n\n\nThis code takes the current system date and time, visits the appropriate NOMADS url based off of date and time, downloads the 24hr GRIB2 conus file at that url, writes a shell script that is used in the next container to inject prompts to wgrib2.exe, and stores both the shell script and the actual GRIB2 file in a shared storage volume (controlled through docker-compose.yml) that all three containers are linked to.\n\n\n\nThis code simply executes the wgrib2.exe application and takes the data from the binary format and dumps as text. This works in this case since the only thing wrapped in this GRIB2 file is stage4 rainfall. In many meteorological GRIBs there are many variables. The .sh shell file written in container one contain the instructions that are given to the wgrib2.exe applicatoin. This is controlled through docker-compose.yml, namely: “command:”wgrib2_commands.sh”“. The text output which is a bunch of lat/lons and rain values are stored in the same shared volume as the output files from Container 1.\n\n\n\nThis docker container connects to your “stg4-texas-24hr” AWS S3 bucket, tidy’s the GRIB2 csv output, clips to area of interest (Texas), and then writes .parquet files that are partitioned by (year, month, day). A note, you would probably have to partition by hour if you do this hourly, because you can’t append a .parquet file to existing data because of how it is written on the disk. Your partitions have to be separate when you are doing this in an automated fashion.\n\n\n\n\nGitHub Actions is a CI/CD platform integrated into GitHub. It runs workflows defined in YAML to build, test, and deploy code on events like pushes, pull requests, or schedules. Jobs run on hosted runners or self-hosted machines, support matrices, secrets, caching, and reusable actions, enabling reliable pipelines across repositories globally.\nGithub Actions\nI keep everything in a single GitHub repo—Dockerfiles, R scripts, and the workflow YAML. GitHub Actions is the scheduler and glue: a nightly cron (plus a manual trigger) launches a three-step, containerized pipeline. Step 1 is scrape: a lightweight Docker image pulls the latest NCEP Stage IV GRIB2 files from NOMADS, drops them and a ready-to-run shell script into a shared volume/workspace, and records the timestamp/digest so runs are idempotent. Step 2 is degrib: I spin up a container that contains wgrib2 and simply execute the script produced in step 1. That dumps the GRIB2 into tidy, delimited text (or binary) with fields like time, grib_id (HRAP bin), and precip. Step 3 is parq-s3: an R-based Docker image (arrow + dplyr + aws.s3) ingests the dump, standardizes column types, and writes partitioned Parquet to AWS S3 with keys like s3://bucket/stg4/ymd=YYYY-MM-DD/…. Partitions (year/month/day) make append-only updates safe and keep queries fast. AWS credentials are injected into the job via GitHub Secrets as environment variables; nothing sensitive is baked into images.\nFor mining the parquet, I use Arrow’s S3FileSystem to read datasets lazily straight from S3: open_dataset(“s3://bucket/stg4”) |&gt; filter(time &gt;= …) |&gt; group_by(hrap_id) |&gt; summarize(…). Most analysis is in R with dplyr, which Arrow translates into efficient scans and column pruning. I keep small, pre-aggregated artifacts (e.g., basin/day totals) in S3 as Parquet to accelerate the UI.\nThe front end is a Shiny app deployed to Posit Connect Cloud. It reads directly from S3 at runtime (no local copies), applies filters (date range, basin, thresholds), and renders maps/plots (ggplot2 + sf). Because the backend is columnar/parquet and partitioned, responses stay snappy even on large time windows. Deploys are automated: a Connect API token lets Actions redeploy the app on successful nightly runs, so the site reflects fresh data each morning. Net result: GitHub manages source and orchestration, Docker guarantees reproducibility, wgrib2 handles the meteorology-specific conversion, S3 provides durable and cheap storage, Arrow/dplyr power analysis, and Posit Connect publishes a clean, always-current front end."
  },
  {
    "objectID": "content/containers.html#github-repos",
    "href": "content/containers.html#github-repos",
    "title": "Images and Containers",
    "section": "Github repos",
    "text": "Github repos\nLooking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFay lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory."
  },
  {
    "objectID": "content/frontend.html",
    "href": "content/frontend.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland."
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "content/stg4-product.html",
    "href": "content/stg4-product.html",
    "title": "Stage IV Precipitation",
    "section": "",
    "text": "The NCEP “Stage IV” precipitation product is a national, multi-sensor quantitative precipitation estimate (QPE) created by mosaicking the 12 NWS River Forecast Centers’ hourly/6-hourly Stage III analyses—each of which blends WSR-88D radar with rain-gauge observations and RFC forecaster quality control—onto a common ~4 km HRAP grid for CONUS (with extensions for Alaska and Puerto Rico). It is delivered as hourly, 6-hourly, and 24-hour accumulations, with the key distinction that Stage IV benefits from manual QC inherited from the RFC inputs, in contrast to automated-only Stage II.\nReal-time Stage IV fields appear within a couple of hours after the accumulation window and are progressively “filled in” as additional RFCs publish; hourly fields are typically finalized 12–18 hours later, and 6-hour/24-hour mosaics by late the following day. For retrospective analyses, the reprocessed 24-hour Stage IV is generally the most accurate choice.\nOperationally, Stage IV is widely used for hydrologic verification, flood event forensics, and as precipitation forcing for models and water-resources studies. Data are distributed in GRIB (now GRIB2, after a 2020 format change) and, via NOAA Water, as NetCDF and GeoTIFF for CONUS/Alaska/Puerto Rico, which simplifies ingestion into GIS and scientific workflows.\nWhile the product provides near-national coverage at consistent resolution, users should remain mindful of known QPE limitations—e.g., radar beam blockage, bright-band contamination, convective undercatch, and sparse gauges in complex terrain—which can introduce regional biases that propagate into the mosaic. Peer-reviewed assessments document these characteristics and support best-practice choices (e.g., favoring the reprocessed daily field for post-event totals). American Meteorological Society Journals\nPractically, if you need reliable daily totals for historical flood summaries or water-supply studies, use the reprocessed 24-hour Stage IV; for near-real-time dashboards and alerting, the hourly stream is appropriate but should be treated as provisional until RFC finalization. Data access is available via NOMADS and NOAA Water directories, with numerous third-party mirrors and APIs (e.g., IEM) for convenience."
  },
  {
    "objectID": "content/stg4-product.html#overview",
    "href": "content/stg4-product.html#overview",
    "title": "Stage IV Precipitation",
    "section": "",
    "text": "The NCEP “Stage IV” precipitation product is a national, multi-sensor quantitative precipitation estimate (QPE) created by mosaicking the 12 NWS River Forecast Centers’ hourly/6-hourly Stage III analyses—each of which blends WSR-88D radar with rain-gauge observations and RFC forecaster quality control—onto a common ~4 km HRAP grid for CONUS (with extensions for Alaska and Puerto Rico). It is delivered as hourly, 6-hourly, and 24-hour accumulations, with the key distinction that Stage IV benefits from manual QC inherited from the RFC inputs, in contrast to automated-only Stage II.\nReal-time Stage IV fields appear within a couple of hours after the accumulation window and are progressively “filled in” as additional RFCs publish; hourly fields are typically finalized 12–18 hours later, and 6-hour/24-hour mosaics by late the following day. For retrospective analyses, the reprocessed 24-hour Stage IV is generally the most accurate choice.\nOperationally, Stage IV is widely used for hydrologic verification, flood event forensics, and as precipitation forcing for models and water-resources studies. Data are distributed in GRIB (now GRIB2, after a 2020 format change) and, via NOAA Water, as NetCDF and GeoTIFF for CONUS/Alaska/Puerto Rico, which simplifies ingestion into GIS and scientific workflows.\nWhile the product provides near-national coverage at consistent resolution, users should remain mindful of known QPE limitations—e.g., radar beam blockage, bright-band contamination, convective undercatch, and sparse gauges in complex terrain—which can introduce regional biases that propagate into the mosaic. Peer-reviewed assessments document these characteristics and support best-practice choices (e.g., favoring the reprocessed daily field for post-event totals). American Meteorological Society Journals\nPractically, if you need reliable daily totals for historical flood summaries or water-supply studies, use the reprocessed 24-hour Stage IV; for near-real-time dashboards and alerting, the hourly stream is appropriate but should be treated as provisional until RFC finalization. Data access is available via NOMADS and NOAA Water directories, with numerous third-party mirrors and APIs (e.g., IEM) for convenience."
  },
  {
    "objectID": "content/stg4-product.html#additional-reading",
    "href": "content/stg4-product.html#additional-reading",
    "title": "Stage IV Precipitation",
    "section": "Additional reading",
    "text": "Additional reading\nhttps://github.com/nmfs-opensci/NOAA-quarto-simple?utm_source=chatgpt.com\n\nAdd the files to _quarto.yml"
  },
  {
    "objectID": "content/tech_stack.html",
    "href": "content/tech_stack.html",
    "title": "How it works",
    "section": "",
    "text": "Let’s start with a very high level overview of the processes occurring, then we’ll briefly chat about some of the main technologies used. The details page has links to the github repos and further discussion of what is happening at each step.\n\n24-hour GRIB2 radar file available on NOMADS at 12:55 UTC.\n\nGitHub Actions cron job is triggered to start a series of docker containers.\nContainer 1 - scrapes the GRIB2 file from NOMADS site\nContainer 2 - GRIB2 file taken out of binary and dumps txt\nContainer 5 - organize rain data, clip to AOI, and writes .parquet files to AWS S3\nContainer orchestration done through Docker compose - AWS secrets injected at runtime with Github Actions\nDaily rainfall mapped with an R Shiny app and hosted through connect.posit.cloud\nMap is updated daily through an empty github commit to connect orchestrated with Github Actions"
  },
  {
    "objectID": "content/tech_stack.html#workflow-overview",
    "href": "content/tech_stack.html#workflow-overview",
    "title": "How it works",
    "section": "",
    "text": "Let’s start with a very high level overview of the processes occurring, then we’ll briefly chat about some of the main technologies used. The details page has links to the github repos and further discussion of what is happening at each step.\n\n24-hour GRIB2 radar file available on NOMADS at 12:55 UTC.\n\nGitHub Actions cron job is triggered to start a series of docker containers.\nContainer 1 - scrapes the GRIB2 file from NOMADS site\nContainer 2 - GRIB2 file taken out of binary and dumps txt\nContainer 5 - organize rain data, clip to AOI, and writes .parquet files to AWS S3\nContainer orchestration done through Docker compose - AWS secrets injected at runtime with Github Actions\nDaily rainfall mapped with an R Shiny app and hosted through connect.posit.cloud\nMap is updated daily through an empty github commit to connect orchestrated with Github Actions"
  },
  {
    "objectID": "content/tech_stack.html#docker",
    "href": "content/tech_stack.html#docker",
    "title": "How it works",
    "section": "Docker",
    "text": "Docker\nDocker has been the most influential piece of tech I’ve picked up since learning a scripting language. The basics as I can describe it are you write a piece of code like you normally would and then are able to package that code with everything it needs to work - specific r build, packages to install, shapefiles or other data to inject, and then the entire functioning piece of your code lives in an image and works like it would on your normal local environment. You can host this image on a repository (docker hub, AWS ECR, local repository) then everytime you want to run the code a container is spun up from the image. This takes repeatability to a new level. No more messing with environments when sharing code or hosting on different environments, the code and its processes are held in the container and are bullet proof. Additionally, you can link containers together, attach data storage volumes to them all while being super lightweight. My docker hub repository is https://hub.docker.com/repositories/cfurl. I very much enjoyed learning from Docker in a month of lunches by Elton Stoneman."
  },
  {
    "objectID": "content/tech_stack.html#wgrib2.exe",
    "href": "content/tech_stack.html#wgrib2.exe",
    "title": "How it works",
    "section": "wgrib2.exe",
    "text": "wgrib2.exe"
  },
  {
    "objectID": "content/tech_stack.html#parquet-and-apache-arrow",
    "href": "content/tech_stack.html#parquet-and-apache-arrow",
    "title": "How it works",
    "section": "Parquet and Apache Arrow",
    "text": "Parquet and Apache Arrow\nI picked up the .parquet file structure and Apache Arrow r package from a workshop I took at the POSIT conference in Seattle - Big data with R. Working with radar files had always been a grind during the PhD years and after due to file sizes you run into. Dragging some massive amount of data into your memory then manipulating it really limited you to maybe 10G worth of data which is not hard to get to with data coming off radar. Apache Arrow + .parquet solves this issue by allowing you to query and script on data while it resides on disk and is held out of memory. I’m the wrong guy to ask about the details, but you can query and manipulate hundreds of gigs of data at a time in a time efficient manner with a simple laptop. Parquet is the only way I’ll keep data files of any size now. Csv is dead and big data problems are rapidly becoming dead."
  },
  {
    "objectID": "content/tech_stack.html#s3",
    "href": "content/tech_stack.html#s3",
    "title": "How it works",
    "section": "S3",
    "text": "S3\nThere isn’t really anything special here or groundbreaking, just had not used S3 storage before. Since the data collected, manipulated, and displayed automatically I had to pick a cloud storage. This is seems to be pretty pain free. aws.s3 and apache arrow are pre-built to work with it, and I’ve had almost zero trouble spinning it up and maintaining it after some initial AWS woes."
  },
  {
    "objectID": "content/tech_stack.html#github-actions",
    "href": "content/tech_stack.html#github-actions",
    "title": "How it works",
    "section": "Github Actions",
    "text": "Github Actions\nGithub actions allows you to automatically run code on a variety of triggers including a cron trigger. This has been a very nice ‘training-wheels’ tool for me to use to execute code while I sleep. As I’ll discuss later in the ‘next steps’ I want to move this to an all AWS backend like a real developer would (making excellent progress), but for the time being this works. It’s nice because you can manage the .yml’s right there in github where you are maintaining your code, run my container orchestration with docker compose, and very importantly i can inject AWS credentials at run-time with repository secrets. Went through some pain here trying to hide them in docker images and other tricks. I also use it to republish this guide everyday after the new radar image drops that you see on the ‘Home’ page."
  },
  {
    "objectID": "content/tech_stack.html#posit.connect.cloud",
    "href": "content/tech_stack.html#posit.connect.cloud",
    "title": "How it works",
    "section": "posit.connect.cloud",
    "text": "posit.connect.cloud\nI’ve spent less than a day building the rshiny app you see here. Posit.cloud.connect was super easy to spin up, had a way to manage AWS credentials, and is able to auto-update daily with github actions. In the future I think the the shiny app (or some other web application) should be held in a container and hosted on AWS. That’s more stuff to learn and inthe meantime I can deliver rainfall maps and metrics quickly to end users with Connect."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Automated Radar Rainfall Mapping",
    "section": "",
    "text": "Most weather-radar data come from government agencies, yet the outputs are difficult for water resource managers to process or tailor to local basins. This guide documents the development of an automated radar precipitation processing system across a watershed of interest using the Stage IV QPE product from NCEP. Following the framework described in this guide, users can automate collection of radar rainfall across a watershed that can be used to track droughts, forecast reservoir inflows, estimate soil moisture, force surface water models, etc.\nThe geographic region used in this guide is the Edwards Aquifer recharge zone located in south-central Texas. Rainfall maps and basin averaged accumulation values are published daily at approximately 9:15 AM Central here - Edwards Aquifer rainfall.\n\n\n\nMost recent daily precipitation across Edwards Aquifer Recharge Zone"
  }
]